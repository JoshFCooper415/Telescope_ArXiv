title,original_abstract,generated_abstract,original_id,created,categories
"Summarize and Search: Learning Consensus-aware Dynamic Convolution for
  Co-Saliency Detection","Humans perform co-saliency detection by first summarizing the consensus
knowledge in the whole group and then searching corresponding objects in each
image. Previous methods usually lack robustness, scalability, or stability for
the first process and simply fuse consensus features with image features for
the second process. In this paper, we propose a novel consensus-aware dynamic
convolution model to explicitly and effectively perform the ""summarize and
search"" process. To summarize consensus image features, we first summarize
robust features for every single image using an effective pooling method and
then aggregate cross-image consensus cues via the self-attention mechanism. By
doing this, our model meets the scalability and stability requirements. Next,
we generate dynamic kernels from consensus features to encode the summarized
consensus knowledge. Two kinds of kernels are generated in a supplementary way
to summarize fine-grained image-specific consensus object cues and the coarse
group-wise common knowledge, respectively. Then, we can effectively perform
object searching by employing dynamic convolution at multiple scales. Besides,
a novel and effective data synthesis method is also proposed to train our
network. Experimental results on four benchmark datasets verify the
effectiveness of our proposed method. Our code and saliency maps are available
at \url{https://github.com/nnizhang/CADC}.","Title: Learning Consensus-Aware Dynamic Convolution for Co-Saliency Detection: A Summarize and Search Approach

Rewritten Abstract: The human ability to detect co-saliency involves summarizing consensus knowledge within a group, followed by identifying corresponding objects within each image. Existing techniques often lack robustness, scalability, or consistency in the initial phase, and typically merge consensus features with image features within the second phase. This study introduces a unique consensus-aware dynamic convolution model, designed to efficiently and distinctly execute the 'summarize and search' process. In order to summarize consensus image features, we initially collate robust features for each individual image through an efficient pooling method, and subsequently amalgamate cross-image consensus indications through the self-attention mechanism. This ensures our model satisfies the scalability and stability prerequisites. Subsequently, we create dynamic kernels from consensus features to encapsulate the summarized consensus knowledge. We generate two types of kernels in a complementary fashion to summarize detailed image-specific consensus object indications and the broad group-wide common knowledge. Consequently, we can efficiently conduct object searching through the use of dynamic convolution across various scales. Additionally, we propose a new and efficient data synthesis technique to train our network. Experiments conducted on four benchmark datasets confirm the efficacy of our proposed technique. Our code and saliency maps can be accessed at \url{https://github.com/nnizhang/CADC}.",oai:arXiv.org:2110.00338,2021-10-01,['cs.CV']
"H-FL: A Hierarchical Communication-Efficient and Privacy-Protected
  Architecture for Federated Learning","The longstanding goals of federated learning (FL) require rigorous privacy
guarantees and low communication overhead while holding a relatively high model
accuracy. However, simultaneously achieving all the goals is extremely
challenging. In this paper, we propose a novel framework called hierarchical
federated learning (H-FL) to tackle this challenge. Considering the degradation
of the model performance due to the statistic heterogeneity of the training
data, we devise a runtime distribution reconstruction strategy, which
reallocates the clients appropriately and utilizes mediators to rearrange the
local training of the clients. In addition, we design a compression-correction
mechanism incorporated into H-FL to reduce the communication overhead while not
sacrificing the model performance. To further provide privacy guarantees, we
introduce differential privacy while performing local training, which injects
moderate amount of noise into only part of the complete model. Experimental
results show that our H-FL framework achieves the state-of-art performance on
different datasets for the real-world image recognition tasks.","Title: H-FL: An Advanced, Hierarchical Structure for Efficient, Privacy-Safe Federated Learning

Revised Abstract: The consistent objectives of federated learning (FL) necessitate robust privacy protection, reduced communication overhead, and reasonably high model precision. Nonetheless, fulfilling these objectives concurrently presents a significant challenge. This study introduces a novel hierarchical federated learning (H-FL) framework designed to address this issue. Given the performance decline in the model due to statistical heterogeneity in the training data, we have developed a runtime distribution reconstruction method. This method strategically reallocates clients and employs mediators to reorganize the local training of clients. Moreover, we integrate a compression-correction mechanism within the H-FL framework to lower the communication overhead without compromising the performance of the model. We also incorporate differential privacy into the local training process to ensure privacy protection, by injecting a controlled level of noise into only a segment of the comprehensive model. Experimental outcomes demonstrate that our H-FL framework achieves unparalleled performance across various datasets for practical image recognition tasks.",oai:arXiv.org:2106.00275,2021-06-01,"['cs.LG', 'cs.CR', 'cs.DC']"
Online Fashion Commerce: Modelling Customer Promise Date,"In the e-commerce space, accurate prediction of delivery dates plays a major
role in customer experience as well as in optimizing the supply chain
operations. Predicting a date later than the actual delivery date might
sometimes result in the customer not placing the order (lost sales) while
promising a date earlier than the actual delivery date would lead to a bad
customer experience and consequent customer churn. In this paper, we present a
machine learning-based approach for penalizing incorrect predictions
differently using non-conventional loss functions, while working under various
uncertainties involved in making successful deliveries such as traffic
disruptions, weather conditions, supply chain, and logistics. We examine
statistical, deep learning, and conventional machine learning approaches, and
we propose an approach that outperformed the pre-existing rule-based models.
The proposed model is deployed internally for Fashion e-Commerce and is
operational.","Title: Digital Fashion Trade: Predicting Client Delivery Dates

Revamped Abstract: In the domain of online commerce, an essential factor influencing customer satisfaction and supply chain management efficacy is the precise forecast of delivery dates. Overestimating the delivery date may discourage a customer from completing an order, leading to lost sales. Conversely, underestimating the delivery date can result in poor customer experience, thereby increasing customer attrition rates. This research introduces a machine learning strategy that applies unconventional loss functions to penalize inaccurate predictions, taking into consideration the uncertainties inherent in successful deliveries, such as logistical issues, supply chain disruptions, adverse weather, and traffic congestion. We scrutinize statistical, conventional machine learning, and deep learning methodologies, putting forth a strategy that surpasses the existing rule-based models. The proposed model has been implemented internally for digital fashion commerce and is currently in operation.",oai:arXiv.org:2105.00315,2021-05-01,['cs.LG']
"Quantum crypto-economics: Blockchain prediction markets for the
  evolution of quantum technology","Two of the most important technological advancements currently underway are
the advent of quantum technologies, and the transitioning of global financial
systems towards cryptographic assets, notably blockchain-based cryptocurrencies
and smart contracts. There is, however, an important interplay between the two,
given that, in due course, quantum technology will have the ability to directly
compromise the cryptographic foundations of blockchain. We explore this complex
interplay by building financial models for quantum failure in various
scenarios, including pricing quantum risk premiums. We call this quantum
crypto-economics.","Title: Quantum Cryptoeconomic Interactions: The Implication of Blockchain Predictive Marketplaces on Quantum Technology Progression.

Revised Abstract: The ongoing development of quantum technologies and the gradual shift of global financial structures towards cryptographic assets, particularly blockchain-driven cryptocurrencies and smart contracts, are among the most significant current technological trends. However, a crucial interaction exists between these two domains, as quantum technology will eventually possess the capacity to undermine the cryptographic underpinnings of the blockchain. This paper delves into this intricate interaction, creating financial models that forecast quantum failure under different conditions, inclusive of the calculation of quantum risk premiums. We refer to this study as quantum cryptoeconomics.",oai:arXiv.org:2102.00659,2021-02-01,"['q-fin.PR', 'quant-ph']"
"Fool Me Once: Robust Selective Segmentation via Out-of-Distribution
  Detection with Contrastive Learning","In this work, we train a network to simultaneously perform segmentation and
pixel-wise Out-of-Distribution (OoD) detection, such that the segmentation of
unknown regions of scenes can be rejected. This is made possible by leveraging
an OoD dataset with a novel contrastive objective and data augmentation scheme.
By combining data including unknown classes in the training data, a more robust
feature representation can be learned with known classes represented distinctly
from those unknown. When presented with unknown classes or conditions, many
current approaches for segmentation frequently exhibit high confidence in their
inaccurate segmentations and cannot be trusted in many operational
environments. We validate our system on a real-world dataset of unusual driving
scenes, and show that by selectively segmenting scenes based on what is
predicted as OoD, we can increase the segmentation accuracy by an IoU of 0.2
with respect to alternative techniques.","Title: Duped No More: Sturdy Selective Segmentation through Out-of-Distribution Detection Using Contrastive Learning 

Revised Abstract: This research concentrates on the training of a network to concurrently conduct segmentation and pixel-wise Out-of-Distribution (OoD) detection, facilitating the rejection of segmentation in unknown scene areas. The accomplishment of this task is driven by the use of an OoD dataset, a groundbreaking contrastive objective, and a data augmentation scheme. By integrating data with unknown classes into the training data, a more resilient feature representation can be developed, with known classes distinctly differentiated from unknown ones. Current segmentation methods often display overconfidence in their erroneous segmentations when encountering unknown classes or conditions, rendering them unreliable in many operational contexts. We evaluate our model using an authentic dataset of abnormal driving situations, demonstrating that selective scene segmentation, based on OoD prediction, can enhance segmentation precision by an Intersection over Union (IoU) of 0.2 compared to alternative methods.",oai:arXiv.org:2103.00869,2021-03-01,"['cs.CV', 'cs.RO']"
"JAS-GAN: Generative Adversarial Network Based Joint Atrium and Scar
  Segmentations on Unbalanced Atrial Targets","Automated and accurate segmentations of left atrium (LA) and atrial scars
from late gadolinium-enhanced cardiac magnetic resonance (LGE CMR) images are
in high demand for quantifying atrial scars. The previous quantification of
atrial scars relies on a two-phase segmentation for LA and atrial scars due to
their large volume difference (unbalanced atrial targets). In this paper, we
propose an inter-cascade generative adversarial network, namely JAS-GAN, to
segment the unbalanced atrial targets from LGE CMR images automatically and
accurately in an end-to-end way. Firstly, JAS-GAN investigates an adaptive
attention cascade to automatically correlate the segmentation tasks of the
unbalanced atrial targets. The adaptive attention cascade mainly models the
inclusion relationship of the two unbalanced atrial targets, where the
estimated LA acts as the attention map to adaptively focus on the small atrial
scars roughly. Then, an adversarial regularization is applied to the
segmentation tasks of the unbalanced atrial targets for making a consistent
optimization. It mainly forces the estimated joint distribution of LA and
atrial scars to match the real ones. We evaluated the performance of our
JAS-GAN on a 3D LGE CMR dataset with 192 scans. Compared with the
state-of-the-art methods, our proposed approach yielded better segmentation
performance (Average Dice Similarity Coefficient (DSC) values of 0.946 and
0.821 for LA and atrial scars, respectively), which indicated the effectiveness
of our proposed approach for segmenting unbalanced atrial targets.","Title: JAS-GAN: An Advanced Generative Adversarial Network for Simultaneous Segmentation of Atrium and Scars on Unbalanced Atrial Targets 

Generated Abstract: The necessity for automated and precise segmentations of the left atrium (LA) and atrial scars from late gadolinium-enhanced cardiac magnetic resonance (LGE CMR) images for the purpose of atrial scar quantification is burgeoning. Prior methods for atrial scar quantification were reliant on a two-phase segmentation process for LA and atrial scars, due to the significant volume disparity, or unbalanced atrial targets. This study introduces JAS-GAN, an advanced inter-cascade generative adversarial network designed to segment unbalanced atrial targets from LGE CMR images in an automatic and accurate, end-to-end manner. The JAS-GAN incorporates an adaptive attention cascade to autonomously correlate the segmentation tasks of the unbalanced atrial targets, specifically modeling the inclusion relationship between the two unbalanced atrial targets, with the predicted LA serving as the attention map to adaptively focus on the smaller atrial scars. Adversarial regularization is subsequently applied to these segmentation tasks to achieve consistent optimization, compelling the predicted joint distribution of LA and atrial scars to align with the actual ones. Our JAS-GAN was assessed on a 3D LGE CMR dataset containing 192 scans. The results demonstrated superior segmentation performance in comparison to existing techniques, with Average Dice Similarity Coefficient (DSC) values of 0.946 and 0.821 for LA and atrial scars, respectively, thus validating the effectiveness of our approach in segmenting unbalanced atrial targets.",oai:arXiv.org:2105.00234,2021-05-01,"['eess.IV', 'cs.CV']"
"Researching of magnetic cutoff for local sources of charged particles in
  the halo of the Galaxy","Models of highly inhomogeneous baryosynthesis of the baryonic asymmetric
Universe allow for the existence of macroscopic domains of antimatter, which
could evolve in a globular cluster of antimatter stars in our Galaxy. We assume
the symmetry of the evolution of a globular cluster of stars and antistars
based on the symmetry of the properties of matter and antimatter. Such object
can be a source of a fraction of antihelium nuclei in galactic cosmic rays. It
makes possible to predict the expected fluxes of cosmic antinuclei with use of
known properties of matter star globular clusters We have estimated the lower
cutoff energy for the penetration of antinuclei from the antimatter globular
cluster, situated in halo, into the galactic disk based on the simulation of
particle motion in the large-scale structure of magnetic fields in the Galaxy.
We have estimated the magnitude of the magnetic cutoff for the globular cluster
M4.","Title: Investigation into Magnetic Cutoffs for Local Sources of Charged Particles in the Galactic Halo

Revised Abstract: The highly irregular models of baryosynthesis of the asymmetric baryonic Universe suggest the potential existence of large antimatter domains. These could potentially transform into a globular cluster of antimatter stars within our own Galaxy. We hypothesize a symmetrical evolution of a globular cluster of stars and antistars, reflecting the symmetrical properties of matter and antimatter. Such an entity could contribute to a percentage of antihelium nuclei in galactic cosmic rays, enabling the prediction of cosmic antinuclei fluxes by utilizing the known properties of matter star globular clusters. We have calculated the minimum energy cutoff required for antinuclei to penetrate from the antimatter globular cluster located in the halo into the galactic disk. This is based on simulations of particle movement within the Galaxy's large-scale magnetic field structure. Furthermore, we have determined the magnetic cutoff's extent for the globular cluster M4.",oai:arXiv.org:2112.00361,2021-12-01,"['astro-ph.HE', 'astro-ph.CO']"
"More Behind Your Electricity Bill: a Dual-DNN Approach to Non-Intrusive
  Load Monitoring","Non-intrusive load monitoring (NILM) is a well-known single-channel blind
source separation problem that aims to decompose the household energy
consumption into itemised energy usage of individual appliances. In this way,
considerable energy savings could be achieved by enhancing household's
awareness of energy usage. Recent investigations have shown that deep neural
networks (DNNs) based approaches are promising for the NILM task. Nevertheless,
they normally ignore the inherent properties of appliance operations in the
network design, potentially leading to implausible results. We are thus
motivated to develop the dual Deep Neural Networks (dual-DNN), which aims to i)
take advantage of DNNs' learning capability of latent features and ii) empower
the DNN architecture with identification ability of universal properties.
Specifically in the design of dual-DNN, we adopt one subnetwork to measure
power ratings of different appliances' operation states, and the other
subnetwork to identify the running states of target appliances. The final
result is then obtained by multiplying these two network outputs and meanwhile
considering the multi-state property of household appliances. To enforce the
sparsity property in appliance's state operating, we employ median filtering
and hard gating mechanisms to the subnetwork for state identification. Compared
with the state-of-the-art NILM methods, our dual-DNN approach demonstrates a
21.67% performance improvement in average on two public benchmark datasets.","Title: Unveiling the Hidden Components of Your Electricity Bill: Implementing a Dual-Deep Neural Network Methodology for Non-Invasive Load Monitoring

Rewritten Abstract: Non-invasive load monitoring (NILM) is a recognized method for resolving the single-channel blind source separation issue by partitioning the overall energy usage of a home into the individual energy consumption of each appliance. This practice can potentially lead to significant energy conservation by increasing household knowledge of energy usage. Recent research indicates that deep neural network (DNN) based strategies show potential in addressing NILM tasks. However, these strategies often overlook the inherent characteristics of appliance operation in their network design, which could potentially lead to unreliable results. Consequently, we have been motivated to innovate the dual deep neural networks (dual-DNN) to exploit the DNNs' latent feature learning capacity and to equip the DNN structure with the ability to identify universal properties. In the development of the dual-DNN, we utilize one subnetwork to evaluate the power ratings of various appliance operation states, and a different subnetwork to discern the operational states of the appliances in question. The final outcome is calculated by multiplying the outputs of these two networks and simultaneously accounting for the multi-state nature of household appliances. To enforce the sparsity property in the operation state of an appliance, we incorporate median filtering and hard gating mechanisms into the state identification subnetwork. Our dual-DNN methodology, when compared with leading NILM techniques, exhibits an average performance enhancement of 21.67% on two publicly available benchmark datasets.",oai:arXiv.org:2106.00297,2021-06-01,"['cs.LG', 'cs.AI']"
"Unconventional satellite resistance peaks in moir\'e superlattice of
  h-BN/ AB-stacked tetralayer-graphene heterostructure","Most studies on moir\'e superlattices formed from a stack of h-BN and
graphene have focused on single layer graphene; graphene with multiple layers
is less understood. Here, we show that a moir\'e superlattice of multilayer
graphene shows new features arising from the anisotropic Fermi surface affected
by the superlattice structure. The moir\'e superlattice of a h-BN/AB-stacked
tetralayer graphene heterostructure exhibited resistivity peaks showing a
complicated dependence on the perpendicular electric field. The peaks were not
due to secondary Dirac cones forming, but rather opening of the energy gap due
to folding of the anisotropic Fermi surface. In addition, superlattice peaks
resulted from mixing of light- and heavy-mass bilayer-like bands via the
superlattice potential. The gaps did not open on the boundary of the
superlattice Brillouin zone, but rather opened inside it, which reflected the
anisotropy of the Fermi surface of multilayer graphene.","Title: Novel Satellite Resistance Peaks in h-BN/ AB-stacked Tetralayer-Graphene Heterostructure's Moiré Superlattice

Alternative Abstract: The exploration of moiré superlattices formed from the combination of h-BN and graphene has predominantly been centered on single layer graphene, leaving multilayer graphene somewhat unexamined. This study demonstrates new properties exhibited by a multilayer graphene's moiré superlattice, which are a result of the anisotropic Fermi surface being influenced by the superlattice structure. The h-BN/AB-stacked tetralayer graphene heterostructure's moiré superlattice displayed resistivity peaks that exhibited a complex relationship with the orthogonal electric field. These peaks did not originate from the formation of secondary Dirac cones, but were instead due to the opening of the energy gap caused by folding of the anisotropic Fermi surface. Furthermore, the superlattice peaks were a consequence of the intermingling of light- and heavy-mass bilayer-like bands via the superlattice potential. The energy gaps did not form on the periphery of the superlattice Brillouin zone, but rather within it, which mirrors the anisotropy of the Fermi surface of multilayer graphene.",oai:arXiv.org:2104.00261,2021-04-01,"['cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cond-mat.soft']"
"Characterizing and Detecting Configuration Compatibility Issues in
  Android Apps","XML configuration files are widely used in Android to define an app's user
interface and essential runtime information such as system permissions. As
Android evolves, it might introduce functional changes in the configuration
environment, thus causing compatibility issues that manifest as inconsistent
app behaviors at different API levels. Such issues can often induce software
crashes and inconsistent look-and-feel when running at specific Android
versions. Existing works incur plenty of false positive and false negative
issue-detection rules by conducting trivial data-flow analysis while failing to
model the XML tree hierarchies of the Android configuration files. Besides,
little is known about how the changes in an Android framework can induce such
compatibility issues. To bridge such gaps, we conducted a systematic study by
analyzing 196 real-world issues collected from 43 popular apps. We identified
common patterns of Android framework code changes that induce such
\textsc{ConfDroid} that can automatically extract rules for detecting
configuration compatibility issues. The intuition is to perform symbolic
execution based on a model learned from the common code change patterns.
Experiment results show that ConfDroid can successfully extract 282 valid
issue-detection rules with a precision of 91.9%. Among them, 65 extracted rules
can manifest issues that cannot be detected by the rules of state-of-the-art
baselines. More importantly, 11 out of them have led to the detection of 107
reproducible configuration compatibility issues that the baselines cannot
detect in 30 out of 316 real-world Android apps.","Title: Identifying and Resolving Configuration Compatibility Challenges in Android Applications

Revised Abstract: Android applications extensively utilize XML configuration files to establish the user interface and vital runtime parameters, including system permissions. With the advancement of Android, functional alterations in the configuration setting may be introduced, leading to compatibility problems that result in incongruous application functionalities at varying API levels. These issues often result in software crashes and inconsistent user experiences across different Android versions. Current research presents a high number of false positives and negatives in issue detection owing to insufficient data flow analysis and an inability to accurately represent the XML tree hierarchies in Android configuration files. Additionally, there is a dearth of knowledge regarding how modifications in the Android framework can lead to these compatibility problems. To address these gaps, we undertook a comprehensive study, analyzing 196 real-world issues from 43 widely-used applications. We discerned recurring patterns in Android framework code modifications that contribute to these configuration compatibility issues. Based on these findings, we developed ConfDroid, an innovative system that can autonomously derive rules for identifying configuration compatibility problems. The primary approach is to perform symbolic execution based on a model derived from common code modification patterns. Experimental outcomes indicate that ConfDroid can successfully generate 282 reliable issue detection rules with a precision rate of 91.9%. Of these, 65 rules can identify issues not detectable by existing state-of-the-art baseline rules. Most notably, 11 of these rules have led to the identification of 107 reproducible configuration compatibility issues in 30 out of 316 real-world Android applications, which were previously undetectable by the baseline methods.",oai:arXiv.org:2109.00300,2021-09-01,['cs.SE']
Seeing Implicit Neural Representations as Fourier Series,"Implicit Neural Representations (INR) use multilayer perceptrons to represent
high-frequency functions in low-dimensional problem domains. Recently these
representations achieved state-of-the-art results on tasks related to complex
3D objects and scenes. A core problem is the representation of highly detailed
signals, which is tackled using networks with periodic activation functions
(SIRENs) or applying Fourier mappings to the input. This work analyzes the
connection between the two methods and shows that a Fourier mapped perceptron
is structurally like one hidden layer SIREN. Furthermore, we identify the
relationship between the previously proposed Fourier mapping and the general
d-dimensional Fourier series, leading to an integer lattice mapping. Moreover,
we modify a progressive training strategy to work on arbitrary Fourier mappings
and show that it improves the generalization of the interpolation task. Lastly,
we compare the different mappings on the image regression and novel view
synthesis tasks. We confirm the previous finding that the main contributor to
the mapping performance is the size of the embedding and standard deviation of
its elements.","Title: A Comparative Analysis of Implicit Neural Representations via Fourier Series

Revised Abstract: The study explores the usage of Implicit Neural Representations (INR), utilizing multilayer perceptrons in order to depict high-frequency functions within low-dimensional issues. These representations have recently acquired top-notch outcomes within complex 3D objects and scenes-related tasks. The primary challenge lies in the portrayal of exceptionally detailed signals, which is approached by employing networks with periodic activation functions (SIRENs) or introducing Fourier mappings to the input. The research scrutinizes the correlation between these two techniques, illustrating that a Fourier mapped perceptron structurally resembles a single hidden layer SIREN. The study also underscores the link between the formerly suggested Fourier mapping and the generic d-dimensional Fourier series, culminating in an integer lattice mapping. In addition, we adapt a progressive training approach to function with arbitrary Fourier mappings, demonstrating its efficacy in enhancing the generalization of the interpolation task. Lastly, we contrast the varying mappings on image regression and novel view synthesis tasks, corroborating the previous discovery that the primary contributor to the mapping performance is the magnitude of the embedding and the standard deviation of its components.",oai:arXiv.org:2109.00249,2021-09-01,['cs.CV']
"The metal-poor end of the Spite plateau. II. Detailed chemical
  investigation","Context. The study of old, metal-poor stars deepens our knowledge on the
early stages of the universe. In particular, the study of these stars gives us
a valuable insight into the masses of the first massive stars and their
emission of ionising photons. Aims. We present a detailed chemical analysis and
determination of the kinematic and orbital properties of a sample of 11 dwarf
stars. These are metal-poor stars, and a few of them present a low lithium
content. We inspected whether the other elements also present anomalies.
Methods. We analysed the high-resolution UVES spectra of a few metal-poor stars
using the Turbospectrum code to synthesise spectral lines profiles. This
allowed us to derive a detailed chemical analysis of Fe,","Title: An In-depth Chemical Investigation of the Metal-Deficient Spectrum of the Spite Plateau: Part II

Rewritten Abstract: Background. Our understanding of the universe's early periods is significantly enhanced by examining aged, metal-poor stars. Specifically, these stellar studies provide invaluable data regarding the weights of the initial large-scale stars and their output of ionising photons. Objectives. Our research offers a comprehensive chemical assessment and quantification of the kinematic and orbital parameters of a group of 11 dwarf stars, known for their metal-deficiency. A handful of these stars display a reduced lithium presence, prompting us to examine if anomalies are also observable in other elements. Methods. We employed Turbospectrum code to examine the high-resolution UVES spectra of several metal-deficient stars and generate spectral line profiles. This method facilitated a thorough chemical examination of Fe.",oai:arXiv.org:2110.00243,2021-10-01,"['astro-ph.SR', 'astro-ph.GA']"
"Skew-product dynamical systems for crossed product $C^*$-algebras and
  their ergodic properties","Starting from a discrete $C^*$-dynamical system $(\mathfrak{A}, \theta,
\omega_o)$, we define and study most of the main ergodic properties of the
crossed product $C^*$-dynamical system $(\mathfrak{A}\rtimes_\alpha\mathbb{Z},
\Phi_{\theta, u},\om_o\circ E)$,
$E:\mathfrak{A}\rtimes_\alpha\mathbb{Z}\rightarrow\ga$ being the canonical
conditional expectation of $\mathfrak{A}\rtimes_\alpha\mathbb{Z}$ onto
$\mathfrak{A}$, provided $\a\in\aut(\ga)$ commute with the $*$-automorphism
$\th$ up tu a unitary $u\in\ga$. Here, $\Phi_{\theta,
u}\in\aut(\mathfrak{A}\rtimes_\alpha\mathbb{Z})$ can be considered as the fully
noncommutative generalisation of the celebrated skew-product defined by H.
Anzai for the product of two tori in the classical case.","Title: Investigation of Skew-Product Dynamical Systems in Crossed Product $C^*$-Algebras and Assessment of Their Ergodic Attributes

Revised Abstract: Utilizing a discrete $C^*$-dynamical system, designated as $(\mathfrak{A}, \theta, \omega_o)$, as our starting point, this study examines the primary ergodic characteristics of the crossed product $C^*$-dynamical system, denoted as $(\mathfrak{A}\rtimes_\alpha\mathbb{Z}, \Phi_{\theta, u},\om_o\circ E)$. Here, $E:\mathfrak{A}\rtimes_\alpha\mathbb{Z}\rightarrow\ga$ is the standard conditional expectation of $\mathfrak{A}\rtimes_\alpha\mathbb{Z}$ onto $\mathfrak{A}$, assuming $\a\in\aut(\ga)$ aligns with the $*$-automorphism $\th$, up to a unitary $u\in\ga$. In this context, $\Phi_{\theta, u}\in\aut(\mathfrak{A}\rtimes_\alpha\mathbb{Z})$ can be interpreted as the comprehensive noncommutative extension of the well-known skew-product, which H. Anzai originally defined for the multiplication of two tori in the traditional scenario.",oai:arXiv.org:2105.00197,2021-05-01,"['math.OA', 'math.DS']"
"What Is the Generalized Representation of Dirac Equation in Two
  Dimensions?","In this work, the general form of $2\times2$ Dirac matrices for 2+1 dimension
is found. In order to find this general representation, all relations among the
elements of the matrices and matrices themselves are found,and the generalized
Lorentz transform matrix is also found under the effect of the general
representation of Dirac matrices. As we know, the well known equation of Dirac,
$ \left( i\gamma^{\mu}\partial_{\mu}-m\right) \Psi=0 $, is consist of matrices
of even dimension known as the general representation of Dirac matrices or
Dirac matrices. Our motivation for this study was lack of the general
representation of these matrices despite the fact that more than nine decades
have been passed since the discovery of this well known equation. Everyone has
used a specific representation of this equation according to their need; such
as the standard representation known as Dirac-Pauli Representation, Weyl
Representation or Majorana representation. In this work, the general form which
these matrices can have is found once for all.","Title: Unveiling the Universal Form of Dirac Equation in Bi-Dimensional Space

Revised Abstract: This study explores and identifies the universal structure of Dirac matrices in 2+1 dimensions, specifically those in the $2\times2$ form. The process involved a comprehensive examination of the interrelations between matrix elements and the matrices themselves. Furthermore, the generalized Lorentz transform matrix was derived in light of the universal representation of Dirac matrices. The Dirac equation, $ \left( i\gamma^{\mu}\partial_{mu}-m\right) \Psi=0 $, is constituted by matrices of even dimensions, commonly referred to as Dirac matrices or the general representation of Dirac matrices. The impetus for this research was the absence of a universal representation of these matrices, despite over ninety years having passed since the inception of the renowned equation. To date, various specific representations, such as the Dirac-Pauli, Weyl, or Majorana representations, have been employed based on individual requirement. This investigation successfully uncovers the universal structure that these matrices can adopt, providing a comprehensive solution.",oai:arXiv.org:2104.00388,2021-04-01,['quant-ph']
"Boosting Certified $\ell_\infty$ Robustness with EMA Method and Ensemble
  Model","The neural network with $1$-Lipschitz property based on $\ell_\infty$-dist
neuron has a theoretical guarantee in certified $\ell_\infty$ robustness.
However, due to the inherent difficulties in the training of the network, the
certified accuracy of previous work is limited. In this paper, we propose two
approaches to deal with these difficuties. Aiming at the characteristics of the
training process based on $\ell_\infty$-norm neural network, we introduce the
EMA method to improve the training process. Considering the randomness of the
training algorithm, we propose an ensemble method based on trained base models
that have the $1$-Lipschitz property and gain significant improvement in the
small parameter network. Moreover, we give the theoretical analysis of the
ensemble method based on the $1$-Lipschitz property on the certified
robustness, which ensures the effectiveness and stability of the algorithm. Our
code is available at
https://github.com/Theia-4869/EMA-and-Ensemble-Lip-Networks.","Title: Enhancing Certified $\ell_\infty$ Resilience using the EMA Technique and Ensemble Model

Generated Abstract: Neural networks that exhibit the $1$-Lipschitz attribute reliant on an $\ell_\infty$-dist neuron provide a theoretical assurance to certified $\ell_\infty$ robustness. Nonetheless, the inherent complexities involved in training such networks limit the certified accuracy established in prior research. This study presents two strategies to overcome these challenges. To address the unique properties of the training process reliant on the $\ell_\infty$-norm neural network, we incorporate the EMA method to refine the training process. Furthermore, considering the unpredictable nature of the training algorithm, we suggest an ensemble approach that uses previously trained base models that possess the $1$-Lipschitz attribute. This significantly improves the performance in networks with small parameters. Additionally, we provide a theoretical analysis of the ensemble approach, indicating its effectiveness and stability based on the $1$-Lipschitz feature in certified robustness. The algorithm's effectiveness and stability are thus ensured. The code for this study is accessible at https://github.com/Theia-4869/EMA-and-Ensemble-Lip-Networks.",oai:arXiv.org:2107.00230,2021-07-01,['cs.LG']
"Collision Chains among the Terrestrial Planets. III. Formation of the
  Moon","In the canonical model of Moon formation, a Mars-sized protoplanet ""Theia""
collides with proto-Earth at close to their mutual escape velocity $v_{\rm
esc}$ and a common impact angle 45{\deg}. The ""graze-and-merge"" collision
strands a fraction of Theia's mantle into orbit, while Earth accretes most of
Theia and its momentum. Simulations show that this produces a hot, high angular
momentum, silicate-dominated protolunar system, in substantial agreement with
lunar geology, geochemistry, and dynamics. However, a Moon that derives mostly
from Theia's mantle, as angular momentum dictates, is challenged by the fact
that O, Ti, Cr, radiogenic W, and other elements are indistinguishable in Earth
and lunar rocks. Moreover, the model requires an improbably low initial
velocity. Here we develop a scenario for Moon formation that begins with a
somewhat faster collision, when proto-Theia impacts proto-Earth at ~1.2 $v_{\rm
esc}$, also around 45{\deg}. Instead of merging, the bodies come into violent
contact for a half-hour and their major components escape, a ""hit-and-run
collision."" N-body evolutions show that the ""runner"" often returns ~0.1-1 Myr
later for a second giant impact, closer to $v_{\rm esc}$; this produces a
postimpact disk of ~2-3 lunar masses in smoothed particle hydrodynamics
simulations, with angular momentum comparable to canonical scenarios. The disk
ends up substantially inclined, in most cases, because the terminal collision
is randomly oriented to the first. Proto-Earth contributions to the silicate
disk are enhanced by the compounded mixing and greater energy of a collision
chain.","Title: Series of Collisions Among Terrestrial Planets: The Development of the Moon, Part III

Revised Abstract: The traditional model of lunar formation suggests a Mars-sized celestial body coined ""Theia"" initiated a collision with the early Earth at nearly their combined escape velocity $v_{\rm esc}$ and shared an impact angle of 45{\deg}. This ""graze-and-merge"" impact results in a portion of Theia's mantle being cast into orbit, while the Earth absorbs the majority of Theia and its momentum. Computational models reveal this leads to a heated, high-angular momentum, and silicate-rich protolunar system, which aligns well with the established lunar geology, geochemistry, and dynamics. However, the model encounters obstacles, primarily that the Moon, which should be largely composed of Theia's mantle as per angular momentum, exhibits O, Ti, Cr, radiogenic W, and other elements that are identical in both lunar and terrestrial rocks. Moreover, the model is dependent on an unlikely low initial velocity. This study proposes a modified scenario for lunar genesis, wherein proto-Theia collides with proto-Earth at approximately 1.2 $v_{\rm esc}$, also around 45{\deg}. Rather than merging, the bodies undergo a violent, brief contact, termed a ""hit-and-run collision,"" resulting in the escape of their primary components. The ""hit-and-run"" object often returns for a second major impact with velocities closer to $v_{\rm esc}$ after an estimated 0.1-1 Myr. Smoothed particle hydrodynamics simulations demonstrate this leads to a post-impact disk with around 2-3 lunar masses and an angular momentum similar to traditional models. The resulting disk is typically significantly inclined due to the random orientation of the terminal collision to the initial one. The compounded mixing and amplified energy of a collision chain bolster the contributions of the proto-Earth to the silicate disk.",oai:arXiv.org:2110.00222,2021-10-01,['astro-ph.EP']
Knowledge-driven Site Selection via Urban Knowledge Graph,"Site selection determines optimal locations for new stores, which is of
crucial importance to business success. Especially, the wide application of
artificial intelligence with multi-source urban data makes intelligent site
selection promising. However, existing data-driven methods heavily rely on
feature engineering, facing the issues of business generalization and complex
relationship modeling. To get rid of the dilemma, in this work, we borrow ideas
from knowledge graph (KG), and propose a knowledge-driven model for site
selection, short for KnowSite. Specifically, motivated by distilled knowledge
and rich semantics in KG, we firstly construct an urban KG (UrbanKG) with
cities' key elements and semantic relationships captured. Based on UrbanKG, we
employ pre-training techniques for semantic representations, which are fed into
an encoder-decoder structure for site decisions. With multi-relational message
passing and relation path-based attention mechanism developed, KnowSite
successfully reveals the relationship between various businesses and site
selection criteria. Extensive experiments on two datasets demonstrate that
KnowSite outperforms representative baselines with both effectiveness and
explainability achieved.","Title: Utilizing Urban Knowledge Graph for Informed Site Selection: A Knowledge-based Approach

Revised Abstract: The determination of optimal locations for new business ventures, such as stores, is critical to their success. The current landscape of artificial intelligence application, coupled with the availability of multi-source urban data, offers great potential for intelligent site selection. Nevertheless, existing data-driven methodologies are largely dependent on feature engineering, thus posing challenges in business generalization and modeling complex relationships. In response to this, our research introduces a model, referred to as KnowSite, that leverages ideas from knowledge graph (KG) for site selection. Inspired by the distilled knowledge and rich semantics in KG, we initially create an Urban Knowledge Graph (UrbanKG), encapsulating key elements and semantic relationships of cities. Utilizing UrbanKG, we apply pre-training methods for semantic representations, which are subsequently input into an encoder-decoder structure for site selection decisions. KnowSite, enhanced by multi-relational message passing and relation path-based attention mechanism, effectively uncovers the interconnection between diverse businesses and site selection parameters. Rigorous testing on two datasets affirms that KnowSite surpasses comparable baselines in terms of both efficiency and interpretability.",oai:arXiv.org:2111.00787,2021-11-01,['cs.AI']
"Did the Event Horizon Telescope Detect the Base of the Sub-Milliarsecond
  Tubular Jet in M\,87?","A high sensitivity, 7mm Very Long Baseline Array image of M\,87 was
previously analyzed in order to estimate the bulk flow jet velocity between 0.4
and 0.65 mas from the point of origin using the asymmetry between the
well-characterized double-ridged counter-jet (unique to this image) and the
double ridged jet. We use this same image to estimate the cross-sectional area
of this tubular stream. The velocity, acceleration, cross-sectional area and
flux density along this stream determines a unique, perfect magnetohydrodynamic
jet solution that satisfies, conservation of energy, angular momentum and mass
(a monotonic conversion of Poynting flux to kinetic energy flux along the jet).
The solution is protonic and magnetically dominated. The bilateral jet
transports $\approx 1.2\times10^{-4} M_{\odot}/\rm{yr}$ and $\approx
1.1\times10^{42}$ erg/sec, placing strong constraints on the central engine. A
Keplerian disk source that also produces the Event Horizon Telescope (EHT)
annulus of emission can supply the energy and mass if the vertical magnetic
field at the equator is $\sim 1-3.5$ G (depending on location). A Parker spiral
magnetic field, characteristic of a wind or jet, is consistent with the
observed EHT polarization pattern. Even though there is no image of the jet
connecting with the annulus, it is argued that these circumstances are not
coincidental and the polarized portion of the EHT emission is mainly jet
emission in the top layers of the disk that is diluted by emission from an
underlying turbulent disk. This is a contributing factor to the relatively low
polarization levels that were detected.","Title: The Detection of the Sub-Milliarcsecond Tubular Jet Base in M\,87 by the Event Horizon Telescope: An Analysis

Revised Abstract: A highly sensitive image of M\,87, captured at 7mm by the Very Long Baseline Array, was previously scrutinized to deduce the bulk flow jet velocity in the range of 0.4 to 0.65 mas from its source. Utilizing the asymmetry between the uniquely double-ridged counter-jet and the double-ridged jet, the image was re-analyzed to quantify the tubular stream's cross-sectional area. The jet's velocity, acceleration, cross-sectional area, and flux density were used to determine a singular, ideal magnetohydrodynamic jet solution that adheres to the laws of conservation of energy, angular momentum, and mass, signifying a steady conversion of Poynting flux into kinetic energy flux along the jet. The solution is predominantly protonic and magnetic. The bilateral jet carries approximately 1.2x10^-4 M_{☉}/yr and approximately 1.1x10^42 erg/sec, thereby imposing stringent constraints on the central engine. A Keplerian disk source, which also generates the Event Horizon Telescope's (EHT) emission ring, can deliver the required energy and mass, given a vertical magnetic field at the equator of about 1-3.5 G, contingent on location. The EHT's observed polarization pattern aligns with a Parker spiral magnetic field, indicative of a wind or jet. Despite the lack of a visual connection between the jet and the emission ring, it is contended that this is not random, and the EHT's polarized emission majorly stems from jet emission in the disk's upper strata, which is diluted by emission from a turbulent disk beneath. This serves as an explanation for the relatively low detected polarization levels.",oai:arXiv.org:2111.00692,2021-11-01,"['astro-ph.GA', 'astro-ph.HE']"
"Binary Mean Field Stochastic Games: Stationary Equilibria and
  Comparative Statics","This paper considers mean field games in a multi-agent Markov decision
process (MDP) framework. Each player has a continuum state and binary action,
and benefits from the improvement of the condition of the overall population.
Based on an infinite horizon discounted individual cost, we show existence of a
stationary equilibrium, and prove its uniqueness under a positive externality
condition. We further analyze comparative statics of the stationary equilibrium
by quantitatively determining the impact of the effort cost.","Title: An Examination of Binary Mean Field Stochastic Games: Identifying Stationary Equilibria and Analyzing Comparative Statics

Revised Abstract: This study delves into the realm of mean field games, utilizing a multi-agent Markov decision process (MDP) framework. Each participant possesses a continuous state and a binary action, reaping benefits from the enhancement of the overall population's condition. By employing an infinite horizon discounted individual cost, the existence of a stationary equilibrium is demonstrated, and its singularity is established under the presence of a positive externality circumstance. The research extends to assess the comparative statics of this stationary equilibrium, accurately ascertaining the influence of the effort cost.",oai:arXiv.org:2101.00335,2021-01-01,['math.OC']
"PHOENIX: Device-Centric Cellular Network Protocol Monitoring using
  Runtime Verification","End-user-devices in the current cellular ecosystem are prone to many
different vulnerabilities across different generations and protocol layers.
Fixing these vulnerabilities retrospectively can be expensive, challenging, or
just infeasible. A pragmatic approach for dealing with such a diverse set of
vulnerabilities would be to identify attack attempts at runtime on the device
side, and thwart them with mitigating and corrective actions. Towards this
goal, in the paper we propose a general and extendable approach called Phoenix
for identifying n-day cellular network control-plane vulnerabilities as well as
dangerous practices of network operators from the device vantage point. Phoenix
monitors the device-side cellular network traffic for performing
signature-based unexpected behavior detection through lightweight runtime
verification techniques. Signatures in Phoenix can be manually-crafted by a
cellular network security expert or can be automatically synthesized using an
optional component of Phoenix, which reduces the signature synthesis problem to
the language learning from the informant problem. Based on the corrective
actions that are available to Phoenix when an undesired behavior is detected,
different instantiations of Phoenix are possible: a full-fledged defense when
deployed inside a baseband processor; a user warning system when deployed as a
mobile application; a probe for identifying attacks in the wild. One such
instantiation of Phoenix was able to identify all 15 representative n-day
vulnerabilities and unsafe practices of 4G LTE networks considered in our
evaluation with a high packet processing speed (~68000 packets/second) while
inducing only a moderate amount of energy overhead (~4mW).","Title: PHOENIX: Monitoring Cellular Network Protocols via Device-Centered and Real-Time Verification

Revised Abstract: The existing cellular ecosystem exposes end-user devices to a plethora of vulnerabilities across various generations and protocol layers. Retrospective rectification of these vulnerabilities can be costly, complicated, and sometimes unachievable. A practical solution to address such a broad spectrum of vulnerabilities involves detecting attack attempts in real-time from the device side and responding with preventative and rectifying measures. In this paper, we introduce an adaptable methodology named Phoenix. This approach enables the identification of n-day cellular network control-plane vulnerabilities and risky network operator practices from the device perspective. Phoenix scrutinizes the cellular network traffic on the device side, performing signature-based anomaly detection using lightweight real-time verification techniques. Phoenix's signatures can either be manually designed by a cellular network security professional or automatically created using a supplementary Phoenix component, which simplifies the signature creation problem to the informant's language learning issue. Depending on the remedial measures available to Phoenix upon detection of an undesired behavior, various Phoenix implementations are feasible: a comprehensive defense when installed in a baseband processor; a user alert system when utilized as a mobile app; a probe to detect attacks in the wild. An implementation of Phoenix successfully identified all 15 representative n-day vulnerabilities and unsafe 4G LTE network practices in our assessment, demonstrating a high packet processing speed (~68000 packets/second) and only causing a modest energy overhead (~4mW).",oai:arXiv.org:2101.00328,2021-01-01,['cs.CR']
Livestock Monitoring with Transformer,"Tracking the behaviour of livestock enables early detection and thus
prevention of contagious diseases in modern animal farms. Apart from economic
gains, this would reduce the amount of antibiotics used in livestock farming
which otherwise enters the human diet exasperating the epidemic of antibiotic
resistance - a leading cause of death. We could use standard video cameras,
available in most modern farms, to monitor livestock. However, most computer
vision algorithms perform poorly on this task, primarily because, (i) animals
bred in farms look identical, lacking any obvious spatial signature, (ii) none
of the existing trackers are robust for long duration, and (iii) real-world
conditions such as changing illumination, frequent occlusion, varying camera
angles, and sizes of the animals make it hard for models to generalize. Given
these challenges, we develop an end-to-end behaviour monitoring system for
group-housed pigs to perform simultaneous instance level segmentation,
tracking, action recognition and re-identification (STAR) tasks. We present
starformer, the first end-to-end multiple-object livestock monitoring framework
that learns instance-level embeddings for grouped pigs through the use of
transformer architecture. For benchmarking, we present Pigtrace, a carefully
curated dataset comprising video sequences with instance level bounding box,
segmentation, tracking and activity classification of pigs in real indoor
farming environment. Using simultaneous optimization on STAR tasks we show that
starformer outperforms popular baseline models trained for individual tasks.","Title: Implementation of Transformer for Livestock Surveillance

Revised Abstract: The surveillance of livestock behaviour holds the potential for early identification and subsequent mitigation of infectious diseases in contemporary animal farming practices. This not only yields economic benefits but also decreases the use of antibiotics in animal farming, thereby reducing the contribution to the rising issue of antibiotic resistance - a major mortality cause. Standard video cameras prevalent in most modern farms could be utilized for monitoring livestock. Yet, most computer vision algorithms underperform in this area, primarily due to a) the similarity in appearance of farm-bred animals, b) lack of robustness in existing trackers for extended durations, and c) real-world factors like changing light conditions, frequent concealment, varying camera perspectives, and animal size disparities, which hinder model generalization. In response to these challenges, we have devised a comprehensive behaviour monitoring system for group-housed pigs, capable of simultaneous instance level segmentation, tracking, action recognition, and re-identification (STAR) tasks. We introduce starformer, the inaugural end-to-end multiple-object livestock surveillance framework that learns instance-level embeddings for grouped pigs via transformer architecture. For performance evaluation, we unveil Pigtrace, a meticulously assembled dataset featuring video sequences with instance level bounding box, segmentation, tracking, and activity classification of pigs in authentic indoor farming settings. By employing simultaneous optimization on STAR tasks, we demonstrate that starformer surpasses well-known baseline models designed for individual tasks.",oai:arXiv.org:2111.00801,2021-11-01,"['cs.CV', 'cs.AI']"
"Multi-view Clustering with Deep Matrix Factorization and Global Graph
  Refinement","Multi-view clustering is an important yet challenging task in machine
learning and data mining community. One popular strategy for multi-view
clustering is matrix factorization which could explore useful feature
representations at lower-dimensional space and therefore alleviate dimension
curse. However, there are two major drawbacks in the existing work: i) most
matrix factorization methods are limited to shadow depth, which leads to the
inability to fully discover the rich hidden information of original data. Few
deep matrix factorization methods provide a basis for the selection of the new
representation's dimensions of different layers. ii) the majority of current
approaches only concentrate on the view-shared information and ignore the
specific local features in different views. To tackle the above issues, we
propose a novel Multi-View Clustering method with Deep semi-NMF and Global
Graph Refinement (MVC-DMF-GGR) in this paper. Firstly, we capture new
representation matrices for each view by hierarchical decomposition, then learn
a common graph by approximating a combination of graphs which are reconstructed
from these new representations to refine the new representations in return. An
alternate algorithm with proved convergence is then developed to solve the
optimization problem and the results on six multi-view benchmarks demonstrate
the effectiveness and superiority of our proposed algorithm.","Title: Advanced Clustering using Deep Matrix Factorization and Global Graph Refinement in Multiple Views

Redefined Abstract: The execution of multi-view clustering presents a significant, yet complex undertaking within the realms of machine learning and data mining. A widely utilized approach for this process involves matrix factorization, which allows for the extraction of valuable feature representations in a lower-dimensional domain, thereby mitigating the curse of dimensionality. Yet, two major shortcomings persist in current practices: i) the majority of matrix factorization techniques are confined to shallow depth, hindering the comprehensive extraction of the abundant covert information inherent in the original data. There is a scarcity of deep matrix factorization methods that offer solid grounds for choosing the dimensions of the new representation layers. ii) Present methods predominantly focus on information shared across views, often overlooking specific local features unique to individual views. In response to these challenges, we introduce a cutting-edge Multi-View Clustering approach using Deep semi-NMF and Global Graph Refinement (MVC-DMF-GGR). Initially, we generate new representation matrices for each view via a hierarchical breakdown, followed by the learning of a shared graph, achieved by approximating a fusion of graphs reconstructed from these new representations to further refine them. Subsequently, we devise an alternating algorithm with confirmed convergence to resolve the optimization issue. The efficacy and superiority of our proposed algorithm are validated through outcomes on six multi-view benchmarks.",oai:arXiv.org:2105.00248,2021-05-01,['cs.LG']
"ADAADepth: Adapting Data Augmentation and Attention for Self-Supervised
  Monocular Depth Estimation","Self-supervised learning of depth has been a highly studied topic of research
as it alleviates the requirement of having ground truth annotations for
predicting depth. Depth is learnt as an intermediate solution to the task of
view synthesis, utilising warped photometric consistency. Although it gives
good results when trained using stereo data, the predicted depth is still
sensitive to noise, illumination changes and specular reflections. Also,
occlusion can be tackled better by learning depth from a single camera. We
propose ADAA, utilising depth augmentation as depth supervision for learning
accurate and robust depth. We propose a relational self-attention module that
learns rich contextual features and further enhances depth results. We also
optimize the auto-masking strategy across all losses by enforcing L1
regularisation over mask. Our novel progressive training strategy first learns
depth at a lower resolution and then progresses to the original resolution with
slight training. We utilise a ResNet18 encoder, learning features for
prediction of both depth and pose. We evaluate our predicted depth on the
standard KITTI driving dataset and achieve state-of-the-art results for
monocular depth estimation whilst having significantly lower number of
trainable parameters in our deep learning framework. We also evaluate our model
on Make3D dataset showing better generalization than other methods.","Title: ADAADepth: Enhancing Monocular Depth Estimation through Adaptive Data Augmentation and Attention Mechanism

Revised Abstract: The self-directed learning approach in depth understanding is a prevalent research field, given that it negates the need for ground truth labels in depth prediction. The technique leverages warped photometric uniformity to learn depth as an intermediate solution in view synthesis. While this method performs well with stereo data, the estimated depth remains susceptible to variations in lighting, noise, and specular reflections. Moreover, learning depth from a single camera can better address occlusion issues. We put forth ADAA, an approach that employs depth augmentation for supervising depth learning to yield precise and robust depth. We suggest a relational self-attention module that harnesses rich contextual features to improve depth results. We further refine the auto-masking approach across all losses via L1 regularisation on the mask. Our unique progressive training approach initially learns depth at a lower resolution and then advances to the original resolution with minimal training. We employ a ResNet18 encoder to learn features for predicting both depth and pose. We test our depth prediction on the benchmark KITTI driving dataset, achieving unparalleled results for monocular depth estimation with a significantly reduced number of trainable parameters in our deep learning model. We also assess our model on the Make3D dataset, demonstrating superior generalization compared to other techniques.",oai:arXiv.org:2103.00853,2021-03-01,['cs.CV']
"NodeSim: Node Similarity based Network Embedding for Diverse Link
  Prediction","In real-world complex networks, understanding the dynamics of their evolution
has been of great interest to the scientific community. Predicting future links
is an essential task of social network analysis as the addition or removal of
the links over time leads to the network evolution. In a network, links can be
categorized as intra-community links if both end nodes of the link belong to
the same community, otherwise inter-community links. The existing
link-prediction methods have mainly focused on achieving high accuracy for
intra-community link prediction. In this work, we propose a network embedding
method, called NodeSim, which captures both similarities between the nodes and
the community structure while learning the low-dimensional representation of
the network. The embedding is learned using the proposed NodeSim random walk,
which efficiently explores the diverse neighborhood while keeping the more
similar nodes closer in the context of the node. We verify the efficacy of the
proposed embedding method over state-of-the-art methods using diverse link
prediction. We propose a machine learning model for link prediction that
considers both the nodes' embedding and their community information to predict
the link between two given nodes. Extensive experimental results on several
real-world networks demonstrate the effectiveness of the proposed framework for
both inter and intra-community link prediction.","Title: NodeSim: A Novel Approach for Network Embedding Using Node Similarity for Varied Link Prediction

Revised Abstract: The scientific community has exhibited profound interest in comprehending the dynamic evolution of real-world complex networks. A crucial aspect of social network analysis involves predicting future links as the network's evolution is dictated by the continual addition or subtraction of these links. Links within a network can be classified into intra-community links when both link endpoints belong to the same community, and inter-community links when they do not. Previous link-prediction methodologies have primarily concentrated on enhancing the accuracy of intra-community link predictions. This study introduces a network embedding method, NodeSim, which concurrently captures node similarities and community structure while determining the network's low-dimensional representation. NodeSim employs a unique random walk approach to efficiently navigate the diverse neighborhood, ensuring nodes with higher similarity remain proximate within the node context. The effectiveness of NodeSim is validated through diverse link prediction, outperforming contemporary methods. Additionally, this study presents a machine learning model for link prediction that incorporates both node embedding and community data to predict a link between any two nodes. Comprehensive experimental outcomes from multiple real-world networks substantiate the efficiency of the proposed framework in predicting both inter and intra-community links.",oai:arXiv.org:2102.00785,2021-02-01,['cs.SI']
"Exact verification of the strong BSD conjecture for some absolutely
  simple abelian surfaces","Let $X$ be one of the $28$ Atkin-Lehner quotients of a curve $X_0(N)$ such
that $X$ has genus $2$ and its Jacobian variety $J$ is absolutely simple. We
show that the Shafarevich-Tate group of $J/\mathbb{Q}$ is trivial. This
verifies the strong BSD conjecture for $J$.","Title: Precise Validation of the Robust BSD Hypothesis for Certain Uncomplicated Abelian Surfaces

Generated Abstract: Assume $X$ to be one among the $28$ Atkin-Lehner divisions of a curve represented as $X_0(N)$. In instances where $X$ possesses a genus of $2$ and the Jacobian variety $J$ is completely simple, we demonstrate that the Shafarevich-Tate group pertaining to $J/\mathbb{Q}$ is insignificant. This provides validation for the robust BSD supposition in relation to $J$.",oai:arXiv.org:2107.00325,2021-07-01,"['math.NT', 'math.AG']"
Bayesian Agency: Linear versus Tractable Contracts,"We study principal-agent problems in which a principal commits to an
outcome-dependent payment scheme (a.k.a. contract) so as to induce an agent to
take a costly, unobservable action. We relax the assumption that the principal
perfectly knows the agent by considering a Bayesian setting where the agent's
type is unknown and randomly selected according to a given probability
distribution, which is known to the principal. Each agent's type is
characterized by her own action costs and action-outcome distributions. In the
literature on non-Bayesian principal-agent problems, considerable attention has
been devoted to linear contracts, which are simple, pure-commission payment
schemes that still provide nice approximation guarantees with respect to
principal-optimal (possibly non-linear) contracts. While in non-Bayesian
settings an optimal contract can be computed efficiently, this is no longer the
case for our Bayesian principal-agent problems. This further motivates our
focus on linear contracts, which can be optimized efficiently given their
single-parameter nature. Our goal is to analyze the properties of linear
contracts in Bayesian settings, in terms of approximation guarantees with
respect to optimal contracts and general tractable contracts (i.e.,
efficiently-computable ones). First, we study the approximation guarantees of
linear contracts with respect to optimal ones, showing that the former suffer
from a multiplicative loss linear in the number of agent's types. Nevertheless,
we prove that linear contracts can still provide a constant multiplicative
approximation $\rho$ of the optimal principal's expected utility, though at the
expense of an exponentially-small additive loss $2^{-\Omega(\rho)}$. Then, we
switch to tractable contracts, showing that, surprisingly, linear contracts
perform well among them.","Title: Comparative Analysis of Linear and Tractable Contracts in a Bayesian Agency Framework

Revised Abstract: This research paper investigates principal-agent dilemmas where the principal establishes an outcome-dependent remuneration plan (also known as a contract) to encourage an agent to perform a costly, unseen action. We move away from the notion that the principal has complete knowledge of the agent and instead consider a Bayesian environment where the agent's type is undetermined and chosen randomly based on a known probability distribution. The agent's type is defined by the cost of their actions and the distribution of action outcomes. Significant emphasis has been placed on linear contracts in non-Bayesian principal-agent problem studies. These contracts are simple, commission-based payment plans that offer sound approximation assurances compared to optimal principal contracts, which may not necessarily be linear. However, the efficient computation of an optimal contract in non-Bayesian scenarios does not apply to our Bayesian principal-agent issues, further justifying our focus on linear contracts due to their single-parameter nature. We aim to explore the characteristics of linear contracts within Bayesian contexts, particularly in relation to approximation assurances compared to optimal contracts and general tractable contracts (those that can be computed efficiently). Initially, we examine the approximation assurances of linear contracts compared to optimal contracts, revealing that the former incurs a multiplicative loss linear to the number of agent types. Despite this, we establish that linear contracts can maintain a constant multiplicative approximation of the optimal principal's expected utility, albeit with an exponentially small additive loss. Finally, we consider tractable contracts and demonstrate that linear contracts perform well when compared to them.",oai:arXiv.org:2106.00319,2021-06-01,['cs.GT']
"Inverse reinforcement learning for autonomous navigation via
  differentiable semantic mapping and planning","This paper focuses on inverse reinforcement learning for autonomous
navigation using distance and semantic category observations. The objective is
to infer a cost function that explains demonstrated behavior while relying only
on the expert's observations and state-control trajectory. We develop a map
encoder, that infers semantic category probabilities from the observation
sequence, and a cost encoder, defined as a deep neural network over the
semantic features. Since the expert cost is not directly observable, the model
parameters can only be optimized by differentiating the error between
demonstrated controls and a control policy computed from the cost estimate. We
propose a new model of expert behavior that enables error minimization using a
closed-form subgradient computed only over a subset of promising states via a
motion planning algorithm. Our approach allows generalizing the learned
behavior to new environments with new spatial configurations of the semantic
categories. We analyze the different components of our model in a minigrid
environment. We also demonstrate that our approach learns to follow traffic
rules in the autonomous driving CARLA simulator by relying on semantic
observations of buildings, sidewalks, and road lanes.","Title: Autonomous Navigation through Inverse Reinforcement Learning using Differentiable Semantic Mapping and Planning

Revised Abstract: This study addresses the application of inverse reinforcement learning in autonomous navigation, utilizing both distance and semantic category observations. The goal is to decipher a cost function that elucidates displayed behavior, depending entirely on observations made by the expert and the trajectory of state-control. We introduce a map encoder that deduces semantic category probabilities from the sequence of observations, alongside a cost encoder, characterized as a deep neural network spanning the semantic features. With the expert cost being unobservable directly, the optimization of model parameters can only be achieved by differentiating the discrepancy between demonstrated controls and a control policy derived from the cost estimation. We advocate a novel model of expert behavior which facilitates error reduction by employing a closed-form subgradient, calculated solely over a selection of promising states via a motion planning algorithm. Our methodology enables the extrapolation of the acquired behavior to fresh environments featuring new spatial arrangements of semantic categories. We evaluate the distinct elements of our model within a minigrid environment and substantiate that our method is capable of adhering to traffic regulations in the CARLA simulator for autonomous driving, dependent on semantic observations of structures, sidewalks, and road lanes.",oai:arXiv.org:2101.00186,2021-01-01,"['cs.LG', 'cs.RO']"
"A Separable Temporal Convolution Neural Network with Attention for
  Small-Footprint Keyword Spotting","Keyword spotting (KWS) on mobile devices generally requires a small memory
footprint. However, most current models still maintain a large number of
parameters in order to ensure good performance. To solve this problem, this
paper proposes a separable temporal convolution neural network with attention,
it has a small number of parameters. Through the time convolution combined with
attention mechanism, a small number of parameters model (32.2K) is implemented
while maintaining high performance. The proposed model achieves 95.7% accuracy
on the Google Speech Commands dataset, which is close to the performance of
Res15(239K), the state-of-the-art model in KWS at present.","Title: Implementing a Temporally Separable Convolution Neural Network with Attention for Efficient Keyword Spotting

Revised Abstract: The challenge of implementing keyword spotting (KWS) on mobile platforms typically lies in balancing the small memory footprint requirement with the need for performance efficiency. Existing models tend to retain a high number of parameters to optimize performance, leading to significant memory usage. This study presents a novel approach through a temporally separable convolution neural network imbued with an attention mechanism, characterized by a minimized parameter count. Utilizing temporal convolution in conjunction with an attention mechanism, the model maintains high performance while implementing a significantly reduced parameter model (32.2K). With a remarkable accuracy of 95.7% on the Google Speech Commands dataset, the proposed model's performance closely rivals that of the current state-of-the-art KWS model, Res15 (with 239K parameters).",oai:arXiv.org:2109.00260,2021-09-01,"['cs.SD', 'eess.AS']"
Pseudo-Spherical Contrastive Divergence,"Energy-based models (EBMs) offer flexible distribution parametrization.
However, due to the intractable partition function, they are typically trained
via contrastive divergence for maximum likelihood estimation. In this paper, we
propose pseudo-spherical contrastive divergence (PS-CD) to generalize maximum
likelihood learning of EBMs. PS-CD is derived from the maximization of a family
of strictly proper homogeneous scoring rules, which avoids the computation of
the intractable partition function and provides a generalized family of
learning objectives that include contrastive divergence as a special case.
Moreover, PS-CD allows us to flexibly choose various learning objectives to
train EBMs without additional computational cost or variational minimax
optimization. Theoretical analysis on the proposed method and extensive
experiments on both synthetic data and commonly used image datasets demonstrate
the effectiveness and modeling flexibility of PS-CD, as well as its robustness
to data contamination, thus showing its superiority over maximum likelihood and
$f$-EBMs.","Title: An Advanced Approach to Contrastive Divergence: The Pseudo-Spherical Technique

Reinterpreted Abstract: Flexible distribution parameterization is a key advantage of Energy-based models (EBMs), although their use is typically hampered by the unmanageable partition function, leading to the adoption of contrastive divergence for maximum likelihood approximation. This paper presents a novel approach - the pseudo-spherical contrastive divergence (PS-CD) - which expands the scope of maximum likelihood training of EBMs. PS-CD is the result of maximizing a series of strictly proper homogeneous scoring rules, thereby circumventing the need to compute the complex partition function. This method also introduces a comprehensive suite of learning objectives, incorporating contrastive divergence as a particular instance. PS-CD offers the flexibility to select diverse learning objectives for EBM training, without incurring extra computational expenses or requiring variational minimax optimization. Detailed theoretical examination of our proposed technique and numerous experiments on both synthetic data and widely recognized image datasets underscore the efficiency, versatility, and robustness of PS-CD, even in the presence of data contamination. These results highlight its superiority over existing methods such as maximum likelihood and $f$-EBMs.",oai:arXiv.org:2111.00780,2021-11-01,['cs.LG']
Extending Harvey's Surface Kernel Maps,"Let $S$ be a compact Riemann surface and $G$ a group of conformal
automorphisms of $S$ with $S_0 = S/G$. $S$ is a finite regular branched cover
of $S_0$. If $U$ denotes the unit disc, let $\Gamma$ and $\Gamma_0$ be the
Fuchsian groups with $S = U/{\Gamma}$ and $S_0 = U/{\Gamma_0}$. There is a
group homomorphism of $\Gamma_0$ onto $G$ with kernel $\Gamma$ and this is
termed a surface kernel map. Two surface kernel maps are equivalent if they
differ by an automorphism of $\Gamma_0$. In his 1971 paper Harvey showed that
when $G$ is a cyclic group, there is a unique simplest representative for this
equivalence class. His result has played an important role in establishing
subsequent results about conformal automorphism groups of surfaces. We extend
his result to some surface kernel maps onto arbitrary finite groups. These can
be used along with the Schreier-Reidemeister Theory to find a set of generators
for $\Gamma$ and the action of $G$ as an outer automorphism group on the
fundamental group of $S$ putting the action on the fundamental group and the
induced action on homology into a relatively simple format. As an example we
compute generators for the fundamental group and a homology basis together with
the action of $G$ when $G$ is ${\mathcal{S}_3$, the symmetric group on three
letters. The action of $G$ shows that the homology basis found is not an
adapted homology basis.","Title: Broadening the Scope of Harvey's Surface Kernel Maps

Revised Abstract: Consider a compact Riemann surface, $S$, and a conformal automorphism group, $G$, such that $S_0 = S/G$. This configuration represents $S$ as a regular finite branched cover over $S_0$. Given the unit disc, $U$, the Fuchsian groups $\Gamma$ and $\Gamma_0$ are defined with $S = U/{\Gamma}$ and $S_0 = U/{\Gamma_0}$. A group homomorphism from $\Gamma_0$ onto $G$ with kernel $\Gamma$ is classified as a surface kernel map. Two such maps are considered equivalent if a $\Gamma_0$ automorphism separates them. In 1971, Harvey demonstrated that a unique simplest representative exists for this equivalence class when $G$ represents a cyclic group. This finding has proven foundational in the development of further results concerning conformal automorphism groups of surfaces. The present work broadens Harvey's finding to certain surface kernel maps onto arbitrary finite groups. This expansion facilitates the application of Schreier-Reidemeister Theory to determine a set of generators for $\Gamma$ and the operation of $G$ as an outer automorphism group on $S$'s fundamental group. This places the action on the fundamental group and the subsequent homology action into a more comprehensible context. An illustrative example is provided whereby the generators for the fundamental group and a homology basis are computed in conjunction with the action of $G$ when $G$ is $\mathcal{S}_3$ - the symmetric group on three letters. The $G$ action reveals that the discovered homology basis is not an adapted homology basis.",oai:arXiv.org:2105.00161,2021-05-01,['math.GR']
"Families of hybridizable interior penalty discontinuous Galerkin methods
  for degenerate advection-diffusion-reaction problems","We analyze families of primal high-order hybridizable discontinuous Galerkin
(HDG) methods for solving degenerate (second-order) elliptic problems. One
major trouble regarding this class of PDEs concerns its mathematical nature,
which may be nonuniform over the domain. Due to the local degeneracy of the
diffusion term, it can be purely hyperbolic in a subregion and elliptic in the
rest. This problem is thus quite delicate to solve since the exact solution is
discontinuous at interfaces separating both elliptic and hyperbolic parts. The
proposed HDG method is developed in a unified and compact fashion. It can
efficiently handle pure diffusive or advective regimes and intermediate regimes
that combine the above mechanisms for a wide range of P\'eclet numbers,
including the delicate situation of local evanescent diffusion. To this end, an
adaptive stabilization strategy based on the addition of jump-penalty terms is
then considered. A $\theta$-upwind-based scheme is favored for the hyperbolic
region, and an inspired Scharfetter--Gummel-based technique is preferred for
the elliptic region. The well-posedness of the HDG method is also discussed by
analyzing the consistency and discrete coercivity properties. Extensive
numerical experiments are finally considered to verify the model's robustness
for all the abovementioned regimes.","Title: Investigation into Hybridizable Discontinuous Galerkin Methods for Degenerate Advection-Diffusion-Reaction Issues

New Abstract: This study delves into the analysis of a group of primal high-order hybridizable discontinuous Galerkin (HDG) methodologies for addressing degenerate second-order elliptic challenges. A significant concern with this type of PDEs is their inherent mathematical complexity, which may exhibit non-uniformity throughout the domain. The local degeneracy in the diffusion term can create a purely hyperbolic subregion and an elliptic region elsewhere. The complexity of finding a solution is increased due to the discontinuity at the interfaces that separate the elliptic and hyperbolic sections. The HDG method proposed herein is structured in a unified and compact manner, capable of effectively managing pure diffusive or advective regimes and mixed regimes across a broad spectrum of Péclet numbers, inclusive of localized evanescent diffusion scenarios. An adjustable stabilization approach, incorporating jump-penalty terms, is therefore proposed. A θ-upwind-based scheme is prioritized for the hyperbolic area, while a Scharfetter–Gummel-inspired method is favoured for the elliptic area. The well-posedness of the HDG method is deliberated upon by scrutinizing its consistency and discrete coercivity characteristics. Extensive numerical testing is performed to affirm the model's reliability for all mentioned regimes.",oai:arXiv.org:2106.00226,2021-06-01,"['math.NA', 'cs.NA', 'math.AP']"
"A Modified Dynamic Time Warping (MDTW) Approach and Innovative Average
  Non-Self Match Distance (ANSD) Method for Anomaly Detection in ECG Recordings","ECGs objectively reflects the working conditions of the hearts as these
signals contain vast physiological and pathological information. In this work,
in order to improve the efficiency and accuracy of ""best so far"" time series
analysis-based ECG anomaly detection methods, a novel method, comprising a
modified dynamic time warping (MDTW) and an innovative average non-self match
distance (ANSD) measure, is proposed for ECG anomaly detection. To evaluate the
performance of the proposed method, the proposed method is applied to real ECG
data selected from the MIT-BIH heartbeat database. To provide a reference for
comparison, two existing anomaly detection methods, namely, brute force discord
discovery (BFDD) and adaptive window discord discovery (AWDD), are also applied
to the same data. The experimental results show that our proposed method
outperforms BFDD and AWD.","Title: Enhanced Efficiency and Accuracy in ECG Anomaly Detection: A Novel Method Combining Modified Dynamic Time Warping (MDTW) and Average Non-Self Match Distance (ANSD)

Revised Abstract: 
The Electrocardiogram (ECG) is a vital tool in the evaluation of heart performance, providing a wealth of both physiological and pathological data. This study introduces a new method for the detection of anomalies in ECG records to augment the efficacy and precision of existing time series analysis-based detection techniques. This novel approach utilizes a modified version of dynamic time warping (MDTW) combined with an innovative measure of average non-self match distance (ANSD). The efficacy of the newly proposed method was assessed by applying it to real ECG data from the MIT-BIH heartbeat database. In order to benchmark the results, the same data were also subjected to two current anomaly detection techniques: brute force discord discovery (BFDD) and adaptive window discord discovery (AWDD). The empirical findings demonstrate that the newly developed approach surpasses the performance of both BFDD and AWDD in detecting anomalies.",oai:arXiv.org:2111.00803,2021-11-01,['eess.SP']
Enhanced Multigradient Dilution Preparation,"Abstract: In our paper the new algorithm enhanced multi gradient Dilution
Preparation (EMDP) is discussed. This new algorithm is reported with a lab on
chip or digital Microfluidic biochip to operate multiple operation on a tiny
chip. We can use Digital Microfluidic biochip to operate multiple operation on
a tiny chip. Samples are very costly which are used in any Biochemical
laboratory Protocols. For the case of fast and high throughput application, It
is essential to minimize the cost of operations and the time of operations and
that is why one of the most challenging and important phase is sample
preparation. In our proposed algorithm, we have hide to reduce sample droplets
and waste droplets and for this purpose waste recycling is used, when different
series of multi gradient targets concentration factors (CFS) are generated. We
have compared our proposed algorithm with recent dilution techniques such as
MTC, REMIA, and WARA. For the storage of intermediate droplets which, and
generated during this process, on chip storage space 0(n) is needed. Key words:
Digital microfluidic Biochip, Drug discovery, sample preparation, Electro
wetting.","Title: Advancements in Multigradient Dilution Preparation: An Enhanced Approach

Revised Abstract: This study elaborates on a novel algorithm termed as Enhanced Multigradient Dilution Preparation (EMDP). This innovative algorithm is applied using a lab-on-chip or digital Microfluidic biochip, enabling the execution of manifold operations on a minuscule chip. The digital Microfluidic biochip offers an invaluable solution to perform multiple tasks on a compact chip. Given that the specimens used in any Biochemical lab protocols are highly expensive, it becomes crucial, especially for rapid and high throughput applications, to cut down the operational costs and time. Therefore, sample preparation emerges as a critical and challenging phase. Our proposed algorithm is designed to decrease the number of sample droplets and waste droplets, incorporating waste recycling when different sequences of multigradient target concentration factors (CFS) are produced. We evaluated our proposed algorithm against recent dilution methods like MTC, REMIA, and WARA. To store the intermediate droplets produced during this process, an on-chip storage space of 0(n) is required. Key words: Digital Microfluidic Biochip, Drug Discovery, Sample Preparation, Electrowetting.",oai:arXiv.org:2110.00232,2021-10-01,"['cs.ET', 'cs.AR']"
"Towards Utilitarian Combinatorial Assignment with Deep Neural Networks
  and Heuristic Algorithms","This paper presents preliminary work on using deep neural networks to guide
general-purpose heuristic algorithms for performing utilitarian combinatorial
assignment. In more detail, we use deep learning in an attempt to produce
heuristics that can be used together with e.g., search algorithms to generate
feasible solutions of higher quality more quickly. Our results indicate that
our approach could be a promising future method for constructing such
heuristics.","Title: Progressing Towards Functional Combinatorial Assignments via Deep Neural Networks and Heuristic Algorithms

Revised Abstract: This study introduces initial investigations into the application of deep neural networks in steering universal heuristic algorithms for executing utilitarian combinatorial assignment. Specifically, we employ deep learning with the aim of developing heuristics that can complement search algorithms to rapidly produce superior quality feasible solutions. Our findings suggest that this method holds potential to be a future-effective strategy for the creation of such heuristics.",oai:arXiv.org:2107.00317,2021-07-01,['cs.AI']
Embarras de richesses in non-DLVO colloid interactions,"In its original formulation, the seminal Deryaguin-Landau-Verwey-Overbeek
(DLVO) theory of colloidal stability seemed like a simple but realistic
description of the world of colloid interactions in electrolyte solutions. It
is based on a straightforward superposition of the mean-field Poisson-Boltzmann
(PB) electrostatics with the electrodynamic van der Waals (vdW) interactions
driven by thermal and quantum fluctuations. However, subsequent developments
continued to reveal a much richer and deeper structure of fundamental
interactions on the nano- and micro-scale: the granularity and structure of the
solvent, charging equilibria of dissociable charge groups, inhomogeneous charge
distributions, the finite size of the ions, non-mean-field electrostatics,
ion-ion correlations, and more. Today, the original simplicity is gone and we
are left with an embarrassingly rich variety of interactions that defy simple
classification and reduction to a few fundamental mechanisms. In this
mini-review, we comment on the contemporary state-of-the-art picture of
colloidal interactions, in view of some recent progress in experiments.","Title: The Complexity of Non-DLVO Colloid Interactions: A Profusion of Wealth

Revised Abstract: Initially, the Deryaguin-Landau-Verwey-Overbeek (DLVO) theory provided a seemingly straightforward and realistic interpretation of colloidal stability in electrolyte solutions, integrating the Poisson-Boltzmann (PB) mean-field electrostatics and thermally and quantum fluctuation-driven van der Waals (vdW) interactions. However, continuing advancements have uncovered an increasingly intricate and profound framework of fundamental interactions at the nano and micro scales. These complexities encompass the granularity and structure of the solvent, charge equilibria of dissociable charge groups, uneven charge distributions, ion size considerations, non-mean-field electrostatics, and ion-ion correlations among others. The initial simplicity of the theory has been replaced by an overwhelmingly abundant array of interactions that resist simplistic categorization and reduction to basic mechanisms. This condensed review provides commentary on the current advanced understanding of colloidal interactions, taking into account recent experimental advancements.",oai:arXiv.org:2101.00187,2021-01-01,"['cond-mat.soft', 'cond-mat.stat-mech', 'physics.chem-ph']"
Stellar and accretion disk parameters of the close binary HD 50526,"We present a photometric and spectroscopic study of HD 50526, an ellipsoidal
binary member of the group Double Periodic Variable stars. Performing
data-mining in photometric surveys and conducting new spectroscopic
observations with several spectrographs during 2008 to 2015, we obtained
orbital and stellar parameters of the system. The radial velocities were
analyzed with the genetic PIKAIA algorithm, whereas Doppler tomography maps for
the H$\alpha$ and H$\beta$ lines were constructed with the Total Variation
Minimization code. An optimized simplex-algorithm was used to solve the
inverse-problem adjusting the light curve with the best stellar parameters for
the system. We find an orbital period of $6.701 \pm 0.001 ~\mathrm{d}$ and a
long photometric cycle of $191 \pm 2 ~\mathrm{d}$. We detected the spectral
features of the coldest star, and modeled it with a $\log{g} = 2.79 \pm 0.02
~\mathrm{dex}$ giant of mass $1.13 \pm 0.02 ~\mathrm{M_{\odot}}$ and effective
temperature $10500 \pm 125 ~\mathrm{K}$. In addition, we determine a mass ratio
$q= 0.206 \pm 0.033$ and that the hot star is a B-type dwarf of mass $5.48 \pm
0.02 ~\mathrm{M_{\odot}}$. The $V$-band orbital light curve can be modeled
including the presence of an accretion disk around the hotter star. This fills
the Roche lobe of the hotter star, and has a radius $14.74 \pm 0.02
~\mathrm{R_{\odot}}$ and temperature at the outer edge $9400 ~\mathrm{K}$. Two
bright spots located in the disk account for the global morphology of the light
curve. The Doppler tomography maps of H$\alpha$ and H$\beta$, reveal complex
structures of mass fluxes in the system.","Title: Analysis of Stellar and Accretion Disk Characteristics in the Close Binary HD 50526
Generated Abstract: This paper details a comprehensive photometric and spectroscopic exploration of HD 50526, a binary star system exhibiting ellipsoidal variations, grouped under Double Periodic Variable stars. Through meticulous data extraction from photometric surveys and fresh spectroscopic observations gathered from multiple spectrographs from 2008 to 2015, we were able to ascertain both the orbital and stellar properties of the system. The PIKAIA genetic algorithm was employed to scrutinize radial velocities, while the Doppler tomography maps for the Hα and Hβ lines were generated with the help of the Total Variation Minimization code. To resolve the inverse problem, we utilized an optimized simplex algorithm, aligning the light curve with the optimal stellar parameters of the system. Our findings indicated an orbital period of $6.701 \pm 0.001 ~\mathrm{d}$ and a protracted photometric cycle of $191 \pm 2 ~\mathrm{d}$. The spectral features of the cooler star were identified and modeled as a giant with a $\log{g} = 2.79 \pm 0.02 ~\mathrm{dex}$, mass of $1.13 \pm 0.02 ~\mathrm{M_{\odot}}$ and an effective temperature of $10500 \pm 125 ~\mathrm{K}$. Moreover, we established a mass ratio of $q= 0.206 \pm 0.033$ and characterized the hotter star as a B-type dwarf with a mass of $5.48 \pm 0.02 ~\mathrm{M_{\odot}}$. The $V$-band orbital light curve was successfully modeled considering the existence of an accretion disk around the hotter star, occupying the Roche lobe of the star, with a radius of $14.74 \pm 0.02 ~\mathrm{R_{\odot}}$ and an outer edge temperature of $9400 ~\mathrm{K}$. The overall light curve morphology could be explained by two luminous spots located on the disk. The Doppler tomography maps of Hα and Hβ disclose intricate mass flux structures within the system.",oai:arXiv.org:2109.00231,2021-09-01,['astro-ph.SR']
Industry Practice of Coverage-Guided Enterprise-Level DBMS Fuzzing,"As an infrastructure for data persistence and analysis, Database Management
Systems (DBMSs) are the cornerstones of modern enterprise software. To improve
their correctness, the industry has been applying blackbox fuzzing for decades.
Recently, the research community achieved impressive fuzzing gains using
coverage guidance. However, due to the complexity and distributed nature of
enterprise-level DBMSs, seldom are these researches applied to the industry.
  In this paper, we apply coverage-guided fuzzing to enterprise-level DBMSs
from Huawei and Bloomberg LP. In our practice of testing GaussDB and Comdb2, we
found major challenges in all three testing stages. The challenges are
collecting precise coverage, optimizing fuzzing performance, and analyzing root
causes. In search of a general method to overcome these challenges, we propose
Ratel, a coverage-guided fuzzer for enterprise-level DBMSs. With its
industry-oriented design, Ratel improves the feedback precision, enhances the
robustness of input generation, and performs an on-line investigation on the
root cause of bugs. As a result, Ratel outperformed other fuzzers in terms of
coverage and bugs. Compared to industrial black box fuzzers SQLsmith and
SQLancer, as well as coverage-guided academic fuzzer Squirrel, Ratel covered
38.38%, 106.14%, 583.05% more basic blocks than the best results of other three
fuzzers in GaussDB, PostgreSQL, and Comdb2, respectively. More importantly,
Ratel has discovered 32, 42, and 5 unknown bugs in GaussDB, Comdb2, and
PostgreSQL.","Title: Implementation and Evaluation of Ratel: A Coverage-Guided Fuzzer for Enterprise-Level DBMSs

Revised Abstract: Database Management Systems (DBMSs) form the backbone of contemporary enterprise software, functioning as platforms for data retention and scrutiny. To enhance their accuracy, blackbox fuzzing has been a long-established industrial practice. Recently, impressive advancements in fuzzing have been made through coverage guidance by the research fraternity. Nevertheless, the intricate and fragmented nature of enterprise-level DBMSs often precludes the application of these research findings in the industry. 
This study explores the application of coverage-guided fuzzing to enterprise-level DBMSs, specifically GaussDB and Comdb2 utilised by Huawei and Bloomberg LP. The investigation encountered significant hurdles in the three crucial stages of testing: gathering accurate coverage, boosting fuzzing efficiency, and root cause analysis. To address these issues, the paper introduces Ratel, a novel coverage-guided fuzzer specifically designed for enterprise-level DBMSs. Ratel improves feedback accuracy, strengthens the generation of robust input, and conducts real-time investigations to identify bug root causes, thereby outperforming other fuzzers in terms of coverage and bug identification. When compared to industry standard black box fuzzers such as SQLsmith and SQLancer, and the coverage-guided academic fuzzer Squirrel, Ratel demonstrated superior performance, covering 38.38%, 106.14%, 583.05% more basic blocks in GaussDB, PostgreSQL, and Comdb2 respectively. Moreover, Ratel was successful in identifying 32, 42, and 5 previously undiscovered bugs in GaussDB, Comdb2, and PostgreSQL correspondingly.",oai:arXiv.org:2103.00804,2021-03-01,['cs.SE']
"Multiple solutions for a class of quasilinear problems with double
  criticality","We establish multiplicity results for the following class of quasilinear
problems $$ \left\{ \begin{array}{l} -\Delta_{\Phi}u=f(x,u) \quad \mbox{in}
\quad \Omega, \\ u=0 \quad \mbox{on} \quad \partial \Omega, \end{array} \right.
\leqno{(P)} $$ where $\Delta_{\Phi}u=\text{div}(\varphi(x,|\nabla u|)\nabla u)$
for a generalized N-function $\Phi(x,t)=\int_{0}^{|t|}\varphi(x,s)s\,ds$. We
consider $\Omega\subset\mathbb{R}^N$ to be a smooth bounded domain that
contains two disjoint open regions $\Omega_N$ and $\Omega_p$ such that
$\overline{\Omega_N}\cap\overline{\Omega_p}=\emptyset$. The main feature of the
problem $(P)$ is that the operator $-\Delta_{\Phi}$ behaves like $-\Delta_N$ on
$\Omega_N$ and $-\Delta_p$ on $\Omega_p$. We assume the nonlinearity
$f:\Omega\times\mathbb{R}\to\mathbb{R}$ of two different types, but both
behaves like $e^{\alpha|t|^\frac{N}{N-1}}$ on $\Omega_N$ and $|t|^{p^*-2}t$ on
$\Omega_p$ as $|t|$ is large enough, for some $\alpha>0$ and
$p^*=\frac{Np}{N-p}$ being the critical Sobolev exponent for $1<p<N$. In this
context, for one type of nonlinearity $f$, we provide multiplicity of solutions
in a general smooth bounded domain and for another type of nonlinearity $f$, in
an annular domain $\Omega$, we establish existence of multiple solutions for
the problem $(P)$ that are nonradial and rotationally nonequivalent.","Title: Numerous Solutions for a Category of Quasilinear Problems with Double Criticality

Revised Abstract: This study demonstrates the existence of multiple solutions for a given set of quasilinear problems, represented as $$ \left\{ \begin{array}{l} -\Delta_{\Phi}u=f(x,u) \quad \mbox{in}
\quad \Omega, \\ u=0 \quad \mbox{on} \quad \partial \Omega, \end{array} \right.
\leqno{(P)} $$ where $\Delta_{\Phi}u=\text{div}(\varphi(x,|\nabla u|)\nabla u)$
is derived from an extended N-function $\Phi(x,t)=\int_{0}^{|t|}\varphi(x,s)s\,ds$. Our focus is on $\Omega\subset\mathbb{R}^N$, a smooth bounded domain inclusive of two separate open regions $\Omega_N$ and $\Omega_p$ such that
$\overline{\Omega_N}\cap\overline{\Omega_p}=\emptyset$. A significant characteristic of the problem $(P)$ is the operator $-\Delta_{\Phi}$'s behavior, which resembles $-\Delta_N$ on
$\Omega_N$ and $-\Delta_p$ on $\Omega_p$. The nonlinearity
$f:\Omega\times\mathbb{R}\to\mathbb{R}$ is contemplated to be of two distinct types, both behaving as $e^{\alpha|t|^\frac{N}{N-1}}$ on $\Omega_N$ and $|t|^{p^*-2}t$ on
$\Omega_p$ when $|t|$ is sufficiently large, given $\alpha>0$ and
$p^*=\frac{Np}{N-p}$ as the critical Sobolev exponent for $1<p<N$. In this setting, we identify multiple solutions within a general smooth bounded domain for one type of nonlinearity $f$, and for another type of nonlinearity $f$, within an annular domain $\Omega$, we confirm the existence of multiple, non-radial, and rotationally nonequivalent solutions for the problem $(P)$.",oai:arXiv.org:2107.00331,2021-07-01,['math.AP']
"Achromatic photonic tricouplers for application in nulling
  interferometry","Integrated-optic components are being increasingly used in astrophysics,
mainly where accuracy and precision are paramount. One such emerging technology
is nulling interferometry that targets high contrast and high angular
resolution. Two of the most critical limitations encountered by nullers are
rapid phase fluctuations in the incoming light causing instability in the
interference and chromaticity of the directional couplers that prevent a deep
broadband interferometric null. We explore the use of a tricoupler designed by
ultrafast laser inscription that solves both issues. Simulations of a
tricoupler, incorporated into a nuller, result in order of a magnitude
improvement in null depth.","Title: Utilization of Achromatic Photonic Tricouplers in Nulling Interferometry

Rewritten Abstract: The use of integrated-optic components in the field of astrophysics has been on the rise, largely due to their enhanced accuracy and precision. A notable technology in this domain is nulling interferometry, which aims to achieve high contrast and superior angular resolution. However, nullers often face two major challenges: rapid phase fluctuations in incoming light that lead to interference instability, and the directional couplers' chromaticity that hinders deep broadband interferometric null. This study investigates the potential of an ultrafast laser-inscribed tricoupler to address these issues. Incorporating a tricoupler into a nuller, according to our simulations, results in a significant improvement in null depth by an order of magnitude.",oai:arXiv.org:2106.00251,2021-06-01,['astro-ph.IM']
NeuTex: Neural Texture Mapping for Volumetric Neural Rendering,"Recent work has demonstrated that volumetric scene representations combined
with differentiable volume rendering can enable photo-realistic rendering for
challenging scenes that mesh reconstruction fails on. However, these methods
entangle geometry and appearance in a ""black-box"" volume that cannot be edited.
Instead, we present an approach that explicitly disentangles
geometry--represented as a continuous 3D volume--from appearance--represented
as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D
texture mapping (or surface parameterization) network into volumetric
representations. We constrain this texture mapping network using an additional
2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D
surface points map to 2D texture points that map back to the original 3D
points. We demonstrate that this representation can be reconstructed using only
multi-view image supervision and generates high-quality rendering results. More
importantly, by separating geometry and texture, we allow users to edit
appearance by simply editing 2D texture maps.","Title: NeuTex: The Application of Neural Texture Mapping in Volumetric Neural Rendering

Revised Abstract: Prior research has displayed how the amalgamation of volumetric scene representations and differentiable volume rendering can facilitate the photo-realistic rendering in complex scenes where mesh reconstruction becomes ineffective. Nevertheless, these techniques intertwine geometry and appearance within an uneditable ""black-box"" volume. Contrarily, our research introduces a method that distinctly separates geometry, depicted as a continuous 3D volume, and appearance, illustrated as a continuous 2D texture map. This is accomplished by incorporating a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We further regulate this texture mapping network with an extra 2D-to-3D inverse mapping network and a novel cycle consistency loss to ensure that 3D surface points correspond to 2D texture points that map back to the originating 3D points. Our findings show that this representation can be reconstructed utilizing solely multi-view image supervision, generating high-caliber rendering outcomes. Crucially, by differentiating geometry and texture, we enable users to modify appearance by merely editing 2D texture maps.",oai:arXiv.org:2103.00762,2021-03-01,['cs.CV']
"On the application of matrix congruence to QUBO formulations for systems
  of linear equations","Recent studies on quantum computing algorithms focus on excavating features
of quantum computers which have potential for contributing to computational
model enhancements. Among various approaches, quantum annealing methods
effectively parallelize quadratic unconstrained binary optimization (QUBO)
formulations of systems of linear equations. In this paper, we simplify these
formulations by exploiting congruence of real symmetric matrices to diagonal
matrices. We further exhibit computational merits of the proposed QUBO models,
which can outperform classical algorithms such as QR and SVD decomposition.","Title: Utilizing Matrix Congruence in QUBO Formulations for Linear Equation Systems

New Abstract: Contemporary research in quantum computing algorithms is centered around the exploration of characteristics inherent to quantum computers that hold promise in augmenting computational models. Among a plethora of methods, the use of quantum annealing techniques to concurrently process quadratic unconstrained binary optimization (QUBO) formulations for systems of linear equations has been found to be particularly effective. This study aims to streamline these formulations through the application of congruence in real symmetric matrices to diagonal matrices. Further, it elucidates the computational advantages of the suggested QUBO models, demonstrating their ability to surpass traditional algorithms like QR and SVD decomposition.",oai:arXiv.org:2111.00747,2021-11-01,['quant-ph']
Discriminating Quantum States with Quantum Machine Learning,"Quantum machine learning (QML) algorithms have obtained great relevance in
the machine learning (ML) field due to the promise of quantum speedups when
performing basic linear algebra subroutines (BLAS), a fundamental element in
most ML algorithms. By making use of BLAS operations, we propose, implement and
analyze a quantum k-means (qk-means) algorithm with a low time complexity of
$\mathcal{O}(NKlog(D)I/C)$ to apply it to the fundamental problem of
discriminating quantum states at readout. Discriminating quantum states allows
the identification of quantum states $|0\rangle$ and $|1\rangle$ from low-level
in-phase and quadrature signal (IQ) data, and can be done using custom ML
models. In order to reduce dependency on a classical computer, we use the
qk-means to perform state discrimination on the IBMQ Bogota device and managed
to find assignment fidelities of up to 98.7% that were only marginally lower
than that of the k-means algorithm. Inspection of assignment fidelity scores
resulting from applying both algorithms to a combination of quantum states
showed concordance to our correlation analysis using Pearson Correlation
coefficients, where evidence shows cross-talk in the (1, 2) and (2, 3)
neighboring qubit couples for the analyzed device.","Title: Differentiating Quantum States Utilizing Quantum Machine Learning Techniques

Revised Abstract: The importance of Quantum machine learning (QML) algorithms has significantly grown in the machine learning (ML) community, primarily due to the potential for superior speed in handling basic linear algebra subroutines (BLAS), which are integral to most ML approaches. Utilizing BLAS operations, we develop, execute, and scrutinize a quantum k-means (qk-means) algorithm, featuring a reduced time complexity of $\mathcal{O}(NKlog(D)I/C)$, to address the crucial problem of differentiating quantum states at readout. This discrimination allows for the recognition of quantum states $|0\rangle$ and $|1\rangle$ from base-level in-phase and quadrature signal (IQ) data, achievable through tailor-made ML models. To decrease reliance on classical computers, we employ the qk-means for state differentiation on the IBMQ Bogota device, achieving assignment fidelities as high as 98.7%, only slightly lower than the k-means algorithm. A thorough examination of assignment fidelity rates obtained from applying both algorithms to a mix of quantum states aligns with our correlation study using Pearson Correlation coefficients, indicating evidence of cross-talk in the (1, 2) and (2, 3) adjacent qubit pairs for the device in question.",oai:arXiv.org:2112.00313,2021-12-01,"['quant-ph', 'cs.LG']"
Segmentation of Breast Microcalcifications: A Multi-Scale Approach,"Accurate characterization of microcalcifications (MCs) in 2D full-field
digital screening mammography is a necessary step towards reducing diagnostic
uncertainty associated with the callback of women with suspicious MCs.
Quantitative analysis of MCs has the potential to better identify MCs that have
a higher likelihood of corresponding to invasive cancer. However, automated
identification and segmentation of MCs remains a challenging task with high
false positive rates. We present Hessian Difference of Gaussians Regression
(HDoGReg), a two stage multi-scale approach to MC segmentation. Candidate high
optical density objects are first delineated using blob detection and Hessian
analysis. A regression convolutional network, trained to output a function with
higher response near MCs, chooses the objects which constitute actual MCs. The
method is trained and validated on 435 mammograms from two separate datasets.
HDoGReg achieved a mean intersection over the union of 0.670$\pm$0.121 per
image, intersection over the union per MC object of 0.607$\pm$0.250 and true
positive rate of 0.744 at 0.4 false positive detections per $cm^2$. The results
of HDoGReg perform better when compared to state-of-the-art MC segmentation and
detection methods.","Title: A Multi-Scale Solution to Segmenting Breast Microcalcifications

Revised Abstract: The precise categorization of microcalcifications (MCs) in 2D full-field digital mammography is a crucial initiative in minimizing diagnostic uncertainty that arises from revisiting women with questionable MCs. The potential lies in a quantitative examination of MCs to more accurately pinpoint those that are more likely to be indicative of invasive cancer. Despite this, the autonomous recognition and segmentation of MCs still pose significant challenges, with a high prevalence of false positives. In this research, we introduce the Hessian Difference of Gaussians Regression (HDoGReg), a multi-scale, dual-stage strategy for MC segmentation. The initial stage involves the delineation of prospective high optical density entities using blob detection and Hessian analysis. A regression convolutional network, which is trained to generate a function with an increased response close to MCs, then selects the genuine MCs. The methodology was trained and validated using 435 mammograms from two distinct datasets. HDoGReg yielded a mean intersection over the union of 0.670$\pm$0.121 per image, an intersection over the union per MC object of 0.607$\pm$0.250, and a true positive rate of 0.744 at 0.4 false positive detections per $cm^2$. When juxtaposed with existing MC segmentation and detection techniques, HDoGReg showed superior performance.",oai:arXiv.org:2102.00754,2021-02-01,['eess.IV']
"Early Prediction of Heart Disease Using PCA and Hybrid Genetic Algorithm
  with k-Means","Worldwide research shows that millions of lives lost per year because of
heart disease. The healthcare sector produces massive volumes of data on heart
disease that are sadly not used to locate secret knowledge for successful
decision making. One of the most important aspects at this moment is detecting
heart disease at an early stage. Researchers have applied distinct techniques
to the UCI Machine Learning heart disease dataset. Many researchers have tried
to apply some complex techniques to this dataset, where detailed studies are
still missing. In this paper, Principal Component Analysis (PCA) has been used
to reduce attributes. Apart from a Hybrid genetic algorithm (HGA) with k-means
used for final clustering. Typically, the k-means method is using for
clustering the data. This type of clustering can get stuck in the local optima
because this method is heuristic. We used the Hybrid Genetic Algorithm (HGA)
for data clustering to avoid this problem. Our proposed method can predict
early heart disease with an accuracy of 94.06%.","Title: Proactive Identification of Heart Disease Leveraging PCA and Hybrid Genetic Algorithm with k-Means

Revised Abstract: Global studies reveal that heart disease is a leading cause of mortality, accounting for millions of deaths annually. Despite the vast amount of data generated by the healthcare industry on heart disease, its potential to unveil critical insights for effective decision-making remains largely untapped. A crucial area of focus is the early detection of heart disease. Various methodologies have been applied to the heart disease dataset from the UCI Machine Learning repository, with many researchers employing intricate techniques that have not been extensively explored. This paper utilizes Principal Component Analysis (PCA) to streamline attribute selection, alongside a Hybrid Genetic Algorithm (HGA) combined with k-means for ultimate clustering. Conventionally, k-means is employed for data clustering; however, its heuristic nature often leads to local optima, a limitation overcome by using the HGA. The proposed method demonstrates the potential to predict heart disease at an early stage with a high accuracy of 94.06%.",oai:arXiv.org:2101.00183,2021-01-01,['cs.LG']
Deep Measurement Updates for Bayes Filters,"Measurement update rules for Bayes filters often contain hand-crafted
heuristics to compute observation probabilities for high-dimensional sensor
data, like images. In this work, we propose the novel approach Deep Measurement
Update (DMU) as a general update rule for a wide range of systems. DMU has a
conditional encoder-decoder neural network structure to process depth images as
raw inputs. Even though the network is trained only on synthetic data, the
model shows good performance at evaluation time on real-world data. With our
proposed training scheme primed data training , we demonstrate how the DMU
models can be trained efficiently to be sensitive to condition variables
without having to rely on a stochastic information bottleneck. We validate the
proposed methods in multiple scenarios of increasing complexity, beginning with
the pose estimation of a single object to the joint estimation of the pose and
the internal state of an articulated system. Moreover, we provide a benchmark
against Articulated Signed Distance Functions(A-SDF) on the RBO dataset as a
baseline comparison for articulation state estimation.","Title: Advanced Measurement Update Mechanisms for Bayesian Filters

Revised Abstract: Traditional measurement update practices for Bayes filters often employ man-made heuristics to calculate observation probabilities pertinent to high-dimensional sensor data, such as images. This study introduces a ground-breaking approach known as the Deep Measurement Update (DMU), which serves as a universal update rule for a broad spectrum of systems. The DMU utilizes a conditional encoder-decoder neural network format to handle depth images in their raw form. Despite the network being trained exclusively on synthetic data, it exhibits commendable performance when evaluated on real-world data. With the implementation of our newly proposed training technique, primed data training, we illustrate how DMU models can be trained effectively to respond to condition variables without the necessity of a stochastic information bottleneck. We authenticate our proposed techniques across several scenarios of growing complexity, commencing with the pose estimation of a lone object and progressing to the combined estimation of the pose and the internal status of an interconnected system. Furthermore, we establish a benchmark for articulation state estimation against Articulated Signed Distance Functions (A-SDF) on the RBO dataset, providing a base for comparison.",oai:arXiv.org:2112.00380,2021-12-01,"['cs.CV', 'cs.LG', 'cs.RO']"
Designing nudge agents that promote human altruism,"Previous studies have found that nudging is key to promoting altruism in
human-human interaction. However, in social robotics, there is still a lack of
study on confirming the effect of nudging on altruism. In this paper, we apply
two nudge mechanisms, peak-end and multiple viewpoints, to a video stimulus
performed by social robots (virtual agents) to see whether a subtle change in
the stimulus can promote human altruism. An experiment was conducted online
through crowdsourcing with 136 participants. The result shows that the
participants who watched the peak part set at the end of the video performed
better at the Dictator game, which means that the nudge mechanism of the
peak-end effect actually promoted human altruism.","Title: Development of nudge agents to enhance human altruistic behavior

Revised Abstract: Prior research has established the significant role of nudging in fostering altruistic behavior in human-to-human interactions. Nevertheless, the domain of social robotics has not yet thoroughly investigated the influence of nudging on altruism. In the present study, we incorporate two nudge techniques, namely peak-end and multiple viewpoints, into a video demonstration featuring social robots (virtual agents). The primary aim is to ascertain if minute modifications in the stimulus can boost human altruism. An online experiment was implemented via crowdsourcing, engaging 136 participants. Outcomes illustrate that participants exposed to the peak segment at the video's conclusion performed notably well in the Dictator game. This indicates that the peak-end nudge technique effectively enhances human altruism.",oai:arXiv.org:2110.00319,2021-10-01,['cs.HC']
"Neural Free-Viewpoint Performance Rendering under Complex Human-object
  Interactions","4D reconstruction of human-object interaction is critical for immersive VR/AR
experience and human activity understanding. Recent advances still fail to
recover fine geometry and texture results from sparse RGB inputs, especially
under challenging human-object interactions scenarios. In this paper, we
propose a neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of both human and
objects under challenging interaction scenarios in arbitrary novel views, from
only sparse RGB streams. To deal with complex occlusions raised by human-object
interactions, we adopt a layer-wise scene decoupling strategy and perform
volumetric reconstruction and neural rendering of the human and object.
Specifically, for geometry reconstruction, we propose an interaction-aware
human-object capture scheme that jointly considers the human reconstruction and
object reconstruction with their correlations. Occlusion-aware human
reconstruction and robust human-aware object tracking are proposed for
consistent 4D human-object dynamic reconstruction. For neural texture
rendering, we propose a layer-wise human-object rendering scheme, which
combines direction-aware neural blending weight learning and spatial-temporal
texture completion to provide high-resolution and photo-realistic texture
results in the occluded scenarios. Extensive experiments demonstrate the
effectiveness of our approach to achieve high-quality geometry and texture
reconstruction in free viewpoints for challenging human-object interactions.","Title: Advanced Neural Performance Rendering under Complex Human-Object Interactions

Revised Abstract: The 4D reconstruction of human-object interactions is essential for understanding human activity and enhancing virtual and augmented reality experiences. However, current technological breakthroughs struggle to accurately reconstruct fine geometry and texture from sparse RGB data, particularly during complex human-object interactions. This research introduces a neural system for capturing and rendering human performance, capable of generating superior geometry and photo-realistic textures for both humans and objects in arbitrary novel views, using only sparse RGB streams. We use a layer-wise scene decoupling strategy to address the complicated occlusions caused by human-object interactions, which allows us to carry out a volumetric reconstruction and neural rendering of humans and objects. Specifically, we develop an interaction-aware human-object capture method that simultaneously accounts for human and object reconstruction, taking their correlations into consideration. We propose occlusion-aware human reconstruction and robust human-aware object tracking for consistent 4D dynamic human-object reconstruction. For neural texture rendering, we introduce a layer-wise human-object rendering method, which merges direction-aware neural blending weight learning with spatial-temporal texture completion to deliver high-resolution, photo-realistic texture results in occluded scenarios. Our extensive experimental results validate the effectiveness of our method in achieving superior geometry and texture reconstruction from free viewpoints during complex human-object interactions.",oai:arXiv.org:2108.00362,2021-08-01,"['cs.CV', 'cs.AI', 'cs.GR']"
Mean-Field Game-Theoretic Edge Caching,"In this book chapter, we study a problem of distributed content caching in an
ultra-dense edge caching network (UDCN), in which a large number of small base
stations (SBSs) prefetch popular files to cope with the ever-growing user
demand in 5G and beyond. In a UDCN, even a small misprediction of user demand
may render a large amount of prefetched data obsolete. Furtherproacmore, the
interference variance is high due to the short inter-SBS distances, making it
difficult to quantify data downloading rates. Lastly, since the caching
decision of each SBS interacts with those of all other SBSs, the problem
complexity of exponentially increases with the number of SBSs, which is unfit
for UDCNs. To resolve such challenging issues while reflecting time-varying and
location-dependent user demand, we leverage mean-field game (MFG) theory
through which each SBS interacts only with a single virtual SBS whose state is
drawn from the state distribution of the entire SBS population, i.e.,
mean-field (MF) distribution. This MF approximation asymptotically guarantees
achieving the epsilon Nash equilibrium as the number of SBSs approaches
infinity. To describe such an MFG-theoretic caching framework, this chapter
aims to provide a brief review of MFG, and demonstrate its effectiveness for
UDCNs.","Title: Application of Mean-Field Game Theory in Edge Caching Networks

Rewritten Abstract: This particular chapter of the book delves into the issue of decentralized content caching within an ultra-dense edge caching network (UDCN). This network is characterized by a multitude of small base stations (SBSs) that prefetch trending files, aiming to meet the increasing user demand in next-generation networks, such as 5G and beyond. However, in a UDCN, even a slight miscalculation in predicting user demand can lead to a substantial amount of prefetched data becoming redundant. Adding to the complexity, the interference variance is considerable due to the proximity of the SBSs, complicating the task of measuring data download rates. Moreover, the caching decision of each SBS is inextricably linked with the decisions of all other SBSs, exponentially escalating the problem complexity, making it unsuitable for UDCNs. To address these complexities while accommodating fluctuating and geographically-dependent user demand, we employ the mean-field game (MFG) theory. This theory allows each SBS to interact solely with a theoretical SBS, whose state is derived from the overall state distribution of all SBSs, otherwise known as the mean-field (MF) distribution. This MF approximation provides an asymptotic guarantee of reaching the epsilon Nash equilibrium as the SBS count tends towards infinity. This chapter aims to provide a succinct overview of MFG and its practical application in dealing with the challenges posed by UDCNs.",oai:arXiv.org:2101.00341,2021-01-01,"['cs.IT', 'cs.NI', 'math.IT']"
Energy-constrained Self-training for Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a
labeled source domain distribution to perform well on an unlabeled target
domain. Recently, the deep self-training involves an iterative process of
predicting on the target domain and then taking the confident predictions as
hard pseudo-labels for retraining. However, the pseudo-labels are usually
unreliable, and easily leading to deviated solutions with propagated errors. In
this paper, we resort to the energy-based model and constrain the training of
the unlabeled target sample with the energy function minimization objective. It
can be applied as a simple additional regularization. In this framework, it is
possible to gain the benefits of the energy-based model, while retaining strong
discriminative performance following a plug-and-play fashion. We deliver
extensive experiments on the most popular and large scale UDA benchmarks of
image classification as well as semantic segmentation to demonstrate its
generality and effectiveness.","Title: Implementing Energy-based Constraints in Self-training for Unsupervised Domain Adaptation

Revised Abstract: The primary objective of Unsupervised Domain Adaptation (UDA) is to effectively apply the insights gained from a labelled source domain to enhance performance on a target domain lacking labels. Recently, deep self-training, involving a cyclical process of prediction and retraining on the target domain using confident predictions as rigid pseudo-labels, has been utilized. Nonetheless, the pseudo-labels often prove unreliable, resulting in deviated outcomes and error propagation. This study proposes the use of an energy-based model to restrict the training of the unlabeled target sample via an energy function minimization objective, serving as an uncomplicated additional regularization. Employing this model allows for the exploitation of the benefits of an energy-based framework while ensuring robust discriminative performance in a user-friendly manner. Comprehensive experimentation on widely recognized, large-scale UDA benchmarks for image classification and semantic segmentation confirms the broad applicability and efficacy of this approach.",oai:arXiv.org:2101.00316,2021-01-01,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']"
Unsupervised Discovery of Unaccusative and Unergative Verbs,"We present an unsupervised method to detect English unergative and
unaccusative verbs. These categories allow us to identify verbs participating
in the causative-inchoative alternation without knowing the semantic roles of
the verb. The method is based on the generation of intransitive sentence
variants of candidate verbs and probing a language model. We obtained results
on par with similar approaches, with the added benefit of not relying on
annotated resources.","Title: Autonomous Identification of Unaccusative and Unergative Verbs

Generated Abstract: This study introduces an autonomous technique for the detection of unergative and unaccusative verbs in the English language. The categorization aids in the identification of verbs involved in the causative-inchoative alternation, irrespective of the semantic roles of the verb. The proposed method hinges on the creation of intransitive sentence variations of potential verbs and scrutinizing a language model. The outcomes achieved are consistent with those garnered through comparable methodologies, with the distinctive advantage of eliminating the need for annotated resources.",oai:arXiv.org:2111.00808,2021-11-01,['cs.CL']
"Comparisons of Order Statistics from Some Heterogeneous Discrete
  Distributions","In this paper, we compare extreme order statistics through vector
majorization arising from heterogeneous Poisson and geometric random variables.
These comparisons are carried out with respect to usual stochastic ordering.","Title: An Analysis of Order Statistics from Various Heterogeneous Discrete Distributions

Rewritten Abstract: The present study undertakes a comparative analysis of extreme order statistics derived from disparate Poisson and geometric random variables, utilizing the concept of vector majorization. These comparative assessments are conducted in reference to standard stochastic ordering.",oai:arXiv.org:2103.00763,2021-03-01,"['math.ST', 'stat.TH']"
BORM: Bayesian Object Relation Model for Indoor Scene Recognition,"Scene recognition is a fundamental task in robotic perception. For human
beings, scene recognition is reasonable because they have abundant object
knowledge of the real world. The idea of transferring prior object knowledge
from humans to scene recognition is significant but still less exploited. In
this paper, we propose to utilize meaningful object representations for indoor
scene representation. First, we utilize an improved object model (IOM) as a
baseline that enriches the object knowledge by introducing a scene parsing
algorithm pretrained on the ADE20K dataset with rich object categories related
to the indoor scene. To analyze the object co-occurrences and pairwise object
relations, we formulate the IOM from a Bayesian perspective as the Bayesian
object relation model (BORM). Meanwhile, we incorporate the proposed BORM with
the PlacesCNN model as the combined Bayesian object relation model (CBORM) for
scene recognition and significantly outperforms the state-of-the-art methods on
the reduced Places365 dataset, and SUN RGB-D dataset without retraining,
showing the excellent generalization ability of the proposed method. Code can
be found at https://github.com/hszhoushen/borm.","Title: BORM: A Bayesian Approach to Object Relation Modeling for Indoor Scene Recognition

Revised Abstract: The understanding of scenes is a fundamental component of robotic perception, a task humans perform effortlessly due to their extensive knowledge of real-world objects. However, the significant concept of transferring this pre-existing object understanding from humans to robotic scene recognition remains underutilized. In this study, we introduce the concept of using meaningful object representations to facilitate indoor scene comprehension. We initially employ an enhanced object model (EOM), which augments object knowledge through an advanced scene parsing algorithm. This algorithm is pretrained using the ADE20K dataset, which encompasses a wide range of object categories pertinent to indoor scenes. To scrutinize the co-occurrence of objects and their pairwise relationships, we reinterpret the EOM through a Bayesian lens, leading to the development of the Bayesian object relation model (BORM). Concurrently, we integrate the BORM with the PlacesCNN model, resulting in a combined Bayesian object relation model (CBORM) for scene recognition. This integration yields superior performance over existing methods on the reduced Places365 dataset and the SUN RGB-D dataset, without necessitating retraining. This highlights the exceptional generalization capability of our proposed methodology. The related code can be accessed via https://github.com/hszhoushen/borm.",oai:arXiv.org:2108.00397,2021-08-01,"['cs.CV', 'cs.RO']"
"BINet: Learning to Solve Partial Differential Equations with Boundary
  Integral Networks","We propose a method combining boundary integral equations and neural networks
(BINet) to solve partial differential equations (PDEs) in both bounded and
unbounded domains. Unlike existing solutions that directly operate over
original PDEs, BINet learns to solve, as a proxy, associated boundary integral
equations using neural networks. The benefits are three-fold. Firstly, only the
boundary conditions need to be fitted since the PDE can be automatically
satisfied with single or double layer representations according to the
potential theory. Secondly, the dimension of the boundary integral equations is
typically smaller, and as such, the sample complexity can be reduced
significantly. Lastly, in the proposed method, all differential operators of
the original PDEs have been removed, hence the numerical efficiency and
stability are improved. Adopting neural tangent kernel (NTK) techniques, we
provide proof of the convergence of BINets in the limit that the width of the
neural network goes to infinity. Extensive numerical experiments show that,
without calculating high-order derivatives, BINet is much easier to train and
usually gives more accurate solutions, especially in the cases that the
boundary conditions are not smooth enough. Further, BINet outperforms strong
baselines for both one single PDE and parameterized PDEs in the bounded and
unbounded domains.","Title: BINet: Utilizing Boundary Integral Networks for the Resolution of Partial Differential Equations

Generated Abstract: This paper introduces a novel method (BINet) that merges boundary integral equations and neural networks for the resolution of partial differential equations (PDEs) in various domains, both bounded and unbounded. Differing from current methods that directly interact with the original PDEs, BINet employs a learning approach to solve the associated boundary integral equations through neural networks. This approach offers three distinct advantages. First, the process only necessitates the fitting of boundary conditions as the PDE can be automatically complied with single or double layer representations in line with potential theory. Second, the boundary integral equations generally have a smaller dimension, allowing for a substantial decrease in sample complexity. Finally, the removal of all differential operators from the original PDEs in this method enhances numerical efficiency and stability. Leveraging neural tangent kernel (NTK) methods, we offer evidence of BINets' convergence as the width of the neural network approaches infinity. Comprehensive numerical tests reveal that BINet, which does not require the calculation of high-order derivatives, is simpler to train and typically provides more precise solutions, especially when the boundary conditions lack smoothness. Additionally, BINet surpasses strong baselines for a single PDE and parameterized PDEs across both bounded and unbounded domains.",oai:arXiv.org:2110.00352,2021-10-01,"['math.NA', 'cs.NA']"
"Online anomaly detection using statistical leverage for streaming
  business process events","While several techniques for detecting trace-level anomalies in event logs in
offline settings have appeared recently in the literature, such techniques are
currently lacking for online settings. Event log anomaly detection in online
settings can be crucial for discovering anomalies in process execution as soon
as they occur and, consequently, allowing to promptly take early corrective
actions. This paper describes a novel approach to event log anomaly detection
on event streams that uses statistical leverage. Leverage has been used
extensively in statistics to develop measures to identify outliers and it has
been adapted in this paper to the specific scenario of event stream data. The
proposed approach has been evaluated on both artificial and real event streams.","Title: Utilizing Statistical Leverage for Anomaly Detection in Streaming Business Process Events Online

Revised Abstract: Recent literature has seen the emergence of various techniques aimed at identifying anomalies in offline event logs at the trace-level. However, there is a noticeable absence of such methods for online settings. The early detection of anomalies in process execution, through online event log anomaly detection, is crucial for timely identification and swift corrective action. This study proposes an innovative method for anomaly detection in event streams, using statistical leverage. This statistical tool, widely used for outlier identification, has been tailored to fit the unique requirements of event stream data in this study. The proposed method has been assessed and found effective on both synthetic and actual event streams.",oai:arXiv.org:2103.00831,2021-03-01,"['cs.LG', 'stat.CO']"
"Graph-based Exercise- and Knowledge-Aware Learning Network for Student
  Performance Prediction","Predicting student performance is a fundamental task in Intelligent Tutoring
Systems (ITSs), by which we can learn about students' knowledge level and
provide personalized teaching strategies for them. Researchers have made plenty
of efforts on this task. They either leverage educational psychology methods to
predict students' scores according to the learned knowledge proficiency, or
make full use of Collaborative Filtering (CF) models to represent latent
factors of students and exercises. However, most of these methods either
neglect the exercise-specific characteristics (e.g., exercise materials), or
cannot fully explore the high-order interactions between students, exercises,
as well as knowledge concepts. To this end, we propose a Graph-based Exercise-
and Knowledge-Aware Learning Network for accurate student score prediction.
Specifically, we learn students' mastery of exercises and knowledge concepts
respectively to model the two-fold effects of exercises and knowledge concepts.
Then, to model the high-order interactions, we apply graph convolution
techniques in the prediction process. Extensive experiments on two real-world
datasets prove the effectiveness of our proposed Graph-EKLN.","Title: A Novel Graph-Based Approach for Student Performance Prediction Considering Exercise and Knowledge Factors

Revised Abstract: A cornerstone function of Intelligent Tutoring Systems (ITSs) is the prediction of student performance, enabling the system to gauge individual knowledge proficiency and subsequently tailor teaching strategies. This crucial task has been the focus of extensive research, utilizing approaches from educational psychology to estimate student scores based on their acquired knowledge proficiency, or employing Collaborative Filtering (CF) models to depict the latent attributes of students and their assigned exercises. However, many of these methods either disregard exercise-specific variables such as the nature of the exercise materials, or inadequately investigate high-order interactions between students, the exercises, and the knowledge concepts. In response, we introduce a graph-based learning network that is both exercise- and knowledge-aware, aiming to optimize student score prediction. Specifically, our model considers both the impact of exercises and knowledge concepts by evaluating students' command of these two aspects. To capture high-order interactions, graph convolution techniques are employed during the prediction phase. Rigorous testing on two authentic datasets validates the efficacy of our newly proposed Graph-EKLN.",oai:arXiv.org:2106.00263,2021-06-01,['cs.AI']
"Atomistic simulations of twin boundary effect on the crack growth
  behaviour in BCC Fe","In this paper, the effect of twin boundaries on the crack growth behaviour of
single crystal BCC Fe has been investigated using molecular dynamics
simulations. The growth of an atomically sharp crack with an orientation of
(111)$<$110$>$ (crack plane/crack front) has been studied under mode-I loading
at constant strain rate. In order to study the influence of twin boundaries on
the crack growth behaviour, single and multiple twin boundaries were introduced
perpendicular to crack growth direction. The results indicate that the
(111)$<$110$>$ crack in single crystal BCC Fe grows in brittle manner. However,
following the introduction of twin boundaries, a noticeable plastic deformation
has been observed at the crack tip. Further, increasing the number of twin
boundaries increased the amount of plastic deformation leading to better crack
resistance and high failure strains. Finally, an interesting relationship has
been observed between the crack growth rate and flow stress.","Title: Computational Analysis of Twin Boundary Impact on Crack Propagation Behaviour in BCC Fe

New Abstract: The present study employs molecular dynamics simulations to examine the influence of twin boundaries on crack propagation characteristics in single crystal BCC Fe. The research scrutinizes the expansion of an atomically precise crack aligned with a (111)$<$110$>$ (crack plane/crack front) under mode-I loading at a steady strain rate. In an effort to explore how twin boundaries can modify the crack propagation behaviour, single and multiple twin boundaries were integrated perpendicular to the direction of crack expansion. The findings reveal that the (111)$<$110$>$ crack develops in a brittle fashion in single crystal BCC Fe. Nevertheless, the integration of twin boundaries resulted in discernible plastic deformation at the apex of the crack. Moreover, augmenting the count of twin boundaries escalated the degree of plastic deformation, thereby enhancing crack resistance and causing high strain at failure. The study concludes by noting a compelling correlation between the rate of crack growth and flow stress.",oai:arXiv.org:2112.00354,2021-12-01,['cond-mat.mtrl-sci']
"gnlse-python: Open Source Software to Simulate Nonlinear Light
  Propagation In Optical Fibers","The propagation of pulses in optical fibers is described by the generalized
nonlinear Schrodinger equation (GNLSE), which takes into account the fiber
losses, nonlinear effects, and higher-order chromatic dispersion. The GNLSE is
a partial differential equation, whose order depends on the accounted nonlinear
and dispersion effects. We present gnlse-python, a nonlinear optics modeling
toolbox that contains a rich set of components and modules to solve the GNLSE
using the split-step Fourier transform method (SSFM). The numerical solver is
freely available, implemented in Python language, and includes a number of
optical fiber analysis tools. Code and data are available at
https://github.com/WUST-FOG/gnlse-python.","Title: gnlse-python: A Freely Accessible Program for Modeling Nonlinear Light Propagation in Optical Fibers

Revised Abstract: The generalized nonlinear Schrodinger equation (GNLSE) is a comprehensive model that describes the transmission of pulses in optical fibers, accounting for fiber losses, nonlinear effects, and higher-order chromatic dispersion. The complexity of the GNLSE, a partial differential equation, varies with the inclusion of different nonlinear and dispersion factors. This paper introduces gnlse-python, an open-source toolbox for nonlinear optics simulation, equipped with a diverse suite of modules and components to tackle the GNLSE using the split-step Fourier transform method (SSFM). The computational solver, which is offered without charge, is developed in Python and comes packed with several optical fiber evaluation tools. The code and associated data can be accessed via https://github.com/WUST-FOG/gnlse-python.",oai:arXiv.org:2110.00298,2021-10-01,"['physics.optics', 'cs.NA', 'math.NA', 'nlin.PS']"
"Gradient potential estimates in elliptic obstacle problems with Orlicz
  growth","In this paper,we consider the solutions of the non-homogeneous elliptic
obstacle problems with Orlicz growth involving measure data. We first establish
the pointwise estimates of the approximable solutions to these problems via
fractional maximal operators. As a result, we obtain pointwise and oscillation
estimates for the gradients of solutions by the non-linear Wolff potentials,
and these yield results on $C^{1,\alpha}$-regularity of solutions.","Title: Evaluations of Gradient Potential in Non-Homogeneous Elliptic Obstacle Problems with Orlicz Expansion

Generated Abstract: The focus of this research is on the solutions pertaining to non-homogeneous elliptic obstacle problems that are associated with Orlicz expansion and incorporate measure data. Initially, we propose pointwise estimates of the solutions that can be approximated to these issues through the application of fractional maximal operators. Consequently, we derive both pointwise and oscillation estimates for the solution gradients via the non-linear Wolff potentials. These findings further lead to results concerning $C^{1,\alpha}$-regularity of the solutions.",oai:arXiv.org:2104.00260,2021-04-01,['math.AP']
"Wiki to Automotive: Understanding the Distribution Shift and its impact
  on Named Entity Recognition","While transfer learning has become a ubiquitous technique used across Natural
Language Processing (NLP) tasks, it is often unable to replicate the
performance of pre-trained models on text of niche domains like Automotive. In
this paper we aim to understand the main characteristics of the distribution
shift with automotive domain text (describing technical functionalities such as
Cruise Control) and attempt to explain the potential reasons for the gap in
performance. We focus on performing the Named Entity Recognition (NER) task as
it requires strong lexical, syntactic and semantic understanding by the model.
Our experiments with 2 different encoders, namely BERT-Base-Uncased and
SciBERT-Base-Scivocab-Uncased have lead to interesting findings that showed: 1)
The performance of SciBERT is better than BERT when used for automotive domain,
2) Fine-tuning the language models with automotive domain text did not make
significant improvements to the NER performance, 3) The distribution shift is
challenging as it is characterized by lack of repeating contexts, sparseness of
entities, large number of Out-Of-Vocabulary (OOV) words and class overlap due
to domain specific nuances.","Title: Comprehending the Shift in Distribution and its Effect on Named Entity Recognition in the Automotive Field

Rewritten Abstract: Even though transfer learning is extensively employed across tasks in Natural Language Processing (NLP), it often falls short in emulating the performance of pre-existing models when applied to specialized domains such as Automotive. This research seeks to decipher the primary features of the distribution shift associated with automotive domain text (detailing technical features like Cruise Control) and endeavors to rationalize the disparity in performance. The focus is on executing the task of Named Entity Recognition (NER), which necessitates a robust understanding of lexical, syntactic, and semantic aspects by the model. Our experimental trials with two distinct encoders, BERT-Base-Uncased and SciBERT-Base-Scivocab-Uncased, have resulted in intriguing discoveries: 1) SciBERT outperforms BERT in the automotive domain, 2) Refining the language models with automotive domain text failed to considerably enhance the NER performance, 3) The shift in distribution is a challenge due to the paucity of recurring contexts, entity sparseness, a high quantity of Out-Of-Vocabulary (OOV) terms, and class overlapping as a result of domain-specific subtleties.",oai:arXiv.org:2112.00283,2021-12-01,['cs.CL']
Efficient and Differentiable Shadow Computation for Inverse Problems,"Differentiable rendering has received increasing interest for image-based
inverse problems. It can benefit traditional optimization-based solutions to
inverse problems, but also allows for self-supervision of learning-based
approaches for which training data with ground truth annotation is hard to
obtain. However, existing differentiable renderers either do not model
visibility of the light sources from the different points in the scene,
responsible for shadows in the images, or are too slow for being used to train
deep architectures over thousands of iterations. To this end, we propose an
accurate yet efficient approach for differentiable visibility and soft shadow
computation. Our approach is based on the spherical harmonics approximations of
the scene illumination and visibility, where the occluding surface is
approximated with spheres. This allows for a significantly more efficient
shadow computation compared to methods based on ray tracing. As our formulation
is differentiable, it can be used to solve inverse problems such as texture,
illumination, rigid pose, and geometric deformation recovery from images using
analysis-by-synthesis optimization.","Title: A Proficient and Differentiable Method for Shadow Calculation in Inverse Problems 

Alternative Abstract: The escalating interest in differentiable rendering has been noticeable in the realm of image-based inverse problems. It not only enhances traditional optimization-based resolutions to inverse problems but also enables learning-based methodologies to self-supervise, especially when it is challenging to acquire training data with accurate ground truth annotation. However, the prevailing differentiable renderers either fail to simulate the visibility of light sources from diverse points in the scene, which is crucial for creating shadows in images, or they are inadequately slow to train deep architectures over numerous iterations. In response to this, we introduce a precise and efficient technique for differentiable visibility and gentle shadow computation. Our method hinges on the approximation of scene illumination and visibility using spherical harmonics, where the obstructing surface is replicated with spheres. This significantly enhances the proficiency of shadow calculation, making it superior to ray tracing-based methods. Since our approach is differentiable, it can be employed to tackle inverse problems such as recovery of texture, illumination, rigid pose, and geometric deformation from images through analysis-by-synthesis optimization.",oai:arXiv.org:2104.00359,2021-04-01,['cs.CV']
Mean-field particle swarm optimization,"In this work we survey some recent results on the global minimization of a
non-convex and possibly non-smooth high dimensional objective function by means
of particle based gradient-free methods. Such problems arise in many situations
of contemporary interest in machine learning and signal processing. After a
brief overview of metaheuristic methods based on particle swarm optimization
(PSO), we introduce a continuous formulation via second-order systems of
stochastic differential equations that generalize PSO methods and provide the
basis for their theoretical analysis. Subsequently, we will show how through
the use of mean-field techniques it is possible to derive in the limit of large
particles number the corresponding mean-field PSO description based on
Vlasov-Fokker-Planck type equations. Finally, in the zero inertia limit, we
will analyze the corresponding macroscopic hydrodynamic equations, showing that
they generalize the recently introduced consensus-based optimization (CBO)
methods by including memory effects. Rigorous results concerning the mean-field
limit, the zero-inertia limit, and the convergence of the mean-field PSO method
towards the global minimum are provided along with a suite of numerical
examples.","Title: Analysis of Mean-field Particle Swarm Optimization

Revised Abstract: This study presents an examination of recent findings on the global minimization of a potentially non-smooth and non-convex high dimensional objective function, utilizing gradient-free strategies based on particles. Such complications are prevalent in current areas of interest such as machine learning and signal processing. An initial review of metaheuristic procedures founded on particle swarm optimization (PSO) is given, followed by the presentation of a continuous formulation through second-order systems of stochastic differential equations. These broaden the PSO methods, laying the groundwork for their theoretical assessment. We then demonstrate how the application of mean-field techniques enables the derivation of the corresponding mean-field PSO depiction based on Vlasov-Fokker-Planck type equations for a large number of particles. In the zero-inertia limit, we scrutinize the corresponding macroscopic hydrodynamic equations, indicating they broaden the recently proposed consensus-based optimization (CBO) methods by incorporating memory effects. Detailed findings regarding the mean-field limit, the zero-inertia limit, and the convergence of the mean-field PSO method towards the global minimum are supplied, accompanied by a range of numerical instances.",oai:arXiv.org:2108.00393,2021-08-01,"['math.OC', 'cs.NA', 'math.NA']"
"A translation of G. S. Makanin's 1966 Ph.D. thesis ""On the Identity
  Problem for Finitely Presented Groups and Semigroups""","This is an English translation of the thesis written by G. S. Makanin for the
degree of Candidate of Physical and Mathematical Sciences (equivalent to a
Ph.D.), originally submitted to the Steklov Mathematical Institute in 1966. The
original language is Russian. The named supervisors are A. A. Markov and S. I.
Adian.","Title: An English Interpretation of G. S. Makanin's 1966 Doctoral Thesis ""On the Identity Problem for Finitely Presented Groups and Semigroups""

Revised Abstract: This work presents an English interpretation of G. S. Makanin's pioneering doctoral research, which he initially presented in Russian to the Steklov Mathematical Institute in 1966 to earn the title of Candidate of Physical and Mathematical Sciences, a degree comparable to a Ph.D. The original research was supervised by A. A. Markov and S. I. Adian.",oai:arXiv.org:2102.00745,2021-02-01,"['math.GR', 'math.HO']"
Multi-Sample based Contrastive Loss for Top-k Recommendation,"The top-k recommendation is a fundamental task in recommendation systems
which is generally learned by comparing positive and negative pairs. The
Contrastive Loss (CL) is the key in contrastive learning that has received more
attention recently and we find it is well suited for top-k recommendations.
However, it is a problem that CL treats the importance of the positive and
negative samples as the same. On the one hand, CL faces the imbalance problem
of one positive sample and many negative samples. On the other hand, positive
items are so few in sparser datasets that their importance should be
emphasized. Moreover, the other important issue is that the sparse positive
items are still not sufficiently utilized in recommendations. So we propose a
new data augmentation method by using multiple positive items (or samples)
simultaneously with the CL loss function. Therefore, we propose a Multi-Sample
based Contrastive Loss (MSCL) function which solves the two problems by
balancing the importance of positive and negative samples and data
augmentation. And based on the graph convolution network (GCN) method,
experimental results demonstrate the state-of-the-art performance of MSCL. The
proposed MSCL is simple and can be applied in many methods. We will release our
code on GitHub upon the acceptance.","Title: A Novel Approach to Top-k Recommendation through Multi-Sample Contrastive Loss

Revised Abstract: The primary function of recommendation systems, the top-k recommendation, typically learns by juxtaposing positive and negative pairs. In recent times, there has been a surge of interest in contrastive learning, particularly the Contrastive Loss (CL). Our study identifies the suitability of CL for top-k recommendations but also highlights an inherent issue - the treatment of positive and negative samples with equal importance. This leads to two problems: firstly, an imbalance between a singular positive sample and numerous negative samples, and secondly, the underemphasis of positive items which are scant in sparser datasets. Furthermore, the limited use of sparse positive items in recommendations is another significant concern. To address these issues, we introduce an innovative data augmentation technique that employs multiple positive items simultaneously with the CL loss function. Consequently, we present a Multi-Sample based Contrastive Loss (MSCL) function, adept at resolving the above problems through a balanced approach to positive and negative samples and effective data augmentation. Our experimental results, based on the graph convolution network (GCN) method, show that the MSCL function outperforms other methods, thereby establishing it as a cutting-edge solution. Simple in its design, MSCL can be incorporated into several other methods. Upon acceptance, we will make our code publicly available on GitHub.",oai:arXiv.org:2109.00217,2021-09-01,"['cs.IR', 'cs.AI']"
A note on simple games with two equivalence classes of players,"Many real-world voting systems consist of voters that occur in just two
different types. Indeed, each voting system with a {\lq\lq}House{\rq\rq} and a
{\lq\lq}Senat{\rq\rq} is of that type. Here we present structural
characterizations and explicit enumeration formulas for these so-called
bipartite simple games.","Title: An Observation on Bipartite Simple Games with Two Categories of Participants

Rewritten Abstract: A multitude of practical voting structures comprise only two distinct classifications of voters. This is particularly true for any voting system that includes a ""House"" and a ""Senate"". This study provides comprehensive structural definitions and precise calculation formulas for these types of games, often referred to as bipartite simple games.",oai:arXiv.org:2112.00307,2021-12-01,"['math.CO', 'cs.GT']"
Plasmonic Microbubble Dynamics in Binary Liquids,"The growth of surface plasmonic microbubbles in binary water/ethanol
solutions is experimentally studied. The microbubbles are generated by
illuminating a gold nanoparticle array with a continuous wave laser. Plasmonic
bubbles exhibit ethanol concentration-dependent behaviors. For low ethanol
concentrations (f_e) of < 67.5%, bubbles do not exist at the solid-liquid
interface. For high f_e values of >80%, the bubbles behave as in pure ethanol.
Only in an intermediate window of 67.5% < f_e < 80% do we find sessile
plasmonic bubbles with a highly nontrivial temporal evolution, in which as a
function of time three phases can be discerned. (1) In the first phase, the
microbubbles grow, while wiggling. (2) As soon as the wiggling stops, the
microbubbles enter the second phase in which they suddenly shrink, followed by
(3) a steady reentrant growth phase. Our experiments reveal that the sudden
shrinkage of the microbubbles in the second regime is caused by a depinning
event of the three phase contact line. We systematically vary the ethanol
concentration, laser power, and laser spot size to unravel water recondensation
as the underlying mechanism of the sudden bubble shrinkage in phase 2.","Title: Examination of Plasmonic Microbubble Behavior in Mixed Liquids

Updated Abstract: This study focuses on the experimental analysis of the expansion of surface plasmonic microbubbles in dual water and ethanol solutions. The microbubbles are produced through the illumination of an array of gold nanoparticles with a steady wave laser. It is observed that the behavior of plasmonic bubbles is dependent on the concentration of ethanol. The absence of bubbles at the solid-liquid boundary is noted when the ethanol concentration (f_e) is below 67.5%. On the other hand, with higher f_e values exceeding 80%, the bubbles mimic the behavior noted in pure ethanol. The formation of sessile plasmonic bubbles featuring a complex temporal evolution is only noted within an intermediate range of 67.5% < f_e < 80%. As time progresses, three distinct phases are discerned: (1) The initial phase involves the growth of the microbubbles, accompanied by a wiggling motion. (2) The cessation of the wiggling motion marks the start of the second phase, which is characterized by a sudden contraction of the microbubbles, followed by (3) a constant phase of regrowth. The study identifies the abrupt contraction of the microbubbles in the second stage as being triggered by a depinning occurrence at the three-phase contact line. By systematically adjusting the ethanol concentration, laser power, and laser spot size, the investigation seeks to understand the recondensation of water as the primary cause of the sudden bubble contraction in phase 2.",oai:arXiv.org:2101.00219,2021-01-01,"['cond-mat.soft', 'physics.flu-dyn']"
Federated Learning: Issues in Medical Application,"Since the federated learning, which makes AI learning possible without moving
local data around, was introduced by google in 2017 it has been actively
studied particularly in the field of medicine. In fact, the idea of machine
learning in AI without collecting data from local clients is very attractive
because data remain in local sites. However, federated learning techniques
still have various open issues due to its own characteristics such as non
identical distribution, client participation management, and vulnerable
environments. In this presentation, the current issues to make federated
learning flawlessly useful in the real world will be briefly overviewed. They
are related to data/system heterogeneity, client management, traceability, and
security. Also, we introduce the modularized federated learning framework, we
currently develop, to experiment various techniques and protocols to find
solutions for aforementioned issues. The framework will be open to public after
development completes.","Title: The Challenges of Implementing Federated Learning in the Medical Field

Rewritten Abstract: The conception of federated learning by Google in 2017, a process enabling artificial intelligence (AI) to learn without the necessity of data relocation, has sparked significant interest, particularly in the realm of healthcare. The ability for AI to learn without gathering data from local clients is appealing as it allows data to remain at the originating sites. However, there exist numerous unresolved issues related to federated learning due to its inherent characteristics such as non-identical distribution, client participation oversight, and susceptibility to security breaches. This paper provides a succinct overview of the current challenges that prevent federated learning from being seamlessly implemented in practical applications, touching upon aspects such as data/system heterogeneity, client management, traceability, and security. Furthermore, we present our ongoing development of a modular federated learning framework designed to experiment with various techniques and protocols with the aim of resolving the highlighted issues. Upon completion, the framework will be made publicly accessible.",oai:arXiv.org:2109.00202,2021-09-01,"['cs.LG', 'cs.AI']"
"LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of
  Dynamic Agents","In this paper, we address the problem of predicting the future motion of a
dynamic agent (called a target agent) given its current and past states as well
as the information on its environment. It is paramount to develop a prediction
model that can exploit the contextual information in both static and dynamic
environments surrounding the target agent and generate diverse trajectory
samples that are meaningful in a traffic context. We propose a novel prediction
model, referred to as the lane-aware prediction (LaPred) network, which uses
the instance-level lane entities extracted from a semantic map to predict the
multi-modal future trajectories. For each lane candidate found in the
neighborhood of the target agent, LaPred extracts the joint features relating
the lane and the trajectories of the neighboring agents. Then, the features for
all lane candidates are fused with the attention weights learned through a
self-supervised learning task that identifies the lane candidate likely to be
followed by the target agent. Using the instance-level lane information, LaPred
can produce the trajectories compliant with the surroundings better than 2D
raster image-based methods and generate the diverse future trajectories given
multiple lane candidates. The experiments conducted on the public nuScenes
dataset and Argoverse dataset demonstrate that the proposed LaPred method
significantly outperforms the existing prediction models, achieving
state-of-the-art performance in the benchmarks.","Title: LaPred: A Novel Approach to Predicting Multi-modal Future Trajectories of Dynamic Agents 

Rewritten Abstract: This study focuses on the prediction of future movements of a dynamic agent, termed a target agent, based on its current and historical states, and environmental context. The creation of a predictive model, capable of utilizing contextual information from both static and dynamic environments surrounding the target agent, is essential for generating a range of meaningful trajectory samples within a traffic context. We introduce a unique predictive model, the Lane-Aware Prediction (LaPred) network, which leverages instance-level lane entities derived from a semantic map to forecast multi-modal future trajectories. LaPred evaluates joint features connecting each lane candidate in the target agent's vicinity and the trajectories of neighbouring agents. These features are then amalgamated with attention weights, gleaned through a self-supervised learning task that determines the most likely lane candidate to be pursued by the target agent. Through the utilization of instance-level lane information, LaPred is able to generate trajectories that are more in line with the surrounding environment than 2D raster image-based methods, while also producing a variety of future trajectories given multiple lane candidates. Experimental results using the public nuScenes and Argoverse datasets reveal that LaPred significantly surpasses existing prediction models, achieving a new standard in benchmark performance.",oai:arXiv.org:2104.00249,2021-04-01,"['cs.CV', 'cs.LG']"
"Backhaul-Aware Intelligent Positioning of UAVs and Association of
  Terrestrial Base Stations for Fronthaul Connectivity","The mushroom growth of cellular users requires novel advancements in the
existing cellular infrastructure. One way to handle such a tremendous increase
is to densely deploy terrestrial small-cell base stations (TSBSs) with careful
management of smart backhaul/fronthaul networks. Nevertheless, terrestrial
backhaul hubs significantly suffer from the dense fading environment and are
difficult to install in a typical urban environment. Therefore, this paper
considers the idea of replacing terrestrial backhaul network with an aerial
network consisting of unmanned aerial vehicles (UAVs) to provide the fronthaul
connectivity between the TSBSs and the ground core-network (GCN). To this end,
we focus on the joint positioning of UAVs and the association of TSBSs such
that the sum-rate of the overall system is maximized. In particular, the
association problem of TSBSs with UAVs is formulated under
communication-related constraints, i.e., bandwidth, number of connections to a
UAV, power limit, interference threshold, UAV heights, and backhaul data rate.
To meet this joint objective, we take advantage of the genetic algorithm (GA)
due to the offline nature of our optimization problem. The performance of the
proposed approach is evaluated using the unsupervised learning-based k-means
clustering algorithm. We observe that the proposed approach is highly effective
to satisfy the requirements of smart fronthaul networks.","Title: Intelligent Placement of UAVs and Affiliation of Terrestrial Base Stations for Fronthaul Connectivity: A Backhaul-Aware Perspective

Revised Abstract: The exponential surge in the number of cellular users necessitates innovative enhancements in the prevailing cellular infrastructure. One possible solution to manage this massive growth is to densely position terrestrial small-cell base stations (TSBSs) while efficiently controlling smart backhaul/fronthaul networks. However, terrestrial backhaul hubs encounter significant challenges due to the dense fading environment and their installation complexity in standard urban settings. This study proposes the substitution of terrestrial backhaul networks with an aerial network composed of unmanned aerial vehicles (UAVs) to facilitate fronthaul connections between the TSBSs and the ground core-network (GCN). The study emphasizes the combined positioning of UAVs and the association of TSBSs to maximize the overall system's sum-rate. Specifically, the paper formulates the TSBSs' association issue with UAVs considering communication-related constraints, including bandwidth, the number of UAV connections, power limits, interference thresholds, UAV heights, and backhaul data rates. To achieve this combined goal, we utilize the genetic algorithm (GA), an ideal approach for the offline nature of our optimization issue. The efficacy of the proposed method is assessed using the unsupervised learning-based k-means clustering algorithm, demonstrating its high effectiveness in meeting the demands of smart fronthaul networks.",oai:arXiv.org:2105.00286,2021-05-01,['eess.SP']
Inequalities for the Radon transform on convex sets,"Several years ago the authors started looking at some problems of convex
geometry from a more general point of view, replacing volume by an arbitrary
measure. This approach led to new general properties of the Radon transform on
convex bodies including an extension of the Busemann-Petty problem and a
slicing inequality for arbitrary functions. The latter means that the sup-norm
of the Radon transform of any probability density on a convex body of volume
one is bounded from below by a positive constant depending only on the
dimension. In this note, we prove an inequality that serves as an umbrella for
these results","Title: Disparities in the Radon Transformation Over Convex Structures

Reconstructed Abstract: A few years back, the researchers initiated an investigation into certain issues of convex geometry, adopting a broader perspective by substituting volume with an arbitrary measure. This innovative methodology unveiled novel general characteristics of the Radon transform applied to convex objects, incorporating an expansion of the Busemann-Petty problem and a slicing inequality applicable to arbitrary functions. The latter implies that the supreme norm of the Radon transform of any probability density, existing on a convex structure of volume one, is constrained from below by an unchanging positive constant, relying solely on the dimension. In this communication, we authenticate an inequality that functions as a comprehensive framework for these findings.",oai:arXiv.org:2101.00287,2021-01-01,"['math.MG', 'math.FA']"
"Combination of component fault trees and Markov chains to analyze
  complex, software-controlled systems","Fault Tree analysis is a widely used failure analysis methodology to assess a
system in terms of safety or reliability in many industrial application
domains. However, with Fault Tree methodology there is no possibility to
express a temporal sequence of events or state-dependent behavior of
software-controlled systems. In contrast to this, Markov Chains are a
state-based analysis technique based on a stochastic model. But the use of
Markov Chains for failure analysis of complex safety-critical systems is
limited due to exponential explosion of the size of the model. In this paper,
we present a concept to integrate Markov Chains in Component Fault Tree models.
Based on a component concept for Markov Chains, which enables the association
of Markov Chains to system development elements such as components, complex or
software-controlled systems can be analyzed w.r.t. safety or reliability in a
modular and compositional way. We illustrate this approach using a case study
from the automotive domain.","Title: Integrating Component Fault Trees and Markov Chains for the Analysis of Complex, Software-Controlled Systems

Revised Abstract: The prevalent failure analysis technique, Fault Tree Analysis, is extensively utilized for the evaluation of safety and reliability within various industrial sectors. Yet, this methodology lacks the capacity to articulate the progression of events over time or the state-dependent conduct of software-controlled mechanisms. Alternatively, Markov Chains, founded on a stochastic model, offer a state-based analysis technique. However, their application to failure analysis in intricate safety-critical systems is hindered by the exponential expansion of the model size. This paper proposes a novel approach that incorporates Markov Chains into Component Fault Tree models, using a component-based concept for Markov Chains. This concept allows for the attachment of Markov Chains to elements of system development, such as components, thereby enabling the analysis of complex or software-controlled systems in terms of safety or reliability in a modular and compositional manner. This method is exemplified through a case study in the automotive sector.",oai:arXiv.org:2106.00247,2021-06-01,['cs.SE']
"Secure Transmission with Different Security Requirements Based on Covert
  Communication and Information-Theoretic Security in Presence of Friendly
  Jammer","In this paper, we investigate joint information-theoretic security and covert
communication on a network in the presence of a single transmitter (Alice), a
friendly jammer, a single untrusted user, two legitimate users, and a single
warden of the channel (Willie). In the considered network, one of the
authorized users, Bob, needs a secure and covert communication, and therefore
his message must be sent securely, and at the same time, the existence of his
communication with the transmitter should not be detected by the channel's
warden, Willie, Meanwhile, another authorized user, Carol, needs covert
communication. The purpose of secure communication is to prevent the message
being decoded by the untrusted user who is present on the network, which leads
us to use one of the physical layer security methods, named the secure
transmission of information theory. In some cases, in addition to protecting
the content of the message, it is important for the user that the existence of
the transmission not being detected by an adversary, which leads us to covert
communication. In the proposed network model, it is assumed that for covert
communication requirements, Alice will not send any messages to legitimate
users in one time slot and in another time slot will send to them both (Bob and
Carol). One of the main challenges in covert communication is low transmission
rate, because we have to reduce the transmission power such that the main
message get hide in background noise.","Title: Ensuring Secure and Covert Communication via Information-Theoretic Security in a Network with a Friendly Jammer

Revised Abstract: This study delves into the intersection of information-theoretic security and covert communication within a network involving a single transmitter (Alice), a friendly jammer, an untrusted user, two authorized users, and a single channel warden (Willie). The network framework under consideration involves one authorized user, Bob, who requires both security and covert nature for his communication. As such, not only should his message be transmitted securely, but the transmission itself should also remain undetected by Willie, the channel warden. Moreover, another authorized user, Carol, requires covert communication. The objective of secure communication is to safeguard the message from being deciphered by the untrusted user within the network, thereby necessitating the use of secure transmission of information theory, a method of physical layer security. In certain instances, it is essential to prevent the detection of the transmission itself, not merely protecting the message content, thus leading to the deployment of covert communication. The proposed network model presumes that Alice will cease communication with the authorized users in one time slot and resume in another with both Bob and Carol to meet the covert communication requirements. The primary challenge of covert communication lies in managing the low transmission rate, since the transmission power must be minimized to conceal the primary message within the background noise.",oai:arXiv.org:2107.00210,2021-07-01,['eess.SP']
"Developing a Compressed Object Detection Model based on YOLOv4 for
  Deployment on Embedded GPU Platform of Autonomous System","Latest CNN-based object detection models are quite accurate but require a
high-performance GPU to run in real-time. They still are heavy in terms of
memory size and speed for an embedded system with limited memory space. Since
the object detection for autonomous system is run on an embedded processor, it
is preferable to compress the detection network as light as possible while
preserving the detection accuracy. There are several popular lightweight
detection models but their accuracy is too low for safe driving applications.
Therefore, this paper proposes a new object detection model, referred as
YOffleNet, which is compressed at a high ratio while minimizing the accuracy
loss for real-time and safe driving application on an autonomous system. The
backbone network architecture is based on YOLOv4, but we could compress the
network greatly by replacing the high-calculation-load CSP DenseNet with the
lighter modules of ShuffleNet. Experiments with KITTI dataset showed that the
proposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could
achieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).
Compared to the high compression ratio, the accuracy is reduced slightly to
85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network
showed a high potential to be deployed on the embedded system of the autonomous
system for the real-time and accurate object detection applications.","Title: Creation of a Highly Compressed Object Detection Model, YOffleNet, Based on YOLOv4 for Utilization on Autonomous System's Embedded GPU Platform

Modified Abstract: Modern object detection models based on Convolutional Neural Networks (CNN) provide high accuracy but necessitate a potent GPU to facilitate real-time operations. However, their extensive memory size and speed requirements pose a challenge for embedded systems, which typically have limited memory capacity. Given that autonomous systems use embedded processors for object detection, it is advantageous to develop a lightweight detection network that retains detection precision. Although there exist several renowned lightweight models, their accuracy levels fall short for applications necessitating safe driving. Consequently, this research introduces a novel object detection model, YOffleNet. This model is highly compressed while ensuring minimal loss in accuracy, making it ideal for real-time and safe driving operations within an autonomous system. The underlying network architecture is YOLOv4-based, but significant network compression is achieved by substituting the computationally intensive CSP DenseNet with the lighter ShuffleNet modules. Trials using the KITTI dataset reveal that the proposed YOffleNet model is 4.7 times more compressed than YOLOv4-s, achieving speeds of up to 46 FPS on an embedded GPU system, specifically the NVIDIA Jetson AGX Xavier. Despite a high compression ratio, accuracy only dips slightly to 85.8% mAP, a mere 2.6% decrement from YOLOv4-s. Therefore, the newly proposed network demonstrates considerable promise for deployment in autonomous systems' embedded platforms, offering real-time and accurate object detection.",oai:arXiv.org:2108.00392,2021-08-01,['cs.CV']
"Incorporating Transformer and LSTM to Kalman Filter with EM algorithm
  for state estimation","Kalman Filter requires the true parameters of the model and solves optimal
state estimation recursively. Expectation Maximization (EM) algorithm is
applicable for estimating the parameters of the model that are not available
before Kalman filtering, which is EM-KF algorithm. To improve the preciseness
of EM-KF algorithm, the author presents a state estimation method by combining
the Long-Short Term Memory network (LSTM), Transformer and EM-KF algorithm in
the framework of Encoder-Decoder in Sequence to Sequence (seq2seq). Simulation
on a linear mobile robot model demonstrates that the new method is more
accurate. Source code of this paper is available at
https://github.com/zshicode/Deep-Learning-Based-State-Estimation.","Title: Integration of Transformer and LSTM with Kalman Filter via the EM Algorithm for Enhanced State Estimation

Revised Abstract: The Kalman Filter is typically reliant on the actual parameters of a model for recursive optimal state estimation. However, the Expectation Maximization (EM) algorithm can be employed to estimate these parameters before the application of the Kalman filter, leading to the EM-KF algorithm. To augment the accuracy of the EM-KF algorithm, this paper introduces a state estimation technique that amalgamates the Long-Short Term Memory network (LSTM), Transformer, and EM-KF algorithm within the context of an Encoder-Decoder in the Sequence to Sequence (seq2seq) schema. A simulation conducted on a linear mobile robot model substantiates that this innovative method offers superior accuracy. The paper's source code can be accessed at https://github.com/zshicode/Deep-Learning-Based-State-Estimation.",oai:arXiv.org:2105.00250,2021-05-01,['cs.LG']
"Inequality in economic shock exposures across the global firm-level
  supply network","For centuries, national economies created wealth by engaging in international
trade and production. The resulting international supply networks not only
increase wealth for countries, but also create systemic risk: economic shocks,
triggered by company failures in one country, may propagate to other countries.
Using global supply network data on the firm-level, we present a method to
estimate a country's exposure to direct and indirect economic losses caused by
the failure of a company in another country. We show the network of systemic
risk-flows across the world. We find that rich countries expose poor countries
much more to systemic risk than the other way round. We demonstrate that higher
systemic risk levels are not compensated with a risk premium in GDP, nor do
they correlate with economic growth. Systemic risk around the globe appears to
be distributed more unequally than wealth. These findings put the often praised
benefits for developing countries from globalized production in a new light,
since they relate them to the involved risks in the production processes.
Exposure risks present a new dimension of global inequality, that most affects
the poor in supply shock crises. It becomes fully quantifiable with the
proposed method.","Title: Economic Shock Disparities in International Firm-Level Supply Chains

Revised Abstract: Over the years, global economies have amassed wealth through participation in international commerce and production. This has led to the establishment of an intricate web of global supply networks that, while profitable, present a systemic risk. Economic disruptions, initiated by the collapse of businesses in one nation, can ripple outwards, impacting other nations. Utilising data from international supply networks at a firm-level, this study introduces a methodology to gauge a nation's vulnerability to both direct and indirect financial damages arising from a company's failure in a different country. Our findings illustrate the global network of systemic risk currents and reveal a greater exposure of impoverished countries to systemic risks from affluent nations, as opposed to the reverse. Contrary to expectations, we found no correlation between higher systemic risk levels and an increase in GDP risk premiums or economic growth. Interestingly, systemic risk appears to be more unevenly distributed worldwide than wealth itself. These results offer a fresh perspective on the purported benefits of globalized production for developing nations by highlighting the associated production risks. The exposure to these risks presents a novel facet of global inequality, which disproportionately influences the economically disadvantaged during supply shock crises. This exposure can now be fully quantified using the proposed method.",oai:arXiv.org:2112.00415,2021-12-01,"['econ.GN', 'physics.soc-ph', 'q-fin.EC']"
"Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene
  Recognition","Accurate perception of the surrounding scene is helpful for robots to make
reasonable judgments and behaviours. Therefore, developing effective scene
representation and recognition methods are of significant importance in
robotics. Currently, a large body of research focuses on developing novel
auxiliary features and networks to improve indoor scene recognition ability.
However, few of them focus on directly constructing object features and
relations for indoor scene recognition. In this paper, we analyze the
weaknesses of current methods and propose an Object-to-Scene (OTS) method,
which extracts object features and learns object relations to recognize indoor
scenes. The proposed OTS first extracts object features based on the
segmentation network and the proposed object feature aggregation module (OFAM).
Afterwards, the object relations are calculated and the scene representation is
constructed based on the proposed object attention module (OAM) and global
relation aggregation module (GRAM). The final results in this work show that
OTS successfully extracts object features and learns object relations from the
segmentation network. Moreover, OTS outperforms the state-of-the-art methods by
more than 2\% on indoor scene recognition without using any additional streams.
Code is publicly available at: https://github.com/FreeformRobotics/OTS.","Title: From Object to Scene: Enhancing Indoor Scene Recognition by Applying Object Knowledge 

Revised Abstract: For robots to form appropriate judgements and actions, a precise understanding of their surrounding environment is crucial. As a result, the development of efficient techniques for scene representation and recognition is of paramount importance in the field of robotics. Presently, the majority of research in this area is centered around the creation of innovative auxiliary features and networks to enhance the capability for indoor scene identification. However, there is a lack of focus on the direct formation of object features and their relationships for indoor scene recognition. This paper scrutinizes the shortcomings of current methodologies and introduces an Object-to-Scene (OTS) approach. This approach focuses on the extraction of object features and the learning of object relationships in order to recognize indoor scenes. Initially, the OTS technique extracts object features using a segmentation network and a newly proposed object feature aggregation module (OFAM). Following this, object relationships are determined, and the scene representation is formed using the newly introduced object attention module (OAM) and global relation aggregation module (GRAM). The ultimate findings of this research indicate that the OTS method successfully extracts object features and learns object relationships through the segmentation network, surpassing leading methods by over 2% in indoor scene recognition without the need for additional streams. The code for this research is openly accessible at: https://github.com/FreeformRobotics/OTS.",oai:arXiv.org:2108.00399,2021-08-01,['cs.CV']
"Volta at SemEval-2021 Task 9: Statement Verification and Evidence
  Finding with Tables using TAPAS and Transfer Learning","Tables are widely used in various kinds of documents to present information
concisely. Understanding tables is a challenging problem that requires an
understanding of language and table structure, along with numerical and logical
reasoning. In this paper, we present our systems to solve Task 9 of
SemEval-2021: Statement Verification and Evidence Finding with Tables
(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a
statement, predicting whether the table supports the statement and (B)
Predicting which cells in the table provide evidence for/against the statement.
We fine-tune TAPAS (a model which extends BERT's architecture to capture
tabular structure) for both the subtasks as it has shown state-of-the-art
performance in various table understanding tasks. In subtask A, we evaluate how
transfer learning and standardizing tables to have a single header row improves
TAPAS' performance. In subtask B, we evaluate how different fine-tuning
strategies can improve TAPAS' performance. Our systems achieve an F1 score of
67.34 in subtask A three-way classification, 72.89 in subtask A two-way
classification, and 62.95 in subtask B.","Title: Utilizing TAPAS and Transfer Learning for Task 9 of SemEval-2021: A Study on Statement Verification and Evidence Discovery with Tables

Revised Abstract: Tables, ubiquitously found in a multitude of documents, serve to succinctly display information. The task of understanding tables poses significant challenges as it necessitates comprehension of the language, table structure, and numerical as well as logical reasoning. This study details our developed systems intended to tackle Task 9 of SemEval-2021, namely, Statement Verification and Evidence Finding with Tables (SEM-TAB-FACTS). The task is bifurcated into two subtasks: Subtask A, which involves predicting if a given table substantiates a statement and Subtask B, which involves identifying the table cells providing evidence supporting or contradicting the statement. Our approach utilizes fine-tuning of the TAPAS model - an extension of the BERT model specifically designed to comprehend tabular structures - for both subtasks, leveraging its proven excellence in numerous table comprehension tasks. In Subtask A, we assess the impact of transfer learning and table standardization into a single header row on enhancing TAPAS' effectiveness. For Subtask B, we examine the potential improvement in TAPAS' performance through varied fine-tuning methodologies. Our systems have demonstrated commendable performance, scoring an F1 of 67.34 in Subtask A's three-way classification, 72.89 in Subtask A's two-way classification, and 62.95 in Subtask B.",oai:arXiv.org:2106.00248,2021-06-01,['cs.CL']
Multi-Access Coded Caching with Demand Privacy,"The demand private coded caching problem in a multi-access network with $K$
users and $K$ caches, where each user has access to $L$ neighbouring caches in
a cyclic wrap-around manner, is studied. The additional constraint imposed is
that one user should not get any information regarding the demands of the
remaining users. A lifting construction of demand private multi-access coded
caching scheme from conventional, non-private multi-access scheme is
introduced. The demand-privacy for a user is ensured by placing some additional
\textit{keys} in a set of caches called the \textit{private set} of that user.
For a given $K$ and $L$, a technique is also devised to find the private sets
of the users.","Title: Examination of Multi-Access Coded Caching in the Context of Demand Privacy

Revised Abstract: The study explores the problem of demand private coded caching within a multi-access network comprising of $K$ users and $K$ caches, in which each user can access $L$ adjacent caches through a cyclical wrap-around approach. It is further stipulated that one user must not obtain any details pertaining to the demands of other users. A novel method of lifting construction is presented, allowing for a transition from a standard, non-private multi-access scheme to a demand private multi-access coded caching plan. To ensure the demand-privacy for each user, additional \textit{keys} are strategically placed within a group of caches, termed as the \textit{private set} for that user. Furthermore, a procedure has been developed to identify the private sets of the users, given the values of $K$ and $L$.",oai:arXiv.org:2107.00226,2021-07-01,"['cs.IT', 'math.IT']"
"Multilingual Central Repository: a Cross-lingual Framework for
  Developing Wordnets","Language resources are necessary for language processing,but building them is
costly, involves many researches from different areas and needs constant
updating. In this paper, we describe the crosslingual framework used for
developing the Multilingual Central Repository (MCR), a multilingual knowledge
base that includes wordnets of Basque, Catalan, English, Galician, Portuguese,
Spanish and the following ontologies: Base Concepts, Top Ontology, WordNet
Domains and Suggested Upper Merged Ontology. We present the story of MCR, its
state in 2017 and the developed tools.","Title: Development of a Multilingual Knowledge Base: A Cross-Lingual Approach for Wordnet Construction

Rewritten Abstract: The establishment of linguistic resources is integral to language processing, albeit being resource-intensive, requiring interdisciplinary research, and necessitating regular updates. This study details the cross-lingual strategy employed in constructing the Multilingual Central Repository (MCR), a comprehensive multilingual knowledge base, incorporating wordnets for languages such as Basque, Catalan, English, Galician, Portuguese, and Spanish. Additionally, the MCR includes ontologies such as Base Concepts, Top Ontology, WordNet Domains, and Suggested Upper Merged Ontology. This paper also charts the evolution of MCR, its status in 2017, and the tools developed for its construction.",oai:arXiv.org:2107.00333,2021-07-01,['cs.CL']
"Algorithms and Complexity for Counting Configurations in Steiner Triple
  Systems","Steiner triple systems form one of the most studied classes of combinatorial
designs. Configurations, including subsystems, play a central role in the
investigation of Steiner triple systems. With sporadic instances of small
systems, ad-hoc algorithms for counting or listing configurations are typically
fast enough for practical needs, but with many systems or large systems, the
relevance of computational complexity and algorithms of low complexity is
highlighted. General theoretical results as well as specific practical
algorithms for important configurations are presented.","Title: Examination of Algorithms and Complexity in Configurations of Steiner Triple Systems

Revised Abstract: The domain of Steiner triple systems, a widely scrutinized class of combinatorial designs, is profoundly influenced by configurations, including subsystems. The study of these systems often utilizes ad-hoc algorithms for enumerating configurations, which prove efficient for infrequent instances of small systems. However, the importance of computational complexity and low-complexity algorithms becomes evident when dealing with numerous or large-scale systems. This paper elucidates both broad theoretical findings and specific practical algorithms associated with critical configurations.",oai:arXiv.org:2110.00320,2021-10-01,['math.CO']
On the number of $q$-ary quasi-perfect codes with covering radius 2,"In this paper we present a family of $q$-ary nonlinear quasi-perfect codes
with covering radius 2. The codes have length $n = q^m$ and size $ M = q^{n - m
- 1}$ where $q$ is a prime power, $q \geq 3$, $m$ is an integer, $m \geq 2$. We
prove that there are more than $q^{q^{cn}}$ nonequivalent such codes of length
$n$, for all sufficiently large $n$ and a constant $c = \frac{1}{q} -
\varepsilon$.","Title: Regarding the Quantity of $q$-ary Quasi-Perfect Codes with a Covering Radius of 2

Rewritten Abstract: This study introduces a collection of $q$-ary nonlinear quasi-perfect codes that possess a covering radius of 2. These codes are defined by their length $n = q^m$ and their size $ M = q^{n - m - 1}$, where $q$ represents a prime power that is equal to or greater than 3, and $m$ is an integer equal to or exceeding 2. We establish that the quantity of these nonequivalent codes of length $n$ exceeds $q^{q^{cn}}$ for all adequately large values of $n$, given a constant $c = \frac{1}{q} - \varepsilon$.",oai:arXiv.org:2111.00774,2021-11-01,"['cs.IT', 'math.IT']"
"Collision Chains among the Terrestrial Planets. II. An Asymmetry between
  Earth and Venus","During the late stage of terrestrial planet formation, hit-and-run collisions
are about as common as accretionary mergers, for expected velocities and angles
of giant impacts. Average hit-and-runs leave two major remnants plus debris:
the target and impactor, somewhat modified through erosion, escaping at lower
relative velocity. Here we continue our study of the dynamical effects of such
collisions. We compare the dynamical fates of intact runners that start from
hit-and-runs with proto-Venus at 0.7 AU and proto-Earth at 1.0 AU. We follow
the orbital evolutions of the runners, including the other terrestrial planets,
Jupiter, and Saturn, in an N-body code. We find that the accretion of these
runners can take $\gtrsim$10 Myr (depending on the egress velocity of the first
collision) and can involve successive collisions with the original target
planet or with other planets. We treat successive collisions that the runner
experiences using surrogate models from machine learning, as in previous work,
and evolve subsequent hit-and-runs in a similar fashion. We identify
asymmetries in the capture, loss, and interchange of runners in the growth of
Venus and Earth. Hit-and-run is a more probable outcome at proto-Venus, being
smaller and faster orbiting than proto-Earth. But Venus acts as a sink,
eventually accreting most of its runners, assuming typical events, whereas
proto-Earth loses about half, many of those continuing to Venus. This leads to
a disparity in the style of late-stage accretion that could have led to
significant differences in geology, composition, and satellite formation at
Earth and Venus.","Title: Comparative Analysis of Collision Sequences Among Terrestrial Planets: Unequal Consequences for Earth and Venus 

Revised Abstract: The concluding stages of terrestrial planet formation observe a comparable frequency of hit-and-run collisions and accretionary mergers, considering the predicted velocities and impact angles of enormous collisions. On average, such collisions result in two primary remnants and associated debris - the target and the impactor, slightly reshaped due to erosion, escaping at a decreased relative velocity. This paper extends our investigation into the dynamic impacts of these collisions. We examine the dynamic outcomes of intact remnants originating from hit-and-run collisions involving proto-Venus at 0.7 AU and proto-Earth at 1.0 AU. By tracking the orbital evolutions of these remnants, and taking into account other terrestrial planets, Jupiter, and Saturn, using an N-body code, we discover that the accretion of these remnants can necessitate over 10 Myr, depending on the escape velocity of the initial collision. This process could include numerous collisions with the initial target planet or other planets. We use surrogate machine learning models to manage consecutive collisions experienced by the remnants, similar to previous studies, and progress subsequent hit-and-run events likewise. We expose disparities in the absorption, loss, and exchange of remnants during the growth of Venus and Earth. Hit-and-run is a more likely result at proto-Venus, due to its smaller size and quicker orbit compared to proto-Earth. However, Venus acts as a gravitational well, eventually accreting the majority of its remnants, given typical scenarios, while proto-Earth loses approximately half, many of which continue to Venus. This results in an imbalance in the late-stage accretion process, potentially leading to significant divergences in geology, composition, and satellite formation between Earth and Venus.",oai:arXiv.org:2110.00221,2021-10-01,['astro-ph.EP']
"Synthetic Design: An Optimization Approach to Experimental Design with
  Synthetic Controls","We investigate the optimal design of experimental studies that have
pre-treatment outcome data available. The average treatment effect is estimated
as the difference between the weighted average outcomes of the treated and
control units. A number of commonly used approaches fit this formulation,
including the difference-in-means estimator and a variety of synthetic-control
techniques. We propose several methods for choosing the set of treated units in
conjunction with the weights. Observing the NP-hardness of the problem, we
introduce a mixed-integer programming formulation which selects both the
treatment and control sets and unit weightings. We prove that these proposed
approaches lead to qualitatively different experimental units being selected
for treatment. We use simulations based on publicly available data from the US
Bureau of Labor Statistics that show improvements in terms of mean squared
error and statistical power when compared to simple and commonly used
alternatives such as randomized trials.","Title: Experimental Design Optimization: A Synthetic Control Approach 

Revised Abstract: This research probes the ideal construction of experimental studies that provide pre-treatment outcome data. The general treatment effect is gauged as the divergence between the weighted mean outcomes of the treated and control units. This formulation is consistent with several prevalent approaches, incorporating the difference-in-means estimator and various synthetic control methods. We introduce multiple strategies for determining the treated units in relation to the weights. Acknowledging the NP- hardness of the issue, we incorporate a mixed-integer programming formulation that simultaneously selects the treatment and control groups and their respective unit weightings. We demonstrate that our proposed methods result in the selection of experimentally distinct units for treatment. Using simulations drawn from publicly accessible data from the US Bureau of Labor Statistics, we illustrate enhancements in terms of mean squared error and statistical power when juxtaposed with basic and frequently employed alternatives such as randomized trials.",oai:arXiv.org:2112.00278,2021-12-01,"['stat.ME', 'stat.ML']"
NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning,"Offline reinforcement learning (RL) aims at learning a good policy from a
batch of collected data, without extra interactions with the environment during
training. However, current offline RL benchmarks commonly have a large reality
gap, because they involve large datasets collected by highly exploratory
policies, and the trained policy is directly evaluated in the environment. In
real-world situations, running a highly exploratory policy is prohibited to
ensure system safety, the data is commonly very limited, and a trained policy
should be well validated before deployment. In this paper, we present a near
real-world offline RL benchmark, named NeoRL, which contains datasets from
various domains with controlled sizes, and extra test datasets for policy
validation. We evaluate existing offline RL algorithms on NeoRL and argue that
the performance of a policy should also be compared with the deterministic
version of the behavior policy, instead of the dataset reward. The empirical
results demonstrate that the tested offline RL algorithms become less
competitive to the deterministic policy on many datasets, and the offline
policy evaluation hardly helps. The NeoRL suit can be found at
http://polixir.ai/research/neorl. We hope this work will shed some light on
future research and draw more attention when deploying RL in real-world
systems.","Title: NeoRL: Establishing a More Realistic Standard for Offline Reinforcement Learning

Revised Abstract: Offline Reinforcement Learning (RL) is designed to learn an effective policy from an accumulated data set, without requiring additional interaction with the environment during the training phase. Nonetheless, the vast reality gap in current offline RL benchmarks is a significant issue due to their reliance on sizeable data collections gathered through highly exploratory policies, and the immediate evaluation of the trained policy in the environment. In real-world scenarios, the execution of highly exploratory policies is restricted to maintain system safety, data availability is typically limited, and it is crucial to thoroughly validate a trained policy prior to implementation. This study introduces NeoRL, a nearly real-world offline RL benchmark that encompasses controlled-size datasets from diverse domains and supplementary test datasets for policy validation. We assess existing offline RL algorithms on NeoRL and propose that policy performance should be juxtaposed with the deterministic version of the behavior policy, rather than the dataset reward. Our empirical findings indicate that the assessed offline RL algorithms are less competitive compared to the deterministic policy across numerous datasets, and offline policy evaluation offers little assistance. The NeoRL suite can be accessed at http://polixir.ai/research/neorl. We anticipate that our research will encourage further investigation and increased vigilance in the implementation of RL in real-world systems.",oai:arXiv.org:2102.00714,2021-02-01,"['cs.LG', 'cs.AI']"
Holonomic functions and prehomogeneous spaces,"A function that is analytic on a domain of $\mathbb{C}^n$ is holonomic if it
is the solution to a holonomic system of linear homogeneous differential
equations with polynomial coefficients. We define and study the Bernstein-Sato
polynomial of a holonomic function on a smooth algebraic variety. We analyze
the structure of certain sheaves of holonomic functions, such as the algebraic
functions along a hypersurface, determining their direct sum decompositions
into indecomposables, that further respect decompositions of Bernstein-Sato
polynomials. When the space is endowed with the action of a linear algebraic
group $G$, we study the class of $G$-finite analytic functions, i.e. functions
that under the action of the Lie algebra of $G$ generate a finite dimensional
rational $G$-module. These are automatically algebraic functions on a variety
with a dense orbit. When $G$ is reductive, we give several
representation-theoretic techniques toward the determination of Bernstein-Sato
polynomials of $G$-finite functions. We classify the $G$-finite functions on
all but one of the irreducible reduced prehomogeneous vector spaces, and
compute the Bernstein-Sato polynomials for distinguished $G$-finite functions.
The results can be used to construct explicitly equivariant
$\mathcal{D}$-modules.","Title: Examination of Holonomic Functions and Prehomogeneous Spaces

In this research, we explore the concept of a holonomic function as a solution to a holonomic system of linear homogeneous differential equations with polynomial coefficients, within a domain of $\mathbb{C}^n$ where the function is analytic. We delve into the Bernstein-Sato polynomial of a holonomic function situated on a smooth algebraic variety, elucidating its definition and properties. The study further scrutinizes the architecture of specific sheaves of holonomic functions, such as the algebraic functions present on a hypersurface, with an emphasis on the delineation of their direct sum decompositions into indecomposables that adhere to the decomposition of Bernstein-Sato polynomials. In spaces where a linear algebraic group $G$ is operational, we investigate the category of $G$-finite analytic functions - functions that generate a finite dimensional rational $G$-module under the influence of $G$'s Lie algebra. These functions are inherently algebraic on a variety with a dense orbit. In reductive $G$ instances, we present several representation-theoretic methods to identify Bernstein-Sato polynomials of $G$-finite functions. We categorize the $G$-finite functions on nearly all irreducible reduced prehomogeneous vector spaces, besides one, and compute Bernstein-Sato polynomials for notable $G$-finite functions. The findings offer a foundation to construct unequivocally equivariant $\mathcal{D}$-modules.",oai:arXiv.org:2102.00766,2021-02-01,"['math.AG', 'math.RT']"
Combating small molecule aggregation with machine learning,"Biological screens are plagued by false positive hits resulting from
aggregation. Thus, methods to triage small colloidally aggregating molecules
(SCAMs) are in high demand. Herein, we disclose a bespoke machine-learning tool
to confidently and intelligibly flag such entities. Our data demonstrate an
unprecedented utility of machine learning for predicting SCAMs, achieving 80%
of correct predictions in a challenging out-of-sample validation. The tool
outperformed a panel of expert chemists, who correctly predicted 61 +/- 7% of
the same test molecules in a Turing-like test. Further, the computational
routine provided insight into molecular features governing aggregation that had
remained hidden to expert intuition. Leveraging our tool, we quantify that up
to 15-20% of ligands in publicly available chemogenomic databases have the high
potential to aggregate at typical screening concentrations, imposing caution in
systems biology and drug design programs. Our approach provides a means to
augment human intuition, mitigate attrition and a pathway to accelerate future
molecular medicine.","Title: Addressing the Issue of Small Molecule Aggregation Through Machine Learning

Revised Abstract: The occurrence of false positives in biological screenings due to aggregation has become a significant concern. Therefore, there is a pressing need for tools that can effectively identify small colloidally aggregating molecules (SCAMs). In this study, we introduce a specialized machine-learning tool, designed to accurately and comprehensibly detect these aggregating entities. The data presented showcases the exceptional capability of machine learning in predicting SCAMs, with an 80% accuracy rate in a demanding out-of-sample validation. This tool surpassed the performance of a group of expert chemists who managed to correctly predict only 61 +/- 7% of the same test molecules in a comparable Turing-like test. Moreover, the computational method offered insights into molecular features that influence aggregation, which were previously undetectable by expert intuition. Utilizing this tool, we deduced that approximately 15-20% of ligands in publicly accessible chemogenomic databases exhibit a high propensity to aggregate at usual screening concentrations, indicating the need for caution in systems biology and drug design projects. Our methodology offers a solution to enhance human intuition, reduce attrition, and pave the way for expediting future molecular medicine.",oai:arXiv.org:2105.00267,2021-05-01,"['q-bio.QM', 'cs.LG']"
"Generalized torsion for hyperbolic $3$--manifold groups with arbitrary
  large rank","Let $G$ be a group and $g$ a non-trivial element in $G$. If some non-empty
finite product of conjugates of $g$ equals to the trivial element, then $g$ is
called a generalized torsion element. To the best of our knowledge, we have no
hyperbolic $3$--manifold groups with generalized torsion elements whose rank is
explicitly known to be greater than two. The aim of this short note is to
demonstrate that for a given integer $n > 1$ there are infinitely many closed
hyperbolic $3$--manifolds $M_n$ which enjoy the property: (i) the Heegaard
genus of $M_n$ is $n$, (ii) the rank of the fundamental group of $M_n$ is $n$,
and (ii) the fundamental group of $M_n$ has a generalized torsion element.
Furthermore, we may choose $M_n$ as homology lens spaces and so that the order
of the generalized torsion element is arbitrarily large.","Title: The Extended Scope of Torsion in Hyperbolic 3-Manifold Groups of Indefinite Large Rank

Revised Abstract: Suppose that $G$ is a group and a non-trivial element, $g$, resides in $G$. The element $g$ is classified as a generalized torsion element if any finite non-null product of its conjugates equals the trivial element. To date, our understanding lacks any hyperbolic 3-manifold groups containing generalized torsion elements with a rank known to exceed two. This brief communication endeavors to prove that for any given integer $n > 1$, there exists an infinite number of closed hyperbolic 3-manifolds $M_n$ that possess the following characteristics: (i) the Heegaard genus of $M_n$ equals $n$, (ii) the fundamental group of $M_n$ has a rank of $n$, and (iii) the fundamental group of $M_n$ includes a generalized torsion element. Additionally, we can select $M_n$ as homology lens spaces, and the generalized torsion element's order can be chosen to be arbitrarily large.",oai:arXiv.org:2112.00418,2021-12-01,"['math.GT', 'math.GR']"
"Improving Automatic Hate Speech Detection with Multiword Expression
  Features","The task of automatically detecting hate speech in social media is gaining
more and more attention. Given the enormous volume of content posted daily,
human monitoring of hate speech is unfeasible. In this work, we propose new
word-level features for automatic hate speech detection (HSD): multiword
expressions (MWEs). MWEs are lexical units greater than a word that have
idiomatic and compositional meanings. We propose to integrate MWE features in a
deep neural network-based HSD framework. Our baseline HSD system relies on
Universal Sentence Encoder (USE). To incorporate MWE features, we create a
three-branch deep neural network: one branch for USE, one for MWE categories,
and one for MWE embeddings. We conduct experiments on two hate speech tweet
corpora with different MWE categories and with two types of MWE embeddings,
word2vec and BERT. Our experiments demonstrate that the proposed HSD system
with MWE features significantly outperforms the baseline system in terms of
macro-F1.","Title: Enhancement of Automated Hate Speech Identification through the Utilization of Multiword Expression Characteristics

Revised Abstract: The growing focus on the automatic identification of hate speech in social media platforms necessitates more effective detection techniques. Given the impracticality of manually monitoring the sheer volume of content shared daily, we suggest the introduction of novel word-level attributes for automated hate speech identification (HSD): multiword expressions (MWEs). These are lexical entities that exceed a single word and carry both idiomatic and compositional implications. We advocate for the incorporation of these MWE elements into a deep learning-based HSD structure. Our initial HSD system is dependent on Universal Sentence Encoder (USE). To implement MWE characteristics, we establish a tripartite deep learning network: one segment for USE, one for MWE types, and one for MWE embeddings. We carry out tests on two separate hate speech Twitter datasets with varying MWE types and two variants of MWE embeddings - word2vec and BERT. Our studies reveal that the suggested HSD mechanism, outfitted with MWE characteristics, notably surpasses the original system in macro-F1 performance.",oai:arXiv.org:2106.00237,2021-06-01,['cs.CL']
Multi-Messenger studies with the Pierre Auger Observatory,"Over the past decade the multi-messenger astrophysics has emerged as a
distinct discipline, providing unique insights into the properties of
high-energy phenomena in the Universe. The Pierre Auger Observatory, located in
Malarg\""ue, Argentina, is the world's largest cosmic ray detector sensitive to
photons, neutrinos, and hadrons at ultra-high energies. Using its data,
stringent limits on photon and neutrino fluxes at EeV energies have been
obtained. The collaboration uses the excellent angular resolution and the
neutrino identification capabilities of the Observatory for follow-up studies
of events detected in gravitational waves or other messengers, through
cooperation with global multi-messenger networks. We present a science
motivation together with an overview of the multi-messenger capabilities and
results of the Pierre Auger Observatory.","Title: Comprehensive Analysis Utilizing the Pierre Auger Observatory in Multi-Messenger Studies 

In the recent decade, multi-messenger astrophysics has evolved into a distinct field, offering unparalleled perspectives into the characteristics of high-energy cosmic events. Situated in Malargüe, Argentina, the Pierre Auger Observatory stands as the most extensive cosmic ray detector globally, with the capacity to detect photons, neutrinos, and hadrons at extreme energies. The application of its data has led to the establishment of strict boundaries on photon and neutrino fluxes at EeV energies. The collaborative team harnesses the exceptional angular resolution and neutrino identification abilities of the Observatory to conduct supplementary studies of incidents detected in gravitational waves or alternate messengers, through alliances with international multi-messenger networks. In this paper, we provide the scientific rationale, a comprehensive review of the multi-messenger capacities, and the resultant findings of the Pierre Auger Observatory.",oai:arXiv.org:2102.00828,2021-02-01,['astro-ph.HE']
A New Tool for Efficiently Generating Quality Estimation Datasets,"Building of data for quality estimation (QE) training is expensive and
requires significant human labor. In this study, we focus on a data-centric
approach while performing QE, and subsequently propose a fully automatic
pseudo-QE dataset generation tool that generates QE datasets by receiving only
monolingual or parallel corpus as the input. Consequently, the QE performance
is enhanced either by data augmentation or by encouraging multiple language
pairs to exploit the applicability of QE. Further, we intend to publicly
release this user friendly QE dataset generation tool as we believe this tool
provides a new, inexpensive method to the community for developing QE datasets.","Title: An Innovative Approach to Cost-Effective Creation of Quality Estimation Datasets

Generated Abstract: The process of creating datasets for quality estimation (QE) training is both time-consuming and costly, necessitating a substantial amount of human effort. Our research concentrates on a method driven by data during the QE process, subsequently introducing a fully automated tool for generating pseudo-QE datasets. This tool operates by accepting either a monolingual or parallel corpus as its sole input, thereby generating QE datasets. As a result, the QE performance is improved, either through data augmentation or by enabling multiple language pairs to utilize the potential of QE. We plan to make this user-friendly QE dataset creation tool available to the public, as we are confident it offers the academic community a novel and economical technique for the creation of QE datasets.",oai:arXiv.org:2111.00767,2021-11-01,['cs.CL']
"VA-GCN: A Vector Attention Graph Convolution Network for learning on
  Point Clouds","Owing to the development of research on local aggregation operators, dramatic
breakthrough has been made in point cloud analysis models. However, existing
local aggregation operators in the current literature fail to attach decent
importance to the local information of the point cloud, which limits the power
of the models. To fit this gap, we propose an efficient Vector Attention
Convolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract
the neighbor points of each input point, and then uses the elevation and
azimuth relationship of the vectors between the center point and its neighbors
to construct an attention weight matrix for edge features. Afterwards, the
VAConv adopts a dual-channel structure to fuse weighted edge features and
global features. To verify the efficiency of the VAConv, we connect the VAConvs
with different receptive fields in parallel to obtain a Multi-scale graph
convolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art
performance on standard benchmarks including ModelNet40, S3DIS and ShapeNet.
Remarkably, on the ModelNet40 dataset for 3D classification, VA-GCN increased
by 2.4% compared to the baseline.","Title: VA-GCN: An Innovative Vector Attention Graph Convolution Network for Point Cloud Analysis

Newly Revised Abstract: The advancement in local aggregation operators has significantly revolutionized point cloud analysis models. Despite this progress, current local aggregation operators do not sufficiently emphasize the local information within the point cloud, thereby limiting the effectiveness of the models. To address this issue, we introduce a robust Vector Attention Convolution module (VAConv) that employs K-Nearest Neighbor (KNN) to identify the neighboring points of each input point. The VAConv then determines the attention weight matrix for edge features based on the elevation and azimuth relationship of vectors between a central point and its neighboring points. The VAConv further enhances its efficacy by using a dual-channel structure to amalgamate weighted edge features and global features. To assess the potency of the VAConv, we amalgamate the VAConvs with varying receptive fields in parallel to create a Multi-scale graph convolutional network, coined VA-GCN. The cutting-edge VA-GCN outperforms other models on standard benchmarks such as ModelNet40, S3DIS, and ShapeNet. Notably, the VA-GCN brought about an impressive 2.4% improvement compared to the baseline on the ModelNet40 dataset for 3D classification.",oai:arXiv.org:2106.00227,2021-06-01,['cs.CV']
"Global existence and boundedness in a fully parabolic
  attraction-repulsion chemotaxis system with signal-dependent sensitivities
  without logistic source","This paper deals with the fully parabolic attraction-repulsion chemotaxis
system with signal-dependent sensitivities, \begin{align*} \begin{cases}
  u_t=\Delta u-\nabla \cdot (u\chi(v)\nabla v)
  +\nabla \cdot (u\xi(w)\nabla w),
  &x \in \Omega,\ t>0,\\[1.05mm]
  v_t=\Delta v-v+u,
  &x \in \Omega,\ t>0,\\[1.05mm]
  w_t=\Delta w-w+u,
  &x \in \Omega,\ t>0 \end{cases} \end{align*} under homogeneous Neumann
boundary conditions and initial conditions, where $\Omega \subset \mathbb{R}^n$
$(n \ge 2)$ is a bounded domain with smooth boundary, $\chi, \xi$ are functions
satisfying some conditions. Global existence and boundedness of classical
solutions to the system with logistic source have already been obtained by
taking advantage of the effect of logistic dampening (J. Math. Anal. Appl.;
2020;489;124153). This paper establishes existence of global bounded classical
solutions despite the loss of logistic dampening.","Title: Worldwide Presence and Limitations in a Fully Parabolic Attraction-Repulsion Chemotaxis System with Signal-Dependent Sensitivities Absent of Logistic Source

Remodelled Abstract: This research investigates the fully parabolic attraction-repulsion chemotaxis system, which incorporates signal-dependent sensitivities, represented by \begin{align*} \begin{cases}
  u_t=\Delta u-\nabla \cdot (u\chi(v)\nabla v)
  +\nabla \cdot (u\xi(w)\nabla w),
  &x \in \Omega,\ t>0,\\[1.05mm]
  v_t=\Delta v-v+u,
  &x \in \Omega,\ t>0,\\[1.05mm]
  w_t=\Delta w-w+u,
  &x \in \Omega,\ t>0 \end{cases} \end{align*}, under the same homogeneous Neumann boundary conditions and initial conditions. Here, $\Omega \subset \mathbb{R}^n$ $(n \ge 2)$ is a restricted domain with a smooth boundary, and $\chi, \xi$ are functions that adhere to specific conditions. The global existence and the limitations of the traditional solutions to the system with a logistic source have previously been identified by leveraging the effect of logistic dampening (J. Math. Anal. Appl.; 2020;489;124153). This study confirms the existence of global, limited, traditional solutions, even in the absence of logistic dampening.",oai:arXiv.org:2104.00381,2021-04-01,['math.AP']
"AAPM DL-Sparse-View CT Challenge Submission Report: Designing an
  Iterative Network for Fanbeam-CT with Unknown Geometry","This report is dedicated to a short motivation and description of our
contribution to the AAPM DL-Sparse-View CT Challenge (team name:
""robust-and-stable""). The task is to recover breast model phantom images from
limited view fanbeam measurements using data-driven reconstruction techniques.
The challenge is distinctive in the sense that participants are provided with a
collection of ground truth images and their noiseless, subsampled sinograms (as
well as the associated limited view filtered backprojection images), but not
with the actual forward model. Therefore, our approach first estimates the
fanbeam geometry in a data-driven geometric calibration step. In a subsequent
two-step procedure, we design an iterative end-to-end network that enables the
computation of near-exact solutions.","Title: AAPM DL-Sparse-View CT Challenge Submission Report: Development of an Iterative Network for Fanbeam-CT with Undefined Geometry

Revised Abstract: This paper presents a concise motivation and outline of our submission to the AAPM DL-Sparse-View CT Challenge, represented by the team ""robust-and-stable"". Our objective is to reconstruct breast model phantom images from limited view fanbeam measurements utilizing data-oriented reconstruction methodologies. The competition is unique as it provides participants with a series of reference images and their noise-free, undersampled sinograms, alongside the associated limited view filtered backprojection images, but omits the actual forward model. Consequently, our method initiates with the estimation of the fanbeam geometry through a data-driven geometric calibration stage. Following this, we devise an iterative end-to-end network in a two-step process that facilitates the calculation of near-precise solutions.",oai:arXiv.org:2106.00280,2021-06-01,"['cs.LG', 'cs.NA', 'eess.IV', 'math.NA', 'physics.med-ph']"
Algebraic perspectives on signomial optimization,"Signomials are obtained by generalizing polynomials to allow for arbitrary
real exponents. This generalization offers great expressive power, but has
historically sacrificed the organizing principle of ``degree'' that is central
to polynomial optimization theory. We reclaim that principle here through the
concept of signomial rings, which we use to derive complete convex relaxation
hierarchies of upper and lower bounds for signomial optimization via sums of
arithmetic-geometric exponentials (SAGE) nonnegativity certificates. The
Positivstellensatz underlying the lower bounds relies on the concept of
conditional SAGE and removes regularity conditions required by earlier works,
such as convexity and Archimedeanity of the feasible set. Through worked
examples we illustrate the practicality of this hierarchy in areas such as
chemical reaction network theory and chemical engineering. These examples
include comparisons to direct global solvers (e.g., BARON and ANTIGONE) and the
Lasserre hierarchy (where appropriate). The completeness of our hierarchy of
upper bounds follows from a generic construction whereby a Positivstellensatz
for signomial nonnegativity over a compact set provides for arbitrarily strong
outer approximations of the corresponding cone of nonnegative signomials. While
working toward that result, we prove basic facts on the existence and
uniqueness of solutions to signomial moment problems.","Title: An Algebraic Examination of Signomial Optimization 

Generated Abstract: By expanding the scope of polynomials to encompass any real exponents, we arrive at signomials. Such expansion yields a substantial increase in expressive power, albeit at the expense of the pivotal principle of ""degree"" that is integral to polynomial optimization theory. In this study, we reintroduce this principle by exploiting the concept of signomial rings. This approach facilitates the derivation of comprehensive convex relaxation hierarchies for both upper and lower bounds in signomial optimization, using sums of arithmetic-geometric exponentials (SAGE) nonnegativity certificates. The Positivstellensatz forming the basis for the lower bounds hinges on the concept of conditional SAGE, thereby eliminating the regularity conditions mandated by previous studies, such as the convexity and Archimedeanity of the feasible set. The practical application of this hierarchy is demonstrated through examples drawn from fields like chemical reaction network theory and chemical engineering. These examples are compared to direct global solvers (e.g., BARON and ANTIGONE) and the Lasserre hierarchy (where applicable). A generic construction ensures the comprehensive nature of our hierarchy of upper bounds, as a Positivstellensatz for signomial nonnegativity over a compact set allows for increasingly accurate outer approximations of the respective cone of nonnegative signomials. In the process of achieving this result, we establish basic truths concerning the existence and uniqueness of solutions to signomial moment problems.",oai:arXiv.org:2107.00345,2021-07-01,"['math.AG', 'math.OC']"
"The Comprehensive Blub Archive Network: Towards Design Principals for
  Open Source Programming Language Repositories","Many popular open source programming languages (Perl, Ruby or Python for
example) have systems for distributing packaged source code that software
developers can use when working in that particular programming language. This
paper will consider the design principals that should be followed if designing
such an open source code repository.","Title: A Holistic Examination of the Blub Archive Network: Proposing Design Guidelines for Open Source Programming Language Libraries 

Revised Abstract: Numerous widely-used open source programming languages, such as Perl, Ruby, or Python, incorporate systems that distribute packaged source code, which software developers leverage while operating within that specific programming language. This study intends to deliberate the essential design guidelines that ought to be observed in the creation of such open source code libraries.",oai:arXiv.org:2104.00378,2021-04-01,['cs.PL']
"MeanShift++: Extremely Fast Mode-Seeking With Applications to
  Segmentation and Object Tracking","MeanShift is a popular mode-seeking clustering algorithm used in a wide range
of applications in machine learning. However, it is known to be prohibitively
slow, with quadratic runtime per iteration. We propose MeanShift++, an
extremely fast mode-seeking algorithm based on MeanShift that uses a grid-based
approach to speed up the mean shift step, replacing the computationally
expensive neighbors search with a density-weighted mean of adjacent grid cells.
In addition, we show that this grid-based technique for density estimation
comes with theoretical guarantees. The runtime is linear in the number of
points and exponential in dimension, which makes MeanShift++ ideal on
low-dimensional applications such as image segmentation and object tracking. We
provide extensive experimental analysis showing that MeanShift++ can be more
than 10,000x faster than MeanShift with competitive clustering results on
benchmark datasets and nearly identical image segmentations as MeanShift.
Finally, we show promising results for object tracking.","Title: MeanShift++: Advanced Fast Mode-Seeking Algorithm with Relevance to Segmentation and Object Tracking 

New Abstract: MeanShift is a widely adopted clustering algorithm for mode-seeking that is employed extensively in various machine learning applications. The algorithm, however, is recognized to be exceedingly slow, characterized by a quadratic runtime for each iteration. In this paper, we present MeanShift++, an enhanced fast mode-seeking algorithm that is built on the foundations of MeanShift. It employs a grid-based methodology to expedite the mean shift step, substituting the computationally demanding neighbors search with a density-weighted average of adjacent grid cells. We provide evidence that this grid-based density estimation method is supported by theoretical assurances. MeanShift++ has a runtime that scales linearly with the number of points and exponentially with the dimension, making it ideally suited for applications with low dimensions, such as image segmentation and object tracking. We offer a comprehensive experimental analysis demonstrating that MeanShift++ can achieve speeds that are over 10,000 times faster than MeanShift, while still producing competitive clustering results on benchmark datasets and generating image segmentations almost identical to those of MeanShift. We conclude by presenting encouraging outcomes for object tracking.",oai:arXiv.org:2104.00303,2021-04-01,"['cs.CV', 'cs.LG']"
"Mean-field BDSDEs and associated nonlocal semi-linear backward
  stochastic partial differential equations","In this paper we investigate mean-field backward doubly stochastic
differential equations (BDSDEs), i.e., BDSDEs whose driving coefficients also
depend on the joint law of the solution process as well as the solution of an
associated mean-field forward SDE. Unlike the pioneering paper on BDSDEs by
Pardoux-Peng (1994), we handle a driving coefficient in the backward integral
of the BDSDE for which the Lipschitz assumption w.r.t. the law of the solution
is sufficient, without assuming that this Lipschitz constant is small enough.
On the other hand, as the parameters $(x,P_\xi)$ and $(x,P_\xi,y)$ run an
infinite-dimensional space, unlike Pardoux and Peng, we cannot apply
Kolmogorov's continuity criterion to the value function
$V(t,x,P_{\xi}):=Y_t^{t,x,P_{\xi}}$, while in the classical case studied in
Pardoux-Peng the value function $V(t,x)=Y_t^{t,x}$ can be shown to be of class
$C^{1,2}([0,T]\times\mathbb{R}^d)$, we have for our value function
$V(t,x,P_{\xi})$ and its derivative $\partial_\mu V(t,x,P_{\xi},y)$ only the
$L^2$-differentiability with respect to $x$ and $y$, respectively. Using a new
method we prove the characterization of $V=(V(t,x,P_{\xi}))$ as the unique
solution of the associated mean-field backward stochastic PDE.","Title: Investigation of Mean-field BDSDEs and Related Semi-linear Backward Stochastic Partial Differential Equations

New Abstract: This study delves into the exploration of mean-field backward doubly stochastic differential equations (BDSDEs), specifically those BDSDEs where the driving coefficients are influenced by both the combined law of the solution process and the solution of a related mean-field forward SDE. In contrast to the foundational paper on BDSDEs by Pardoux-Peng (1994), we manage a driving coefficient in the backward integral of the BDSDE that is sufficiently explained by the Lipschitz assumption concerning the law of the solution, without requiring the Lipschitz constant to be minimal. On the contrary, while the parameters $(x,P_\xi)$ and $(x,P_\xi,y)$ traverse an infinite-dimensional space, differing from Pardoux and Peng, we are unable to apply Kolmogorov's continuity criterion to the value function $V(t,x,P_{\xi}):=Y_t^{t,x,P_{\xi}}$. In the conventional case examined in Pardoux-Peng where the value function $V(t,x)=Y_t^{t,x}$ is demonstrated to be of class $C^{1,2}([0,T]\times\mathbb{R}^d)$, our value function $V(t,x,P_{\xi})$ and its derivative $\partial_\mu V(t,x,P_{\xi},y)$ display only $L^2$-differentiability with respect to $x$ and $y$. Utilizing a novel method, we demonstrate the characterization of $V=(V(t,x,P_{\xi}))$ as the singular solution of the related mean-field backward stochastic PDE.",oai:arXiv.org:2111.00759,2021-11-01,['math.PR']
"Short wavelength infrared avalanche photodetector using Sb-based
  strained layer superlattice","We demonstrate a low noise short wavelength infrared (SWIR) Sb based type II
superlattice (T2SL) avalanche photodiodes (APD). The SWIR GaSb/(AlAsSb/GaSb)
APD structure was designed based on impact ionization engineering and grown by
molecular beam epitaxy on GaSb substrate. At room temperature, the device
exhibits a 50 % cut-off wavelength of 1.74 micron. The device revealed to have
electron dominated avalanching mechanism with a gain value of 48 at room
temperature. The electron and hole impact ionization coefficients were
calculated and compared to give better prospect of the performance of the
device. Low excess noise, as characterized by the carrier ionization ratio of ~
0.07, has been achieved.","Title: An Sb-Based Strained Layer Superlattice Utilized in Short Wavelength Infrared Avalanche Photodetectors

Generated Abstract: This research presents a short wavelength infrared (SWIR) avalanche photodiode (APD) with low noise, utilizing an Sb-based type II superlattice (T2SL). The SWIR APD, consisting of GaSb/(AlAsSb/GaSb), was designed utilizing impact ionization engineering and was developed on a GaSb substrate through molecular beam epitaxy. Under room conditions, the device demonstrated a 50% cut-off wavelength at 1.74 microns. The analysis revealed that the device is dominated by an electron avalanching mechanism, with a gain value of 48 at room temperature. The comparison of calculated electron and hole impact ionization coefficients provided a more in-depth understanding of the device's performance. The device achieved a low excess noise, marked by a carrier ionization ratio of approximately 0.07.",oai:arXiv.org:2104.00251,2021-04-01,['physics.app-ph']
"Geometric Control for Load Transportation with Quadrotor UAVs by Elastic
  Cables","Groups of unmanned aerial vehicles (UAVs) are increasingly utilized in
transportation task as the combined strength allows to increase the maximum
payload. However, the resulting mechanical coupling of the UAVs impose new
challenges in terms of the tracking control. Thus, we design a geometric
trajectory tracking controller for the cooperative task of four quadrotor UAVs
carrying and transporting a rigid body, which is attached to the quadrotors via
inflexible elastic cables. The elasticity of the cables together with
techniques of singular perturbation allows a reduction in the model to that of
a similar model with inelastic cables. In this reduced model, we design a
controller such that the position and attitude of the load exponentially
converges to a given desired trajectory. We then show that this result leads to
an uniformly converging tracking error for the original elastic model under
some assumptions. Furthermore, under the presence of unstructured disturbances
on the system, we show that the error is ultimately bounded with an arbitrarily
small bound. Finally, a simulation illustrates the theoretical results.","Title: Load Transportation via Quadrotor UAVs: A Study on Geometric Control with Rigid Elastic Cables

Revised Abstract: The collective power of unmanned aerial vehicle (UAV) groups is increasingly being harnessed for transportation tasks, enhancing their maximum payload capacity. However, this mechanical interconnection introduces new complexities in tracking control. As a solution, we have developed a geometric trajectory tracking controller for a team of four quadrotor UAVs tasked with transporting a rigid object, connected to the UAVs through non-flexible elastic cables. By combining the cable's elasticity with singular perturbation techniques, the model is simplified to resemble one using inelastic cables. Within this simplified model, a controller is designed, ensuring both the position and orientation of the load converges exponentially to a pre-determined trajectory. Our research demonstrates that this outcome prompts a uniformly converging tracking error in the initial elastic model, given certain presumptions. Moreover, even in the presence of unstructured system disturbances, the error remains within an arbitrarily small range. Theoretical findings are further substantiated through simulation.",oai:arXiv.org:2111.00777,2021-11-01,"['math.OC', 'cs.SY', 'eess.SY']"
"Edge-competing Pathological Liver Vessel Segmentation with Limited
  Labels","The microvascular invasion (MVI) is a major prognostic factor in
hepatocellular carcinoma, which is one of the malignant tumors with the highest
mortality rate. The diagnosis of MVI needs discovering the vessels that contain
hepatocellular carcinoma cells and counting their number in each vessel, which
depends heavily on experiences of the doctor, is largely subjective and
time-consuming. However, there is no algorithm as yet tailored for the MVI
detection from pathological images. This paper collects the first pathological
liver image dataset containing 522 whole slide images with labels of vessels,
MVI, and hepatocellular carcinoma grades. The first and essential step for the
automatic diagnosis of MVI is the accurate segmentation of vessels. The unique
characteristics of pathological liver images, such as super-large size,
multi-scale vessel, and blurred vessel edges, make the accurate vessel
segmentation challenging. Based on the collected dataset, we propose an
Edge-competing Vessel Segmentation Network (EVS-Net), which contains a
segmentation network and two edge segmentation discriminators. The segmentation
network, combined with an edge-aware self-supervision mechanism, is devised to
conduct vessel segmentation with limited labeled patches. Meanwhile, two
discriminators are introduced to distinguish whether the segmented vessel and
background contain residual features in an adversarial manner. In the training
stage, two discriminators are devised tocompete for the predicted position of
edges. Exhaustive experiments demonstrate that, with only limited labeled
patches, EVS-Net achieves a close performance of fully supervised methods,
which provides a convenient tool for the pathological liver vessel
segmentation. Code is publicly available at
https://github.com/zju-vipa/EVS-Net.","Title: Advanced Pathological Liver Vessel Segmentation via Edge-Competing Techniques with Limited Labels 

Generated Abstract: Hepatocellular carcinoma, a highly lethal malignancy, has microvascular invasion (MVI) as a significant determinant of prognosis. The identification and enumeration of vessels containing hepatocellular carcinoma cells, a crucial part of MVI diagnosis, is primarily reliant on physician expertise, leading to subjectivity and lengthy processes. To date, no algorithm specifically designed for MVI detection from pathological images exists. This study presents a novel dataset of 522 pathological liver images featuring whole slide images labelled with vessel, MVI, and hepatocellular carcinoma grade information. The inaugural step to automate MVI diagnosis entails accurate vessel segmentation, a task complicated by the pathological liver images' unique elements such as immense size, multi-scale vessels, and ambiguous vessel edges. Using the newly collected dataset, we introduce an Edge-competing Vessel Segmentation Network (EVS-Net), comprising a segmentation network and two edge segmentation discriminators. The segmentation network, enhanced with an edge-aware self-supervision mechanism, performs vessel segmentation using limited labeled patches. Simultaneously, two discriminators are employed to adversarially determine if the segmented vessel and background retain residual features. During the training phase, two discriminators are designed to contest the predicted edge locations. Comprehensive experiments affirm that EVS-Net, despite relying on limited labeled patches, delivers comparable results to fully supervised methods, thereby offering a practical tool for pathological liver vessel segmentation. The code can be accessed at https://github.com/zju-vipa/EVS-Net.",oai:arXiv.org:2108.00384,2021-08-01,['cs.CV']
"Direct measurement of the 13C({\alpha},n)16O cross section into the
  s-process Gamow peak","One of the main neutron sources for the astrophysical s-process is the
reaction 13C({\alpha},n)16O, taking place in thermally pulsing Asymptotic Giant
Branch stars at temperatures around 90 MK. To model the nucleosynthesis during
this process the reaction cross section needs to be known in the 150-230keV
energy window (Gamow peak). At these sub-Coulomb energies cross section direct
measurements are severely affected by the low event rate, making us rely on
input from indirect methods and extrapolations from higher-energy direct data.
This leads to an uncertainty in the cross section at the relevant energies too
high to reliably constrain the nuclear physics input to s-process calculations.
We present the results from a new deep-underground measurement of
13C({\alpha},n)16O, covering the energy range 230-300keV, with drastically
reduced uncertainties over previous measurements and for the first time
providing data directly inside the s-process Gamow peak. Selected stellar
models have been computed to estimate the impact of our revised reaction rate.
For stars of nearly solar composition, we find sizeable variations of some
isotopes, whose production is influenced by the activation of close-by
branching points that are sensitive to the neutron density, in particular the
two radioactive nuclei 60Fe and 205Pb, as well as 152Gd","Title: Direct Evaluation of the 13C({\alpha},n)16O Reaction Cross Section Within the s-process Gamow Peak

Revised Abstract: The astrophysical s-process is primarily influenced by the reaction 13C({\alpha},n)16O, occurring within thermally pulsating Asymptotic Giant Branch stars at approximately 90 MK temperatures. Accurate modeling of nucleosynthesis during this process necessitates precise knowledge of the reaction cross section within the energy window of 150-230keV, known as the Gamow peak. Due to the low event rate at these sub-Coulomb energies, direct measurements of cross sections are significantly compromised, leading to dependence on indirect methodologies and extrapolations from high-energy direct data. This results in an uncertainty level in the cross section at relevant energies, which is too elevated to accurately constrain the nuclear physics input for s-process computations. This work presents outcomes from a novel deep-underground measurement of 13C({\alpha},n)16O, encompassing the energy range of 230-300keV, offering substantially reduced uncertainties compared to prior measurements, and for the first time, contributing data directly within the s-process Gamow peak. The impact of our revised reaction rate has been estimated using selected stellar models. For stars with almost solar composition, we discover considerable variations in the production of certain isotopes, specifically the two radioactive nuclei 60Fe and 205Pb, and 152Gd, whose production is influenced by the activation of nearby branching points sensitive to neutron density.",oai:arXiv.org:2110.00303,2021-10-01,['nucl-ex']
