{
  "year": 2021,
  "count": 100,
  "papers": [
    {
      "title": "Summarize and Search: Learning Consensus-aware Dynamic Convolution for\n  Co-Saliency Detection",
      "original_abstract": "Humans perform co-saliency detection by first summarizing the consensus\nknowledge in the whole group and then searching corresponding objects in each\nimage. Previous methods usually lack robustness, scalability, or stability for\nthe first process and simply fuse consensus features with image features for\nthe second process. In this paper, we propose a novel consensus-aware dynamic\nconvolution model to explicitly and effectively perform the \"summarize and\nsearch\" process. To summarize consensus image features, we first summarize\nrobust features for every single image using an effective pooling method and\nthen aggregate cross-image consensus cues via the self-attention mechanism. By\ndoing this, our model meets the scalability and stability requirements. Next,\nwe generate dynamic kernels from consensus features to encode the summarized\nconsensus knowledge. Two kinds of kernels are generated in a supplementary way\nto summarize fine-grained image-specific consensus object cues and the coarse\ngroup-wise common knowledge, respectively. Then, we can effectively perform\nobject searching by employing dynamic convolution at multiple scales. Besides,\na novel and effective data synthesis method is also proposed to train our\nnetwork. Experimental results on four benchmark datasets verify the\neffectiveness of our proposed method. Our code and saliency maps are available\nat \\url{https://github.com/nnizhang/CADC}.",
      "generated_abstract": "Title: Learning Consensus-Aware Dynamic Convolution for Co-Saliency Detection: A Summarize and Search Approach\n\nRewritten Abstract: The human ability to detect co-saliency involves summarizing consensus knowledge within a group, followed by identifying corresponding objects within each image. Existing techniques often lack robustness, scalability, or consistency in the initial phase, and typically merge consensus features with image features within the second phase. This study introduces a unique consensus-aware dynamic convolution model, designed to efficiently and distinctly execute the 'summarize and search' process. In order to summarize consensus image features, we initially collate robust features for each individual image through an efficient pooling method, and subsequently amalgamate cross-image consensus indications through the self-attention mechanism. This ensures our model satisfies the scalability and stability prerequisites. Subsequently, we create dynamic kernels from consensus features to encapsulate the summarized consensus knowledge. We generate two types of kernels in a complementary fashion to summarize detailed image-specific consensus object indications and the broad group-wide common knowledge. Consequently, we can efficiently conduct object searching through the use of dynamic convolution across various scales. Additionally, we propose a new and efficient data synthesis technique to train our network. Experiments conducted on four benchmark datasets confirm the efficacy of our proposed technique. Our code and saliency maps can be accessed at \\url{https://github.com/nnizhang/CADC}.",
      "original_id": "oai:arXiv.org:2110.00338",
      "created": "2021-10-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "H-FL: A Hierarchical Communication-Efficient and Privacy-Protected\n  Architecture for Federated Learning",
      "original_abstract": "The longstanding goals of federated learning (FL) require rigorous privacy\nguarantees and low communication overhead while holding a relatively high model\naccuracy. However, simultaneously achieving all the goals is extremely\nchallenging. In this paper, we propose a novel framework called hierarchical\nfederated learning (H-FL) to tackle this challenge. Considering the degradation\nof the model performance due to the statistic heterogeneity of the training\ndata, we devise a runtime distribution reconstruction strategy, which\nreallocates the clients appropriately and utilizes mediators to rearrange the\nlocal training of the clients. In addition, we design a compression-correction\nmechanism incorporated into H-FL to reduce the communication overhead while not\nsacrificing the model performance. To further provide privacy guarantees, we\nintroduce differential privacy while performing local training, which injects\nmoderate amount of noise into only part of the complete model. Experimental\nresults show that our H-FL framework achieves the state-of-art performance on\ndifferent datasets for the real-world image recognition tasks.",
      "generated_abstract": "Title: H-FL: An Advanced, Hierarchical Structure for Efficient, Privacy-Safe Federated Learning\n\nRevised Abstract: The consistent objectives of federated learning (FL) necessitate robust privacy protection, reduced communication overhead, and reasonably high model precision. Nonetheless, fulfilling these objectives concurrently presents a significant challenge. This study introduces a novel hierarchical federated learning (H-FL) framework designed to address this issue. Given the performance decline in the model due to statistical heterogeneity in the training data, we have developed a runtime distribution reconstruction method. This method strategically reallocates clients and employs mediators to reorganize the local training of clients. Moreover, we integrate a compression-correction mechanism within the H-FL framework to lower the communication overhead without compromising the performance of the model. We also incorporate differential privacy into the local training process to ensure privacy protection, by injecting a controlled level of noise into only a segment of the comprehensive model. Experimental outcomes demonstrate that our H-FL framework achieves unparalleled performance across various datasets for practical image recognition tasks.",
      "original_id": "oai:arXiv.org:2106.00275",
      "created": "2021-06-01",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.DC"
      ]
    },
    {
      "title": "Online Fashion Commerce: Modelling Customer Promise Date",
      "original_abstract": "In the e-commerce space, accurate prediction of delivery dates plays a major\nrole in customer experience as well as in optimizing the supply chain\noperations. Predicting a date later than the actual delivery date might\nsometimes result in the customer not placing the order (lost sales) while\npromising a date earlier than the actual delivery date would lead to a bad\ncustomer experience and consequent customer churn. In this paper, we present a\nmachine learning-based approach for penalizing incorrect predictions\ndifferently using non-conventional loss functions, while working under various\nuncertainties involved in making successful deliveries such as traffic\ndisruptions, weather conditions, supply chain, and logistics. We examine\nstatistical, deep learning, and conventional machine learning approaches, and\nwe propose an approach that outperformed the pre-existing rule-based models.\nThe proposed model is deployed internally for Fashion e-Commerce and is\noperational.",
      "generated_abstract": "Title: Digital Fashion Trade: Predicting Client Delivery Dates\n\nRevamped Abstract: In the domain of online commerce, an essential factor influencing customer satisfaction and supply chain management efficacy is the precise forecast of delivery dates. Overestimating the delivery date may discourage a customer from completing an order, leading to lost sales. Conversely, underestimating the delivery date can result in poor customer experience, thereby increasing customer attrition rates. This research introduces a machine learning strategy that applies unconventional loss functions to penalize inaccurate predictions, taking into consideration the uncertainties inherent in successful deliveries, such as logistical issues, supply chain disruptions, adverse weather, and traffic congestion. We scrutinize statistical, conventional machine learning, and deep learning methodologies, putting forth a strategy that surpasses the existing rule-based models. The proposed model has been implemented internally for digital fashion commerce and is currently in operation.",
      "original_id": "oai:arXiv.org:2105.00315",
      "created": "2021-05-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Quantum crypto-economics: Blockchain prediction markets for the\n  evolution of quantum technology",
      "original_abstract": "Two of the most important technological advancements currently underway are\nthe advent of quantum technologies, and the transitioning of global financial\nsystems towards cryptographic assets, notably blockchain-based cryptocurrencies\nand smart contracts. There is, however, an important interplay between the two,\ngiven that, in due course, quantum technology will have the ability to directly\ncompromise the cryptographic foundations of blockchain. We explore this complex\ninterplay by building financial models for quantum failure in various\nscenarios, including pricing quantum risk premiums. We call this quantum\ncrypto-economics.",
      "generated_abstract": "Title: Quantum Cryptoeconomic Interactions: The Implication of Blockchain Predictive Marketplaces on Quantum Technology Progression.\n\nRevised Abstract: The ongoing development of quantum technologies and the gradual shift of global financial structures towards cryptographic assets, particularly blockchain-driven cryptocurrencies and smart contracts, are among the most significant current technological trends. However, a crucial interaction exists between these two domains, as quantum technology will eventually possess the capacity to undermine the cryptographic underpinnings of the blockchain. This paper delves into this intricate interaction, creating financial models that forecast quantum failure under different conditions, inclusive of the calculation of quantum risk premiums. We refer to this study as quantum cryptoeconomics.",
      "original_id": "oai:arXiv.org:2102.00659",
      "created": "2021-02-01",
      "categories": [
        "q-fin.PR",
        "quant-ph"
      ]
    },
    {
      "title": "Fool Me Once: Robust Selective Segmentation via Out-of-Distribution\n  Detection with Contrastive Learning",
      "original_abstract": "In this work, we train a network to simultaneously perform segmentation and\npixel-wise Out-of-Distribution (OoD) detection, such that the segmentation of\nunknown regions of scenes can be rejected. This is made possible by leveraging\nan OoD dataset with a novel contrastive objective and data augmentation scheme.\nBy combining data including unknown classes in the training data, a more robust\nfeature representation can be learned with known classes represented distinctly\nfrom those unknown. When presented with unknown classes or conditions, many\ncurrent approaches for segmentation frequently exhibit high confidence in their\ninaccurate segmentations and cannot be trusted in many operational\nenvironments. We validate our system on a real-world dataset of unusual driving\nscenes, and show that by selectively segmenting scenes based on what is\npredicted as OoD, we can increase the segmentation accuracy by an IoU of 0.2\nwith respect to alternative techniques.",
      "generated_abstract": "Title: Duped No More: Sturdy Selective Segmentation through Out-of-Distribution Detection Using Contrastive Learning \n\nRevised Abstract: This research concentrates on the training of a network to concurrently conduct segmentation and pixel-wise Out-of-Distribution (OoD) detection, facilitating the rejection of segmentation in unknown scene areas. The accomplishment of this task is driven by the use of an OoD dataset, a groundbreaking contrastive objective, and a data augmentation scheme. By integrating data with unknown classes into the training data, a more resilient feature representation can be developed, with known classes distinctly differentiated from unknown ones. Current segmentation methods often display overconfidence in their erroneous segmentations when encountering unknown classes or conditions, rendering them unreliable in many operational contexts. We evaluate our model using an authentic dataset of abnormal driving situations, demonstrating that selective scene segmentation, based on OoD prediction, can enhance segmentation precision by an Intersection over Union (IoU) of 0.2 compared to alternative methods.",
      "original_id": "oai:arXiv.org:2103.00869",
      "created": "2021-03-01",
      "categories": [
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "JAS-GAN: Generative Adversarial Network Based Joint Atrium and Scar\n  Segmentations on Unbalanced Atrial Targets",
      "original_abstract": "Automated and accurate segmentations of left atrium (LA) and atrial scars\nfrom late gadolinium-enhanced cardiac magnetic resonance (LGE CMR) images are\nin high demand for quantifying atrial scars. The previous quantification of\natrial scars relies on a two-phase segmentation for LA and atrial scars due to\ntheir large volume difference (unbalanced atrial targets). In this paper, we\npropose an inter-cascade generative adversarial network, namely JAS-GAN, to\nsegment the unbalanced atrial targets from LGE CMR images automatically and\naccurately in an end-to-end way. Firstly, JAS-GAN investigates an adaptive\nattention cascade to automatically correlate the segmentation tasks of the\nunbalanced atrial targets. The adaptive attention cascade mainly models the\ninclusion relationship of the two unbalanced atrial targets, where the\nestimated LA acts as the attention map to adaptively focus on the small atrial\nscars roughly. Then, an adversarial regularization is applied to the\nsegmentation tasks of the unbalanced atrial targets for making a consistent\noptimization. It mainly forces the estimated joint distribution of LA and\natrial scars to match the real ones. We evaluated the performance of our\nJAS-GAN on a 3D LGE CMR dataset with 192 scans. Compared with the\nstate-of-the-art methods, our proposed approach yielded better segmentation\nperformance (Average Dice Similarity Coefficient (DSC) values of 0.946 and\n0.821 for LA and atrial scars, respectively), which indicated the effectiveness\nof our proposed approach for segmenting unbalanced atrial targets.",
      "generated_abstract": "Title: JAS-GAN: An Advanced Generative Adversarial Network for Simultaneous Segmentation of Atrium and Scars on Unbalanced Atrial Targets \n\nGenerated Abstract: The necessity for automated and precise segmentations of the left atrium (LA) and atrial scars from late gadolinium-enhanced cardiac magnetic resonance (LGE CMR) images for the purpose of atrial scar quantification is burgeoning. Prior methods for atrial scar quantification were reliant on a two-phase segmentation process for LA and atrial scars, due to the significant volume disparity, or unbalanced atrial targets. This study introduces JAS-GAN, an advanced inter-cascade generative adversarial network designed to segment unbalanced atrial targets from LGE CMR images in an automatic and accurate, end-to-end manner. The JAS-GAN incorporates an adaptive attention cascade to autonomously correlate the segmentation tasks of the unbalanced atrial targets, specifically modeling the inclusion relationship between the two unbalanced atrial targets, with the predicted LA serving as the attention map to adaptively focus on the smaller atrial scars. Adversarial regularization is subsequently applied to these segmentation tasks to achieve consistent optimization, compelling the predicted joint distribution of LA and atrial scars to align with the actual ones. Our JAS-GAN was assessed on a 3D LGE CMR dataset containing 192 scans. The results demonstrated superior segmentation performance in comparison to existing techniques, with Average Dice Similarity Coefficient (DSC) values of 0.946 and 0.821 for LA and atrial scars, respectively, thus validating the effectiveness of our approach in segmenting unbalanced atrial targets.",
      "original_id": "oai:arXiv.org:2105.00234",
      "created": "2021-05-01",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Researching of magnetic cutoff for local sources of charged particles in\n  the halo of the Galaxy",
      "original_abstract": "Models of highly inhomogeneous baryosynthesis of the baryonic asymmetric\nUniverse allow for the existence of macroscopic domains of antimatter, which\ncould evolve in a globular cluster of antimatter stars in our Galaxy. We assume\nthe symmetry of the evolution of a globular cluster of stars and antistars\nbased on the symmetry of the properties of matter and antimatter. Such object\ncan be a source of a fraction of antihelium nuclei in galactic cosmic rays. It\nmakes possible to predict the expected fluxes of cosmic antinuclei with use of\nknown properties of matter star globular clusters We have estimated the lower\ncutoff energy for the penetration of antinuclei from the antimatter globular\ncluster, situated in halo, into the galactic disk based on the simulation of\nparticle motion in the large-scale structure of magnetic fields in the Galaxy.\nWe have estimated the magnitude of the magnetic cutoff for the globular cluster\nM4.",
      "generated_abstract": "Title: Investigation into Magnetic Cutoffs for Local Sources of Charged Particles in the Galactic Halo\n\nRevised Abstract: The highly irregular models of baryosynthesis of the asymmetric baryonic Universe suggest the potential existence of large antimatter domains. These could potentially transform into a globular cluster of antimatter stars within our own Galaxy. We hypothesize a symmetrical evolution of a globular cluster of stars and antistars, reflecting the symmetrical properties of matter and antimatter. Such an entity could contribute to a percentage of antihelium nuclei in galactic cosmic rays, enabling the prediction of cosmic antinuclei fluxes by utilizing the known properties of matter star globular clusters. We have calculated the minimum energy cutoff required for antinuclei to penetrate from the antimatter globular cluster located in the halo into the galactic disk. This is based on simulations of particle movement within the Galaxy's large-scale magnetic field structure. Furthermore, we have determined the magnetic cutoff's extent for the globular cluster M4.",
      "original_id": "oai:arXiv.org:2112.00361",
      "created": "2021-12-01",
      "categories": [
        "astro-ph.HE",
        "astro-ph.CO"
      ]
    },
    {
      "title": "More Behind Your Electricity Bill: a Dual-DNN Approach to Non-Intrusive\n  Load Monitoring",
      "original_abstract": "Non-intrusive load monitoring (NILM) is a well-known single-channel blind\nsource separation problem that aims to decompose the household energy\nconsumption into itemised energy usage of individual appliances. In this way,\nconsiderable energy savings could be achieved by enhancing household's\nawareness of energy usage. Recent investigations have shown that deep neural\nnetworks (DNNs) based approaches are promising for the NILM task. Nevertheless,\nthey normally ignore the inherent properties of appliance operations in the\nnetwork design, potentially leading to implausible results. We are thus\nmotivated to develop the dual Deep Neural Networks (dual-DNN), which aims to i)\ntake advantage of DNNs' learning capability of latent features and ii) empower\nthe DNN architecture with identification ability of universal properties.\nSpecifically in the design of dual-DNN, we adopt one subnetwork to measure\npower ratings of different appliances' operation states, and the other\nsubnetwork to identify the running states of target appliances. The final\nresult is then obtained by multiplying these two network outputs and meanwhile\nconsidering the multi-state property of household appliances. To enforce the\nsparsity property in appliance's state operating, we employ median filtering\nand hard gating mechanisms to the subnetwork for state identification. Compared\nwith the state-of-the-art NILM methods, our dual-DNN approach demonstrates a\n21.67% performance improvement in average on two public benchmark datasets.",
      "generated_abstract": "Title: Unveiling the Hidden Components of Your Electricity Bill: Implementing a Dual-Deep Neural Network Methodology for Non-Invasive Load Monitoring\n\nRewritten Abstract: Non-invasive load monitoring (NILM) is a recognized method for resolving the single-channel blind source separation issue by partitioning the overall energy usage of a home into the individual energy consumption of each appliance. This practice can potentially lead to significant energy conservation by increasing household knowledge of energy usage. Recent research indicates that deep neural network (DNN) based strategies show potential in addressing NILM tasks. However, these strategies often overlook the inherent characteristics of appliance operation in their network design, which could potentially lead to unreliable results. Consequently, we have been motivated to innovate the dual deep neural networks (dual-DNN) to exploit the DNNs' latent feature learning capacity and to equip the DNN structure with the ability to identify universal properties. In the development of the dual-DNN, we utilize one subnetwork to evaluate the power ratings of various appliance operation states, and a different subnetwork to discern the operational states of the appliances in question. The final outcome is calculated by multiplying the outputs of these two networks and simultaneously accounting for the multi-state nature of household appliances. To enforce the sparsity property in the operation state of an appliance, we incorporate median filtering and hard gating mechanisms into the state identification subnetwork. Our dual-DNN methodology, when compared with leading NILM techniques, exhibits an average performance enhancement of 21.67% on two publicly available benchmark datasets.",
      "original_id": "oai:arXiv.org:2106.00297",
      "created": "2021-06-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Unconventional satellite resistance peaks in moir\\'e superlattice of\n  h-BN/ AB-stacked tetralayer-graphene heterostructure",
      "original_abstract": "Most studies on moir\\'e superlattices formed from a stack of h-BN and\ngraphene have focused on single layer graphene; graphene with multiple layers\nis less understood. Here, we show that a moir\\'e superlattice of multilayer\ngraphene shows new features arising from the anisotropic Fermi surface affected\nby the superlattice structure. The moir\\'e superlattice of a h-BN/AB-stacked\ntetralayer graphene heterostructure exhibited resistivity peaks showing a\ncomplicated dependence on the perpendicular electric field. The peaks were not\ndue to secondary Dirac cones forming, but rather opening of the energy gap due\nto folding of the anisotropic Fermi surface. In addition, superlattice peaks\nresulted from mixing of light- and heavy-mass bilayer-like bands via the\nsuperlattice potential. The gaps did not open on the boundary of the\nsuperlattice Brillouin zone, but rather opened inside it, which reflected the\nanisotropy of the Fermi surface of multilayer graphene.",
      "generated_abstract": "Title: Novel Satellite Resistance Peaks in h-BN/ AB-stacked Tetralayer-Graphene Heterostructure's Moir\u00e9 Superlattice\n\nAlternative Abstract: The exploration of moir\u00e9 superlattices formed from the combination of h-BN and graphene has predominantly been centered on single layer graphene, leaving multilayer graphene somewhat unexamined. This study demonstrates new properties exhibited by a multilayer graphene's moir\u00e9 superlattice, which are a result of the anisotropic Fermi surface being influenced by the superlattice structure. The h-BN/AB-stacked tetralayer graphene heterostructure's moir\u00e9 superlattice displayed resistivity peaks that exhibited a complex relationship with the orthogonal electric field. These peaks did not originate from the formation of secondary Dirac cones, but were instead due to the opening of the energy gap caused by folding of the anisotropic Fermi surface. Furthermore, the superlattice peaks were a consequence of the intermingling of light- and heavy-mass bilayer-like bands via the superlattice potential. The energy gaps did not form on the periphery of the superlattice Brillouin zone, but rather within it, which mirrors the anisotropy of the Fermi surface of multilayer graphene.",
      "original_id": "oai:arXiv.org:2104.00261",
      "created": "2021-04-01",
      "categories": [
        "cond-mat.mes-hall",
        "cond-mat.mtrl-sci",
        "cond-mat.soft"
      ]
    },
    {
      "title": "Characterizing and Detecting Configuration Compatibility Issues in\n  Android Apps",
      "original_abstract": "XML configuration files are widely used in Android to define an app's user\ninterface and essential runtime information such as system permissions. As\nAndroid evolves, it might introduce functional changes in the configuration\nenvironment, thus causing compatibility issues that manifest as inconsistent\napp behaviors at different API levels. Such issues can often induce software\ncrashes and inconsistent look-and-feel when running at specific Android\nversions. Existing works incur plenty of false positive and false negative\nissue-detection rules by conducting trivial data-flow analysis while failing to\nmodel the XML tree hierarchies of the Android configuration files. Besides,\nlittle is known about how the changes in an Android framework can induce such\ncompatibility issues. To bridge such gaps, we conducted a systematic study by\nanalyzing 196 real-world issues collected from 43 popular apps. We identified\ncommon patterns of Android framework code changes that induce such\nconfiguration compatibility issues. Based on the findings, we propose\n\\textsc{ConfDroid} that can automatically extract rules for detecting\nconfiguration compatibility issues. The intuition is to perform symbolic\nexecution based on a model learned from the common code change patterns.\nExperiment results show that ConfDroid can successfully extract 282 valid\nissue-detection rules with a precision of 91.9%. Among them, 65 extracted rules\ncan manifest issues that cannot be detected by the rules of state-of-the-art\nbaselines. More importantly, 11 out of them have led to the detection of 107\nreproducible configuration compatibility issues that the baselines cannot\ndetect in 30 out of 316 real-world Android apps.",
      "generated_abstract": "Title: Identifying and Resolving Configuration Compatibility Challenges in Android Applications\n\nRevised Abstract: Android applications extensively utilize XML configuration files to establish the user interface and vital runtime parameters, including system permissions. With the advancement of Android, functional alterations in the configuration setting may be introduced, leading to compatibility problems that result in incongruous application functionalities at varying API levels. These issues often result in software crashes and inconsistent user experiences across different Android versions. Current research presents a high number of false positives and negatives in issue detection owing to insufficient data flow analysis and an inability to accurately represent the XML tree hierarchies in Android configuration files. Additionally, there is a dearth of knowledge regarding how modifications in the Android framework can lead to these compatibility problems. To address these gaps, we undertook a comprehensive study, analyzing 196 real-world issues from 43 widely-used applications. We discerned recurring patterns in Android framework code modifications that contribute to these configuration compatibility issues. Based on these findings, we developed ConfDroid, an innovative system that can autonomously derive rules for identifying configuration compatibility problems. The primary approach is to perform symbolic execution based on a model derived from common code modification patterns. Experimental outcomes indicate that ConfDroid can successfully generate 282 reliable issue detection rules with a precision rate of 91.9%. Of these, 65 rules can identify issues not detectable by existing state-of-the-art baseline rules. Most notably, 11 of these rules have led to the identification of 107 reproducible configuration compatibility issues in 30 out of 316 real-world Android applications, which were previously undetectable by the baseline methods.",
      "original_id": "oai:arXiv.org:2109.00300",
      "created": "2021-09-01",
      "categories": [
        "cs.SE"
      ]
    },
    {
      "title": "Seeing Implicit Neural Representations as Fourier Series",
      "original_abstract": "Implicit Neural Representations (INR) use multilayer perceptrons to represent\nhigh-frequency functions in low-dimensional problem domains. Recently these\nrepresentations achieved state-of-the-art results on tasks related to complex\n3D objects and scenes. A core problem is the representation of highly detailed\nsignals, which is tackled using networks with periodic activation functions\n(SIRENs) or applying Fourier mappings to the input. This work analyzes the\nconnection between the two methods and shows that a Fourier mapped perceptron\nis structurally like one hidden layer SIREN. Furthermore, we identify the\nrelationship between the previously proposed Fourier mapping and the general\nd-dimensional Fourier series, leading to an integer lattice mapping. Moreover,\nwe modify a progressive training strategy to work on arbitrary Fourier mappings\nand show that it improves the generalization of the interpolation task. Lastly,\nwe compare the different mappings on the image regression and novel view\nsynthesis tasks. We confirm the previous finding that the main contributor to\nthe mapping performance is the size of the embedding and standard deviation of\nits elements.",
      "generated_abstract": "Title: A Comparative Analysis of Implicit Neural Representations via Fourier Series\n\nRevised Abstract: The study explores the usage of Implicit Neural Representations (INR), utilizing multilayer perceptrons in order to depict high-frequency functions within low-dimensional issues. These representations have recently acquired top-notch outcomes within complex 3D objects and scenes-related tasks. The primary challenge lies in the portrayal of exceptionally detailed signals, which is approached by employing networks with periodic activation functions (SIRENs) or introducing Fourier mappings to the input. The research scrutinizes the correlation between these two techniques, illustrating that a Fourier mapped perceptron structurally resembles a single hidden layer SIREN. The study also underscores the link between the formerly suggested Fourier mapping and the generic d-dimensional Fourier series, culminating in an integer lattice mapping. In addition, we adapt a progressive training approach to function with arbitrary Fourier mappings, demonstrating its efficacy in enhancing the generalization of the interpolation task. Lastly, we contrast the varying mappings on image regression and novel view synthesis tasks, corroborating the previous discovery that the primary contributor to the mapping performance is the magnitude of the embedding and the standard deviation of its components.",
      "original_id": "oai:arXiv.org:2109.00249",
      "created": "2021-09-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "The metal-poor end of the Spite plateau. II. Detailed chemical\n  investigation",
      "original_abstract": "Context. The study of old, metal-poor stars deepens our knowledge on the\nearly stages of the universe. In particular, the study of these stars gives us\na valuable insight into the masses of the first massive stars and their\nemission of ionising photons. Aims. We present a detailed chemical analysis and\ndetermination of the kinematic and orbital properties of a sample of 11 dwarf\nstars. These are metal-poor stars, and a few of them present a low lithium\ncontent. We inspected whether the other elements also present anomalies.\nMethods. We analysed the high-resolution UVES spectra of a few metal-poor stars\nusing the Turbospectrum code to synthesise spectral lines profiles. This\nallowed us to derive a detailed chemical analysis of Fe,",
      "generated_abstract": "Title: An In-depth Chemical Investigation of the Metal-Deficient Spectrum of the Spite Plateau: Part II\n\nRewritten Abstract: Background. Our understanding of the universe's early periods is significantly enhanced by examining aged, metal-poor stars. Specifically, these stellar studies provide invaluable data regarding the weights of the initial large-scale stars and their output of ionising photons. Objectives. Our research offers a comprehensive chemical assessment and quantification of the kinematic and orbital parameters of a group of 11 dwarf stars, known for their metal-deficiency. A handful of these stars display a reduced lithium presence, prompting us to examine if anomalies are also observable in other elements. Methods. We employed Turbospectrum code to examine the high-resolution UVES spectra of several metal-deficient stars and generate spectral line profiles. This method facilitated a thorough chemical examination of Fe.",
      "original_id": "oai:arXiv.org:2110.00243",
      "created": "2021-10-01",
      "categories": [
        "astro-ph.SR",
        "astro-ph.GA"
      ]
    },
    {
      "title": "Skew-product dynamical systems for crossed product $C^*$-algebras and\n  their ergodic properties",
      "original_abstract": "Starting from a discrete $C^*$-dynamical system $(\\mathfrak{A}, \\theta,\n\\omega_o)$, we define and study most of the main ergodic properties of the\ncrossed product $C^*$-dynamical system $(\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z},\n\\Phi_{\\theta, u},\\om_o\\circ E)$,\n$E:\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z}\\rightarrow\\ga$ being the canonical\nconditional expectation of $\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z}$ onto\n$\\mathfrak{A}$, provided $\\a\\in\\aut(\\ga)$ commute with the $*$-automorphism\n$\\th$ up tu a unitary $u\\in\\ga$. Here, $\\Phi_{\\theta,\nu}\\in\\aut(\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z})$ can be considered as the fully\nnoncommutative generalisation of the celebrated skew-product defined by H.\nAnzai for the product of two tori in the classical case.",
      "generated_abstract": "Title: Investigation of Skew-Product Dynamical Systems in Crossed Product $C^*$-Algebras and Assessment of Their Ergodic Attributes\n\nRevised Abstract: Utilizing a discrete $C^*$-dynamical system, designated as $(\\mathfrak{A}, \\theta, \\omega_o)$, as our starting point, this study examines the primary ergodic characteristics of the crossed product $C^*$-dynamical system, denoted as $(\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z}, \\Phi_{\\theta, u},\\om_o\\circ E)$. Here, $E:\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z}\\rightarrow\\ga$ is the standard conditional expectation of $\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z}$ onto $\\mathfrak{A}$, assuming $\\a\\in\\aut(\\ga)$ aligns with the $*$-automorphism $\\th$, up to a unitary $u\\in\\ga$. In this context, $\\Phi_{\\theta, u}\\in\\aut(\\mathfrak{A}\\rtimes_\\alpha\\mathbb{Z})$ can be interpreted as the comprehensive noncommutative extension of the well-known skew-product, which H. Anzai originally defined for the multiplication of two tori in the traditional scenario.",
      "original_id": "oai:arXiv.org:2105.00197",
      "created": "2021-05-01",
      "categories": [
        "math.OA",
        "math.DS"
      ]
    },
    {
      "title": "What Is the Generalized Representation of Dirac Equation in Two\n  Dimensions?",
      "original_abstract": "In this work, the general form of $2\\times2$ Dirac matrices for 2+1 dimension\nis found. In order to find this general representation, all relations among the\nelements of the matrices and matrices themselves are found,and the generalized\nLorentz transform matrix is also found under the effect of the general\nrepresentation of Dirac matrices. As we know, the well known equation of Dirac,\n$ \\left( i\\gamma^{\\mu}\\partial_{\\mu}-m\\right) \\Psi=0 $, is consist of matrices\nof even dimension known as the general representation of Dirac matrices or\nDirac matrices. Our motivation for this study was lack of the general\nrepresentation of these matrices despite the fact that more than nine decades\nhave been passed since the discovery of this well known equation. Everyone has\nused a specific representation of this equation according to their need; such\nas the standard representation known as Dirac-Pauli Representation, Weyl\nRepresentation or Majorana representation. In this work, the general form which\nthese matrices can have is found once for all.",
      "generated_abstract": "Title: Unveiling the Universal Form of Dirac Equation in Bi-Dimensional Space\n\nRevised Abstract: This study explores and identifies the universal structure of Dirac matrices in 2+1 dimensions, specifically those in the $2\\times2$ form. The process involved a comprehensive examination of the interrelations between matrix elements and the matrices themselves. Furthermore, the generalized Lorentz transform matrix was derived in light of the universal representation of Dirac matrices. The Dirac equation, $ \\left( i\\gamma^{\\mu}\\partial_{mu}-m\\right) \\Psi=0 $, is constituted by matrices of even dimensions, commonly referred to as Dirac matrices or the general representation of Dirac matrices. The impetus for this research was the absence of a universal representation of these matrices, despite over ninety years having passed since the inception of the renowned equation. To date, various specific representations, such as the Dirac-Pauli, Weyl, or Majorana representations, have been employed based on individual requirement. This investigation successfully uncovers the universal structure that these matrices can adopt, providing a comprehensive solution.",
      "original_id": "oai:arXiv.org:2104.00388",
      "created": "2021-04-01",
      "categories": [
        "quant-ph"
      ]
    },
    {
      "title": "Boosting Certified $\\ell_\\infty$ Robustness with EMA Method and Ensemble\n  Model",
      "original_abstract": "The neural network with $1$-Lipschitz property based on $\\ell_\\infty$-dist\nneuron has a theoretical guarantee in certified $\\ell_\\infty$ robustness.\nHowever, due to the inherent difficulties in the training of the network, the\ncertified accuracy of previous work is limited. In this paper, we propose two\napproaches to deal with these difficuties. Aiming at the characteristics of the\ntraining process based on $\\ell_\\infty$-norm neural network, we introduce the\nEMA method to improve the training process. Considering the randomness of the\ntraining algorithm, we propose an ensemble method based on trained base models\nthat have the $1$-Lipschitz property and gain significant improvement in the\nsmall parameter network. Moreover, we give the theoretical analysis of the\nensemble method based on the $1$-Lipschitz property on the certified\nrobustness, which ensures the effectiveness and stability of the algorithm. Our\ncode is available at\nhttps://github.com/Theia-4869/EMA-and-Ensemble-Lip-Networks.",
      "generated_abstract": "Title: Enhancing Certified $\\ell_\\infty$ Resilience using the EMA Technique and Ensemble Model\n\nGenerated Abstract: Neural networks that exhibit the $1$-Lipschitz attribute reliant on an $\\ell_\\infty$-dist neuron provide a theoretical assurance to certified $\\ell_\\infty$ robustness. Nonetheless, the inherent complexities involved in training such networks limit the certified accuracy established in prior research. This study presents two strategies to overcome these challenges. To address the unique properties of the training process reliant on the $\\ell_\\infty$-norm neural network, we incorporate the EMA method to refine the training process. Furthermore, considering the unpredictable nature of the training algorithm, we suggest an ensemble approach that uses previously trained base models that possess the $1$-Lipschitz attribute. This significantly improves the performance in networks with small parameters. Additionally, we provide a theoretical analysis of the ensemble approach, indicating its effectiveness and stability based on the $1$-Lipschitz feature in certified robustness. The algorithm's effectiveness and stability are thus ensured. The code for this study is accessible at https://github.com/Theia-4869/EMA-and-Ensemble-Lip-Networks.",
      "original_id": "oai:arXiv.org:2107.00230",
      "created": "2021-07-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Collision Chains among the Terrestrial Planets. III. Formation of the\n  Moon",
      "original_abstract": "In the canonical model of Moon formation, a Mars-sized protoplanet \"Theia\"\ncollides with proto-Earth at close to their mutual escape velocity $v_{\\rm\nesc}$ and a common impact angle 45{\\deg}. The \"graze-and-merge\" collision\nstrands a fraction of Theia's mantle into orbit, while Earth accretes most of\nTheia and its momentum. Simulations show that this produces a hot, high angular\nmomentum, silicate-dominated protolunar system, in substantial agreement with\nlunar geology, geochemistry, and dynamics. However, a Moon that derives mostly\nfrom Theia's mantle, as angular momentum dictates, is challenged by the fact\nthat O, Ti, Cr, radiogenic W, and other elements are indistinguishable in Earth\nand lunar rocks. Moreover, the model requires an improbably low initial\nvelocity. Here we develop a scenario for Moon formation that begins with a\nsomewhat faster collision, when proto-Theia impacts proto-Earth at ~1.2 $v_{\\rm\nesc}$, also around 45{\\deg}. Instead of merging, the bodies come into violent\ncontact for a half-hour and their major components escape, a \"hit-and-run\ncollision.\" N-body evolutions show that the \"runner\" often returns ~0.1-1 Myr\nlater for a second giant impact, closer to $v_{\\rm esc}$; this produces a\npostimpact disk of ~2-3 lunar masses in smoothed particle hydrodynamics\nsimulations, with angular momentum comparable to canonical scenarios. The disk\nends up substantially inclined, in most cases, because the terminal collision\nis randomly oriented to the first. Proto-Earth contributions to the silicate\ndisk are enhanced by the compounded mixing and greater energy of a collision\nchain.",
      "generated_abstract": "Title: Series of Collisions Among Terrestrial Planets: The Development of the Moon, Part III\n\nRevised Abstract: The traditional model of lunar formation suggests a Mars-sized celestial body coined \"Theia\" initiated a collision with the early Earth at nearly their combined escape velocity $v_{\\rm esc}$ and shared an impact angle of 45{\\deg}. This \"graze-and-merge\" impact results in a portion of Theia's mantle being cast into orbit, while the Earth absorbs the majority of Theia and its momentum. Computational models reveal this leads to a heated, high-angular momentum, and silicate-rich protolunar system, which aligns well with the established lunar geology, geochemistry, and dynamics. However, the model encounters obstacles, primarily that the Moon, which should be largely composed of Theia's mantle as per angular momentum, exhibits O, Ti, Cr, radiogenic W, and other elements that are identical in both lunar and terrestrial rocks. Moreover, the model is dependent on an unlikely low initial velocity. This study proposes a modified scenario for lunar genesis, wherein proto-Theia collides with proto-Earth at approximately 1.2 $v_{\\rm esc}$, also around 45{\\deg}. Rather than merging, the bodies undergo a violent, brief contact, termed a \"hit-and-run collision,\" resulting in the escape of their primary components. The \"hit-and-run\" object often returns for a second major impact with velocities closer to $v_{\\rm esc}$ after an estimated 0.1-1 Myr. Smoothed particle hydrodynamics simulations demonstrate this leads to a post-impact disk with around 2-3 lunar masses and an angular momentum similar to traditional models. The resulting disk is typically significantly inclined due to the random orientation of the terminal collision to the initial one. The compounded mixing and amplified energy of a collision chain bolster the contributions of the proto-Earth to the silicate disk.",
      "original_id": "oai:arXiv.org:2110.00222",
      "created": "2021-10-01",
      "categories": [
        "astro-ph.EP"
      ]
    },
    {
      "title": "Knowledge-driven Site Selection via Urban Knowledge Graph",
      "original_abstract": "Site selection determines optimal locations for new stores, which is of\ncrucial importance to business success. Especially, the wide application of\nartificial intelligence with multi-source urban data makes intelligent site\nselection promising. However, existing data-driven methods heavily rely on\nfeature engineering, facing the issues of business generalization and complex\nrelationship modeling. To get rid of the dilemma, in this work, we borrow ideas\nfrom knowledge graph (KG), and propose a knowledge-driven model for site\nselection, short for KnowSite. Specifically, motivated by distilled knowledge\nand rich semantics in KG, we firstly construct an urban KG (UrbanKG) with\ncities' key elements and semantic relationships captured. Based on UrbanKG, we\nemploy pre-training techniques for semantic representations, which are fed into\nan encoder-decoder structure for site decisions. With multi-relational message\npassing and relation path-based attention mechanism developed, KnowSite\nsuccessfully reveals the relationship between various businesses and site\nselection criteria. Extensive experiments on two datasets demonstrate that\nKnowSite outperforms representative baselines with both effectiveness and\nexplainability achieved.",
      "generated_abstract": "Title: Utilizing Urban Knowledge Graph for Informed Site Selection: A Knowledge-based Approach\n\nRevised Abstract: The determination of optimal locations for new business ventures, such as stores, is critical to their success. The current landscape of artificial intelligence application, coupled with the availability of multi-source urban data, offers great potential for intelligent site selection. Nevertheless, existing data-driven methodologies are largely dependent on feature engineering, thus posing challenges in business generalization and modeling complex relationships. In response to this, our research introduces a model, referred to as KnowSite, that leverages ideas from knowledge graph (KG) for site selection. Inspired by the distilled knowledge and rich semantics in KG, we initially create an Urban Knowledge Graph (UrbanKG), encapsulating key elements and semantic relationships of cities. Utilizing UrbanKG, we apply pre-training methods for semantic representations, which are subsequently input into an encoder-decoder structure for site selection decisions. KnowSite, enhanced by multi-relational message passing and relation path-based attention mechanism, effectively uncovers the interconnection between diverse businesses and site selection parameters. Rigorous testing on two datasets affirms that KnowSite surpasses comparable baselines in terms of both efficiency and interpretability.",
      "original_id": "oai:arXiv.org:2111.00787",
      "created": "2021-11-01",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Did the Event Horizon Telescope Detect the Base of the Sub-Milliarsecond\n  Tubular Jet in M\\,87?",
      "original_abstract": "A high sensitivity, 7mm Very Long Baseline Array image of M\\,87 was\npreviously analyzed in order to estimate the bulk flow jet velocity between 0.4\nand 0.65 mas from the point of origin using the asymmetry between the\nwell-characterized double-ridged counter-jet (unique to this image) and the\ndouble ridged jet. We use this same image to estimate the cross-sectional area\nof this tubular stream. The velocity, acceleration, cross-sectional area and\nflux density along this stream determines a unique, perfect magnetohydrodynamic\njet solution that satisfies, conservation of energy, angular momentum and mass\n(a monotonic conversion of Poynting flux to kinetic energy flux along the jet).\nThe solution is protonic and magnetically dominated. The bilateral jet\ntransports $\\approx 1.2\\times10^{-4} M_{\\odot}/\\rm{yr}$ and $\\approx\n1.1\\times10^{42}$ erg/sec, placing strong constraints on the central engine. A\nKeplerian disk source that also produces the Event Horizon Telescope (EHT)\nannulus of emission can supply the energy and mass if the vertical magnetic\nfield at the equator is $\\sim 1-3.5$ G (depending on location). A Parker spiral\nmagnetic field, characteristic of a wind or jet, is consistent with the\nobserved EHT polarization pattern. Even though there is no image of the jet\nconnecting with the annulus, it is argued that these circumstances are not\ncoincidental and the polarized portion of the EHT emission is mainly jet\nemission in the top layers of the disk that is diluted by emission from an\nunderlying turbulent disk. This is a contributing factor to the relatively low\npolarization levels that were detected.",
      "generated_abstract": "Title: The Detection of the Sub-Milliarcsecond Tubular Jet Base in M\\,87 by the Event Horizon Telescope: An Analysis\n\nRevised Abstract: A highly sensitive image of M\\,87, captured at 7mm by the Very Long Baseline Array, was previously scrutinized to deduce the bulk flow jet velocity in the range of 0.4 to 0.65 mas from its source. Utilizing the asymmetry between the uniquely double-ridged counter-jet and the double-ridged jet, the image was re-analyzed to quantify the tubular stream's cross-sectional area. The jet's velocity, acceleration, cross-sectional area, and flux density were used to determine a singular, ideal magnetohydrodynamic jet solution that adheres to the laws of conservation of energy, angular momentum, and mass, signifying a steady conversion of Poynting flux into kinetic energy flux along the jet. The solution is predominantly protonic and magnetic. The bilateral jet carries approximately 1.2x10^-4 M_{\u2609}/yr and approximately 1.1x10^42 erg/sec, thereby imposing stringent constraints on the central engine. A Keplerian disk source, which also generates the Event Horizon Telescope's (EHT) emission ring, can deliver the required energy and mass, given a vertical magnetic field at the equator of about 1-3.5 G, contingent on location. The EHT's observed polarization pattern aligns with a Parker spiral magnetic field, indicative of a wind or jet. Despite the lack of a visual connection between the jet and the emission ring, it is contended that this is not random, and the EHT's polarized emission majorly stems from jet emission in the disk's upper strata, which is diluted by emission from a turbulent disk beneath. This serves as an explanation for the relatively low detected polarization levels.",
      "original_id": "oai:arXiv.org:2111.00692",
      "created": "2021-11-01",
      "categories": [
        "astro-ph.GA",
        "astro-ph.HE"
      ]
    },
    {
      "title": "Binary Mean Field Stochastic Games: Stationary Equilibria and\n  Comparative Statics",
      "original_abstract": "This paper considers mean field games in a multi-agent Markov decision\nprocess (MDP) framework. Each player has a continuum state and binary action,\nand benefits from the improvement of the condition of the overall population.\nBased on an infinite horizon discounted individual cost, we show existence of a\nstationary equilibrium, and prove its uniqueness under a positive externality\ncondition. We further analyze comparative statics of the stationary equilibrium\nby quantitatively determining the impact of the effort cost.",
      "generated_abstract": "Title: An Examination of Binary Mean Field Stochastic Games: Identifying Stationary Equilibria and Analyzing Comparative Statics\n\nRevised Abstract: This study delves into the realm of mean field games, utilizing a multi-agent Markov decision process (MDP) framework. Each participant possesses a continuous state and a binary action, reaping benefits from the enhancement of the overall population's condition. By employing an infinite horizon discounted individual cost, the existence of a stationary equilibrium is demonstrated, and its singularity is established under the presence of a positive externality circumstance. The research extends to assess the comparative statics of this stationary equilibrium, accurately ascertaining the influence of the effort cost.",
      "original_id": "oai:arXiv.org:2101.00335",
      "created": "2021-01-01",
      "categories": [
        "math.OC"
      ]
    },
    {
      "title": "PHOENIX: Device-Centric Cellular Network Protocol Monitoring using\n  Runtime Verification",
      "original_abstract": "End-user-devices in the current cellular ecosystem are prone to many\ndifferent vulnerabilities across different generations and protocol layers.\nFixing these vulnerabilities retrospectively can be expensive, challenging, or\njust infeasible. A pragmatic approach for dealing with such a diverse set of\nvulnerabilities would be to identify attack attempts at runtime on the device\nside, and thwart them with mitigating and corrective actions. Towards this\ngoal, in the paper we propose a general and extendable approach called Phoenix\nfor identifying n-day cellular network control-plane vulnerabilities as well as\ndangerous practices of network operators from the device vantage point. Phoenix\nmonitors the device-side cellular network traffic for performing\nsignature-based unexpected behavior detection through lightweight runtime\nverification techniques. Signatures in Phoenix can be manually-crafted by a\ncellular network security expert or can be automatically synthesized using an\noptional component of Phoenix, which reduces the signature synthesis problem to\nthe language learning from the informant problem. Based on the corrective\nactions that are available to Phoenix when an undesired behavior is detected,\ndifferent instantiations of Phoenix are possible: a full-fledged defense when\ndeployed inside a baseband processor; a user warning system when deployed as a\nmobile application; a probe for identifying attacks in the wild. One such\ninstantiation of Phoenix was able to identify all 15 representative n-day\nvulnerabilities and unsafe practices of 4G LTE networks considered in our\nevaluation with a high packet processing speed (~68000 packets/second) while\ninducing only a moderate amount of energy overhead (~4mW).",
      "generated_abstract": "Title: PHOENIX: Monitoring Cellular Network Protocols via Device-Centered and Real-Time Verification\n\nRevised Abstract: The existing cellular ecosystem exposes end-user devices to a plethora of vulnerabilities across various generations and protocol layers. Retrospective rectification of these vulnerabilities can be costly, complicated, and sometimes unachievable. A practical solution to address such a broad spectrum of vulnerabilities involves detecting attack attempts in real-time from the device side and responding with preventative and rectifying measures. In this paper, we introduce an adaptable methodology named Phoenix. This approach enables the identification of n-day cellular network control-plane vulnerabilities and risky network operator practices from the device perspective. Phoenix scrutinizes the cellular network traffic on the device side, performing signature-based anomaly detection using lightweight real-time verification techniques. Phoenix's signatures can either be manually designed by a cellular network security professional or automatically created using a supplementary Phoenix component, which simplifies the signature creation problem to the informant's language learning issue. Depending on the remedial measures available to Phoenix upon detection of an undesired behavior, various Phoenix implementations are feasible: a comprehensive defense when installed in a baseband processor; a user alert system when utilized as a mobile app; a probe to detect attacks in the wild. An implementation of Phoenix successfully identified all 15 representative n-day vulnerabilities and unsafe 4G LTE network practices in our assessment, demonstrating a high packet processing speed (~68000 packets/second) and only causing a modest energy overhead (~4mW).",
      "original_id": "oai:arXiv.org:2101.00328",
      "created": "2021-01-01",
      "categories": [
        "cs.CR"
      ]
    },
    {
      "title": "Livestock Monitoring with Transformer",
      "original_abstract": "Tracking the behaviour of livestock enables early detection and thus\nprevention of contagious diseases in modern animal farms. Apart from economic\ngains, this would reduce the amount of antibiotics used in livestock farming\nwhich otherwise enters the human diet exasperating the epidemic of antibiotic\nresistance - a leading cause of death. We could use standard video cameras,\navailable in most modern farms, to monitor livestock. However, most computer\nvision algorithms perform poorly on this task, primarily because, (i) animals\nbred in farms look identical, lacking any obvious spatial signature, (ii) none\nof the existing trackers are robust for long duration, and (iii) real-world\nconditions such as changing illumination, frequent occlusion, varying camera\nangles, and sizes of the animals make it hard for models to generalize. Given\nthese challenges, we develop an end-to-end behaviour monitoring system for\ngroup-housed pigs to perform simultaneous instance level segmentation,\ntracking, action recognition and re-identification (STAR) tasks. We present\nstarformer, the first end-to-end multiple-object livestock monitoring framework\nthat learns instance-level embeddings for grouped pigs through the use of\ntransformer architecture. For benchmarking, we present Pigtrace, a carefully\ncurated dataset comprising video sequences with instance level bounding box,\nsegmentation, tracking and activity classification of pigs in real indoor\nfarming environment. Using simultaneous optimization on STAR tasks we show that\nstarformer outperforms popular baseline models trained for individual tasks.",
      "generated_abstract": "Title: Implementation of Transformer for Livestock Surveillance\n\nRevised Abstract: The surveillance of livestock behaviour holds the potential for early identification and subsequent mitigation of infectious diseases in contemporary animal farming practices. This not only yields economic benefits but also decreases the use of antibiotics in animal farming, thereby reducing the contribution to the rising issue of antibiotic resistance - a major mortality cause. Standard video cameras prevalent in most modern farms could be utilized for monitoring livestock. Yet, most computer vision algorithms underperform in this area, primarily due to a) the similarity in appearance of farm-bred animals, b) lack of robustness in existing trackers for extended durations, and c) real-world factors like changing light conditions, frequent concealment, varying camera perspectives, and animal size disparities, which hinder model generalization. In response to these challenges, we have devised a comprehensive behaviour monitoring system for group-housed pigs, capable of simultaneous instance level segmentation, tracking, action recognition, and re-identification (STAR) tasks. We introduce starformer, the inaugural end-to-end multiple-object livestock surveillance framework that learns instance-level embeddings for grouped pigs via transformer architecture. For performance evaluation, we unveil Pigtrace, a meticulously assembled dataset featuring video sequences with instance level bounding box, segmentation, tracking, and activity classification of pigs in authentic indoor farming settings. By employing simultaneous optimization on STAR tasks, we demonstrate that starformer surpasses well-known baseline models designed for individual tasks.",
      "original_id": "oai:arXiv.org:2111.00801",
      "created": "2021-11-01",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Multi-view Clustering with Deep Matrix Factorization and Global Graph\n  Refinement",
      "original_abstract": "Multi-view clustering is an important yet challenging task in machine\nlearning and data mining community. One popular strategy for multi-view\nclustering is matrix factorization which could explore useful feature\nrepresentations at lower-dimensional space and therefore alleviate dimension\ncurse. However, there are two major drawbacks in the existing work: i) most\nmatrix factorization methods are limited to shadow depth, which leads to the\ninability to fully discover the rich hidden information of original data. Few\ndeep matrix factorization methods provide a basis for the selection of the new\nrepresentation's dimensions of different layers. ii) the majority of current\napproaches only concentrate on the view-shared information and ignore the\nspecific local features in different views. To tackle the above issues, we\npropose a novel Multi-View Clustering method with Deep semi-NMF and Global\nGraph Refinement (MVC-DMF-GGR) in this paper. Firstly, we capture new\nrepresentation matrices for each view by hierarchical decomposition, then learn\na common graph by approximating a combination of graphs which are reconstructed\nfrom these new representations to refine the new representations in return. An\nalternate algorithm with proved convergence is then developed to solve the\noptimization problem and the results on six multi-view benchmarks demonstrate\nthe effectiveness and superiority of our proposed algorithm.",
      "generated_abstract": "Title: Advanced Clustering using Deep Matrix Factorization and Global Graph Refinement in Multiple Views\n\nRedefined Abstract: The execution of multi-view clustering presents a significant, yet complex undertaking within the realms of machine learning and data mining. A widely utilized approach for this process involves matrix factorization, which allows for the extraction of valuable feature representations in a lower-dimensional domain, thereby mitigating the curse of dimensionality. Yet, two major shortcomings persist in current practices: i) the majority of matrix factorization techniques are confined to shallow depth, hindering the comprehensive extraction of the abundant covert information inherent in the original data. There is a scarcity of deep matrix factorization methods that offer solid grounds for choosing the dimensions of the new representation layers. ii) Present methods predominantly focus on information shared across views, often overlooking specific local features unique to individual views. In response to these challenges, we introduce a cutting-edge Multi-View Clustering approach using Deep semi-NMF and Global Graph Refinement (MVC-DMF-GGR). Initially, we generate new representation matrices for each view via a hierarchical breakdown, followed by the learning of a shared graph, achieved by approximating a fusion of graphs reconstructed from these new representations to further refine them. Subsequently, we devise an alternating algorithm with confirmed convergence to resolve the optimization issue. The efficacy and superiority of our proposed algorithm are validated through outcomes on six multi-view benchmarks.",
      "original_id": "oai:arXiv.org:2105.00248",
      "created": "2021-05-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "ADAADepth: Adapting Data Augmentation and Attention for Self-Supervised\n  Monocular Depth Estimation",
      "original_abstract": "Self-supervised learning of depth has been a highly studied topic of research\nas it alleviates the requirement of having ground truth annotations for\npredicting depth. Depth is learnt as an intermediate solution to the task of\nview synthesis, utilising warped photometric consistency. Although it gives\ngood results when trained using stereo data, the predicted depth is still\nsensitive to noise, illumination changes and specular reflections. Also,\nocclusion can be tackled better by learning depth from a single camera. We\npropose ADAA, utilising depth augmentation as depth supervision for learning\naccurate and robust depth. We propose a relational self-attention module that\nlearns rich contextual features and further enhances depth results. We also\noptimize the auto-masking strategy across all losses by enforcing L1\nregularisation over mask. Our novel progressive training strategy first learns\ndepth at a lower resolution and then progresses to the original resolution with\nslight training. We utilise a ResNet18 encoder, learning features for\nprediction of both depth and pose. We evaluate our predicted depth on the\nstandard KITTI driving dataset and achieve state-of-the-art results for\nmonocular depth estimation whilst having significantly lower number of\ntrainable parameters in our deep learning framework. We also evaluate our model\non Make3D dataset showing better generalization than other methods.",
      "generated_abstract": "Title: ADAADepth: Enhancing Monocular Depth Estimation through Adaptive Data Augmentation and Attention Mechanism\n\nRevised Abstract: The self-directed learning approach in depth understanding is a prevalent research field, given that it negates the need for ground truth labels in depth prediction. The technique leverages warped photometric uniformity to learn depth as an intermediate solution in view synthesis. While this method performs well with stereo data, the estimated depth remains susceptible to variations in lighting, noise, and specular reflections. Moreover, learning depth from a single camera can better address occlusion issues. We put forth ADAA, an approach that employs depth augmentation for supervising depth learning to yield precise and robust depth. We suggest a relational self-attention module that harnesses rich contextual features to improve depth results. We further refine the auto-masking approach across all losses via L1 regularisation on the mask. Our unique progressive training approach initially learns depth at a lower resolution and then advances to the original resolution with minimal training. We employ a ResNet18 encoder to learn features for predicting both depth and pose. We test our depth prediction on the benchmark KITTI driving dataset, achieving unparalleled results for monocular depth estimation with a significantly reduced number of trainable parameters in our deep learning model. We also assess our model on the Make3D dataset, demonstrating superior generalization compared to other techniques.",
      "original_id": "oai:arXiv.org:2103.00853",
      "created": "2021-03-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "NodeSim: Node Similarity based Network Embedding for Diverse Link\n  Prediction",
      "original_abstract": "In real-world complex networks, understanding the dynamics of their evolution\nhas been of great interest to the scientific community. Predicting future links\nis an essential task of social network analysis as the addition or removal of\nthe links over time leads to the network evolution. In a network, links can be\ncategorized as intra-community links if both end nodes of the link belong to\nthe same community, otherwise inter-community links. The existing\nlink-prediction methods have mainly focused on achieving high accuracy for\nintra-community link prediction. In this work, we propose a network embedding\nmethod, called NodeSim, which captures both similarities between the nodes and\nthe community structure while learning the low-dimensional representation of\nthe network. The embedding is learned using the proposed NodeSim random walk,\nwhich efficiently explores the diverse neighborhood while keeping the more\nsimilar nodes closer in the context of the node. We verify the efficacy of the\nproposed embedding method over state-of-the-art methods using diverse link\nprediction. We propose a machine learning model for link prediction that\nconsiders both the nodes' embedding and their community information to predict\nthe link between two given nodes. Extensive experimental results on several\nreal-world networks demonstrate the effectiveness of the proposed framework for\nboth inter and intra-community link prediction.",
      "generated_abstract": "Title: NodeSim: A Novel Approach for Network Embedding Using Node Similarity for Varied Link Prediction\n\nRevised Abstract: The scientific community has exhibited profound interest in comprehending the dynamic evolution of real-world complex networks. A crucial aspect of social network analysis involves predicting future links as the network's evolution is dictated by the continual addition or subtraction of these links. Links within a network can be classified into intra-community links when both link endpoints belong to the same community, and inter-community links when they do not. Previous link-prediction methodologies have primarily concentrated on enhancing the accuracy of intra-community link predictions. This study introduces a network embedding method, NodeSim, which concurrently captures node similarities and community structure while determining the network's low-dimensional representation. NodeSim employs a unique random walk approach to efficiently navigate the diverse neighborhood, ensuring nodes with higher similarity remain proximate within the node context. The effectiveness of NodeSim is validated through diverse link prediction, outperforming contemporary methods. Additionally, this study presents a machine learning model for link prediction that incorporates both node embedding and community data to predict a link between any two nodes. Comprehensive experimental outcomes from multiple real-world networks substantiate the efficiency of the proposed framework in predicting both inter and intra-community links.",
      "original_id": "oai:arXiv.org:2102.00785",
      "created": "2021-02-01",
      "categories": [
        "cs.SI"
      ]
    },
    {
      "title": "Exact verification of the strong BSD conjecture for some absolutely\n  simple abelian surfaces",
      "original_abstract": "Let $X$ be one of the $28$ Atkin-Lehner quotients of a curve $X_0(N)$ such\nthat $X$ has genus $2$ and its Jacobian variety $J$ is absolutely simple. We\nshow that the Shafarevich-Tate group of $J/\\mathbb{Q}$ is trivial. This\nverifies the strong BSD conjecture for $J$.",
      "generated_abstract": "Title: Precise Validation of the Robust BSD Hypothesis for Certain Uncomplicated Abelian Surfaces\n\nGenerated Abstract: Assume $X$ to be one among the $28$ Atkin-Lehner divisions of a curve represented as $X_0(N)$. In instances where $X$ possesses a genus of $2$ and the Jacobian variety $J$ is completely simple, we demonstrate that the Shafarevich-Tate group pertaining to $J/\\mathbb{Q}$ is insignificant. This provides validation for the robust BSD supposition in relation to $J$.",
      "original_id": "oai:arXiv.org:2107.00325",
      "created": "2021-07-01",
      "categories": [
        "math.NT",
        "math.AG"
      ]
    },
    {
      "title": "Bayesian Agency: Linear versus Tractable Contracts",
      "original_abstract": "We study principal-agent problems in which a principal commits to an\noutcome-dependent payment scheme (a.k.a. contract) so as to induce an agent to\ntake a costly, unobservable action. We relax the assumption that the principal\nperfectly knows the agent by considering a Bayesian setting where the agent's\ntype is unknown and randomly selected according to a given probability\ndistribution, which is known to the principal. Each agent's type is\ncharacterized by her own action costs and action-outcome distributions. In the\nliterature on non-Bayesian principal-agent problems, considerable attention has\nbeen devoted to linear contracts, which are simple, pure-commission payment\nschemes that still provide nice approximation guarantees with respect to\nprincipal-optimal (possibly non-linear) contracts. While in non-Bayesian\nsettings an optimal contract can be computed efficiently, this is no longer the\ncase for our Bayesian principal-agent problems. This further motivates our\nfocus on linear contracts, which can be optimized efficiently given their\nsingle-parameter nature. Our goal is to analyze the properties of linear\ncontracts in Bayesian settings, in terms of approximation guarantees with\nrespect to optimal contracts and general tractable contracts (i.e.,\nefficiently-computable ones). First, we study the approximation guarantees of\nlinear contracts with respect to optimal ones, showing that the former suffer\nfrom a multiplicative loss linear in the number of agent's types. Nevertheless,\nwe prove that linear contracts can still provide a constant multiplicative\napproximation $\\rho$ of the optimal principal's expected utility, though at the\nexpense of an exponentially-small additive loss $2^{-\\Omega(\\rho)}$. Then, we\nswitch to tractable contracts, showing that, surprisingly, linear contracts\nperform well among them.",
      "generated_abstract": "Title: Comparative Analysis of Linear and Tractable Contracts in a Bayesian Agency Framework\n\nRevised Abstract: This research paper investigates principal-agent dilemmas where the principal establishes an outcome-dependent remuneration plan (also known as a contract) to encourage an agent to perform a costly, unseen action. We move away from the notion that the principal has complete knowledge of the agent and instead consider a Bayesian environment where the agent's type is undetermined and chosen randomly based on a known probability distribution. The agent's type is defined by the cost of their actions and the distribution of action outcomes. Significant emphasis has been placed on linear contracts in non-Bayesian principal-agent problem studies. These contracts are simple, commission-based payment plans that offer sound approximation assurances compared to optimal principal contracts, which may not necessarily be linear. However, the efficient computation of an optimal contract in non-Bayesian scenarios does not apply to our Bayesian principal-agent issues, further justifying our focus on linear contracts due to their single-parameter nature. We aim to explore the characteristics of linear contracts within Bayesian contexts, particularly in relation to approximation assurances compared to optimal contracts and general tractable contracts (those that can be computed efficiently). Initially, we examine the approximation assurances of linear contracts compared to optimal contracts, revealing that the former incurs a multiplicative loss linear to the number of agent types. Despite this, we establish that linear contracts can maintain a constant multiplicative approximation of the optimal principal's expected utility, albeit with an exponentially small additive loss. Finally, we consider tractable contracts and demonstrate that linear contracts perform well when compared to them.",
      "original_id": "oai:arXiv.org:2106.00319",
      "created": "2021-06-01",
      "categories": [
        "cs.GT"
      ]
    },
    {
      "title": "Inverse reinforcement learning for autonomous navigation via\n  differentiable semantic mapping and planning",
      "original_abstract": "This paper focuses on inverse reinforcement learning for autonomous\nnavigation using distance and semantic category observations. The objective is\nto infer a cost function that explains demonstrated behavior while relying only\non the expert's observations and state-control trajectory. We develop a map\nencoder, that infers semantic category probabilities from the observation\nsequence, and a cost encoder, defined as a deep neural network over the\nsemantic features. Since the expert cost is not directly observable, the model\nparameters can only be optimized by differentiating the error between\ndemonstrated controls and a control policy computed from the cost estimate. We\npropose a new model of expert behavior that enables error minimization using a\nclosed-form subgradient computed only over a subset of promising states via a\nmotion planning algorithm. Our approach allows generalizing the learned\nbehavior to new environments with new spatial configurations of the semantic\ncategories. We analyze the different components of our model in a minigrid\nenvironment. We also demonstrate that our approach learns to follow traffic\nrules in the autonomous driving CARLA simulator by relying on semantic\nobservations of buildings, sidewalks, and road lanes.",
      "generated_abstract": "Title: Autonomous Navigation through Inverse Reinforcement Learning using Differentiable Semantic Mapping and Planning\n\nRevised Abstract: This study addresses the application of inverse reinforcement learning in autonomous navigation, utilizing both distance and semantic category observations. The goal is to decipher a cost function that elucidates displayed behavior, depending entirely on observations made by the expert and the trajectory of state-control. We introduce a map encoder that deduces semantic category probabilities from the sequence of observations, alongside a cost encoder, characterized as a deep neural network spanning the semantic features. With the expert cost being unobservable directly, the optimization of model parameters can only be achieved by differentiating the discrepancy between demonstrated controls and a control policy derived from the cost estimation. We advocate a novel model of expert behavior which facilitates error reduction by employing a closed-form subgradient, calculated solely over a selection of promising states via a motion planning algorithm. Our methodology enables the extrapolation of the acquired behavior to fresh environments featuring new spatial arrangements of semantic categories. We evaluate the distinct elements of our model within a minigrid environment and substantiate that our method is capable of adhering to traffic regulations in the CARLA simulator for autonomous driving, dependent on semantic observations of structures, sidewalks, and road lanes.",
      "original_id": "oai:arXiv.org:2101.00186",
      "created": "2021-01-01",
      "categories": [
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "A Separable Temporal Convolution Neural Network with Attention for\n  Small-Footprint Keyword Spotting",
      "original_abstract": "Keyword spotting (KWS) on mobile devices generally requires a small memory\nfootprint. However, most current models still maintain a large number of\nparameters in order to ensure good performance. To solve this problem, this\npaper proposes a separable temporal convolution neural network with attention,\nit has a small number of parameters. Through the time convolution combined with\nattention mechanism, a small number of parameters model (32.2K) is implemented\nwhile maintaining high performance. The proposed model achieves 95.7% accuracy\non the Google Speech Commands dataset, which is close to the performance of\nRes15(239K), the state-of-the-art model in KWS at present.",
      "generated_abstract": "Title: Implementing a Temporally Separable Convolution Neural Network with Attention for Efficient Keyword Spotting\n\nRevised Abstract: The challenge of implementing keyword spotting (KWS) on mobile platforms typically lies in balancing the small memory footprint requirement with the need for performance efficiency. Existing models tend to retain a high number of parameters to optimize performance, leading to significant memory usage. This study presents a novel approach through a temporally separable convolution neural network imbued with an attention mechanism, characterized by a minimized parameter count. Utilizing temporal convolution in conjunction with an attention mechanism, the model maintains high performance while implementing a significantly reduced parameter model (32.2K). With a remarkable accuracy of 95.7% on the Google Speech Commands dataset, the proposed model's performance closely rivals that of the current state-of-the-art KWS model, Res15 (with 239K parameters).",
      "original_id": "oai:arXiv.org:2109.00260",
      "created": "2021-09-01",
      "categories": [
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Pseudo-Spherical Contrastive Divergence",
      "original_abstract": "Energy-based models (EBMs) offer flexible distribution parametrization.\nHowever, due to the intractable partition function, they are typically trained\nvia contrastive divergence for maximum likelihood estimation. In this paper, we\npropose pseudo-spherical contrastive divergence (PS-CD) to generalize maximum\nlikelihood learning of EBMs. PS-CD is derived from the maximization of a family\nof strictly proper homogeneous scoring rules, which avoids the computation of\nthe intractable partition function and provides a generalized family of\nlearning objectives that include contrastive divergence as a special case.\nMoreover, PS-CD allows us to flexibly choose various learning objectives to\ntrain EBMs without additional computational cost or variational minimax\noptimization. Theoretical analysis on the proposed method and extensive\nexperiments on both synthetic data and commonly used image datasets demonstrate\nthe effectiveness and modeling flexibility of PS-CD, as well as its robustness\nto data contamination, thus showing its superiority over maximum likelihood and\n$f$-EBMs.",
      "generated_abstract": "Title: An Advanced Approach to Contrastive Divergence: The Pseudo-Spherical Technique\n\nReinterpreted Abstract: Flexible distribution parameterization is a key advantage of Energy-based models (EBMs), although their use is typically hampered by the unmanageable partition function, leading to the adoption of contrastive divergence for maximum likelihood approximation. This paper presents a novel approach - the pseudo-spherical contrastive divergence (PS-CD) - which expands the scope of maximum likelihood training of EBMs. PS-CD is the result of maximizing a series of strictly proper homogeneous scoring rules, thereby circumventing the need to compute the complex partition function. This method also introduces a comprehensive suite of learning objectives, incorporating contrastive divergence as a particular instance. PS-CD offers the flexibility to select diverse learning objectives for EBM training, without incurring extra computational expenses or requiring variational minimax optimization. Detailed theoretical examination of our proposed technique and numerous experiments on both synthetic data and widely recognized image datasets underscore the efficiency, versatility, and robustness of PS-CD, even in the presence of data contamination. These results highlight its superiority over existing methods such as maximum likelihood and $f$-EBMs.",
      "original_id": "oai:arXiv.org:2111.00780",
      "created": "2021-11-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Extending Harvey's Surface Kernel Maps",
      "original_abstract": "Let $S$ be a compact Riemann surface and $G$ a group of conformal\nautomorphisms of $S$ with $S_0 = S/G$. $S$ is a finite regular branched cover\nof $S_0$. If $U$ denotes the unit disc, let $\\Gamma$ and $\\Gamma_0$ be the\nFuchsian groups with $S = U/{\\Gamma}$ and $S_0 = U/{\\Gamma_0}$. There is a\ngroup homomorphism of $\\Gamma_0$ onto $G$ with kernel $\\Gamma$ and this is\ntermed a surface kernel map. Two surface kernel maps are equivalent if they\ndiffer by an automorphism of $\\Gamma_0$. In his 1971 paper Harvey showed that\nwhen $G$ is a cyclic group, there is a unique simplest representative for this\nequivalence class. His result has played an important role in establishing\nsubsequent results about conformal automorphism groups of surfaces. We extend\nhis result to some surface kernel maps onto arbitrary finite groups. These can\nbe used along with the Schreier-Reidemeister Theory to find a set of generators\nfor $\\Gamma$ and the action of $G$ as an outer automorphism group on the\nfundamental group of $S$ putting the action on the fundamental group and the\ninduced action on homology into a relatively simple format. As an example we\ncompute generators for the fundamental group and a homology basis together with\nthe action of $G$ when $G$ is ${\\mathcal{S}_3$, the symmetric group on three\nletters. The action of $G$ shows that the homology basis found is not an\nadapted homology basis.",
      "generated_abstract": "Title: Broadening the Scope of Harvey's Surface Kernel Maps\n\nRevised Abstract: Consider a compact Riemann surface, $S$, and a conformal automorphism group, $G$, such that $S_0 = S/G$. This configuration represents $S$ as a regular finite branched cover over $S_0$. Given the unit disc, $U$, the Fuchsian groups $\\Gamma$ and $\\Gamma_0$ are defined with $S = U/{\\Gamma}$ and $S_0 = U/{\\Gamma_0}$. A group homomorphism from $\\Gamma_0$ onto $G$ with kernel $\\Gamma$ is classified as a surface kernel map. Two such maps are considered equivalent if a $\\Gamma_0$ automorphism separates them. In 1971, Harvey demonstrated that a unique simplest representative exists for this equivalence class when $G$ represents a cyclic group. This finding has proven foundational in the development of further results concerning conformal automorphism groups of surfaces. The present work broadens Harvey's finding to certain surface kernel maps onto arbitrary finite groups. This expansion facilitates the application of Schreier-Reidemeister Theory to determine a set of generators for $\\Gamma$ and the operation of $G$ as an outer automorphism group on $S$'s fundamental group. This places the action on the fundamental group and the subsequent homology action into a more comprehensible context. An illustrative example is provided whereby the generators for the fundamental group and a homology basis are computed in conjunction with the action of $G$ when $G$ is $\\mathcal{S}_3$ - the symmetric group on three letters. The $G$ action reveals that the discovered homology basis is not an adapted homology basis.",
      "original_id": "oai:arXiv.org:2105.00161",
      "created": "2021-05-01",
      "categories": [
        "math.GR"
      ]
    },
    {
      "title": "Families of hybridizable interior penalty discontinuous Galerkin methods\n  for degenerate advection-diffusion-reaction problems",
      "original_abstract": "We analyze families of primal high-order hybridizable discontinuous Galerkin\n(HDG) methods for solving degenerate (second-order) elliptic problems. One\nmajor trouble regarding this class of PDEs concerns its mathematical nature,\nwhich may be nonuniform over the domain. Due to the local degeneracy of the\ndiffusion term, it can be purely hyperbolic in a subregion and elliptic in the\nrest. This problem is thus quite delicate to solve since the exact solution is\ndiscontinuous at interfaces separating both elliptic and hyperbolic parts. The\nproposed HDG method is developed in a unified and compact fashion. It can\nefficiently handle pure diffusive or advective regimes and intermediate regimes\nthat combine the above mechanisms for a wide range of P\\'eclet numbers,\nincluding the delicate situation of local evanescent diffusion. To this end, an\nadaptive stabilization strategy based on the addition of jump-penalty terms is\nthen considered. A $\\theta$-upwind-based scheme is favored for the hyperbolic\nregion, and an inspired Scharfetter--Gummel-based technique is preferred for\nthe elliptic region. The well-posedness of the HDG method is also discussed by\nanalyzing the consistency and discrete coercivity properties. Extensive\nnumerical experiments are finally considered to verify the model's robustness\nfor all the abovementioned regimes.",
      "generated_abstract": "Title: Investigation into Hybridizable Discontinuous Galerkin Methods for Degenerate Advection-Diffusion-Reaction Issues\n\nNew Abstract: This study delves into the analysis of a group of primal high-order hybridizable discontinuous Galerkin (HDG) methodologies for addressing degenerate second-order elliptic challenges. A significant concern with this type of PDEs is their inherent mathematical complexity, which may exhibit non-uniformity throughout the domain. The local degeneracy in the diffusion term can create a purely hyperbolic subregion and an elliptic region elsewhere. The complexity of finding a solution is increased due to the discontinuity at the interfaces that separate the elliptic and hyperbolic sections. The HDG method proposed herein is structured in a unified and compact manner, capable of effectively managing pure diffusive or advective regimes and mixed regimes across a broad spectrum of P\u00e9clet numbers, inclusive of localized evanescent diffusion scenarios. An adjustable stabilization approach, incorporating jump-penalty terms, is therefore proposed. A \u03b8-upwind-based scheme is prioritized for the hyperbolic area, while a Scharfetter\u2013Gummel-inspired method is favoured for the elliptic area. The well-posedness of the HDG method is deliberated upon by scrutinizing its consistency and discrete coercivity characteristics. Extensive numerical testing is performed to affirm the model's reliability for all mentioned regimes.",
      "original_id": "oai:arXiv.org:2106.00226",
      "created": "2021-06-01",
      "categories": [
        "math.NA",
        "cs.NA",
        "math.AP"
      ]
    },
    {
      "title": "A Modified Dynamic Time Warping (MDTW) Approach and Innovative Average\n  Non-Self Match Distance (ANSD) Method for Anomaly Detection in ECG Recordings",
      "original_abstract": "ECGs objectively reflects the working conditions of the hearts as these\nsignals contain vast physiological and pathological information. In this work,\nin order to improve the efficiency and accuracy of \"best so far\" time series\nanalysis-based ECG anomaly detection methods, a novel method, comprising a\nmodified dynamic time warping (MDTW) and an innovative average non-self match\ndistance (ANSD) measure, is proposed for ECG anomaly detection. To evaluate the\nperformance of the proposed method, the proposed method is applied to real ECG\ndata selected from the MIT-BIH heartbeat database. To provide a reference for\ncomparison, two existing anomaly detection methods, namely, brute force discord\ndiscovery (BFDD) and adaptive window discord discovery (AWDD), are also applied\nto the same data. The experimental results show that our proposed method\noutperforms BFDD and AWD.",
      "generated_abstract": "Title: Enhanced Efficiency and Accuracy in ECG Anomaly Detection: A Novel Method Combining Modified Dynamic Time Warping (MDTW) and Average Non-Self Match Distance (ANSD)\n\nRevised Abstract: \nThe Electrocardiogram (ECG) is a vital tool in the evaluation of heart performance, providing a wealth of both physiological and pathological data. This study introduces a new method for the detection of anomalies in ECG records to augment the efficacy and precision of existing time series analysis-based detection techniques. This novel approach utilizes a modified version of dynamic time warping (MDTW) combined with an innovative measure of average non-self match distance (ANSD). The efficacy of the newly proposed method was assessed by applying it to real ECG data from the MIT-BIH heartbeat database. In order to benchmark the results, the same data were also subjected to two current anomaly detection techniques: brute force discord discovery (BFDD) and adaptive window discord discovery (AWDD). The empirical findings demonstrate that the newly developed approach surpasses the performance of both BFDD and AWDD in detecting anomalies.",
      "original_id": "oai:arXiv.org:2111.00803",
      "created": "2021-11-01",
      "categories": [
        "eess.SP"
      ]
    },
    {
      "title": "Enhanced Multigradient Dilution Preparation",
      "original_abstract": "Abstract: In our paper the new algorithm enhanced multi gradient Dilution\nPreparation (EMDP) is discussed. This new algorithm is reported with a lab on\nchip or digital Microfluidic biochip to operate multiple operation on a tiny\nchip. We can use Digital Microfluidic biochip to operate multiple operation on\na tiny chip. Samples are very costly which are used in any Biochemical\nlaboratory Protocols. For the case of fast and high throughput application, It\nis essential to minimize the cost of operations and the time of operations and\nthat is why one of the most challenging and important phase is sample\npreparation. In our proposed algorithm, we have hide to reduce sample droplets\nand waste droplets and for this purpose waste recycling is used, when different\nseries of multi gradient targets concentration factors (CFS) are generated. We\nhave compared our proposed algorithm with recent dilution techniques such as\nMTC, REMIA, and WARA. For the storage of intermediate droplets which, and\ngenerated during this process, on chip storage space 0(n) is needed. Key words:\nDigital microfluidic Biochip, Drug discovery, sample preparation, Electro\nwetting.",
      "generated_abstract": "Title: Advancements in Multigradient Dilution Preparation: An Enhanced Approach\n\nRevised Abstract: This study elaborates on a novel algorithm termed as Enhanced Multigradient Dilution Preparation (EMDP). This innovative algorithm is applied using a lab-on-chip or digital Microfluidic biochip, enabling the execution of manifold operations on a minuscule chip. The digital Microfluidic biochip offers an invaluable solution to perform multiple tasks on a compact chip. Given that the specimens used in any Biochemical lab protocols are highly expensive, it becomes crucial, especially for rapid and high throughput applications, to cut down the operational costs and time. Therefore, sample preparation emerges as a critical and challenging phase. Our proposed algorithm is designed to decrease the number of sample droplets and waste droplets, incorporating waste recycling when different sequences of multigradient target concentration factors (CFS) are produced. We evaluated our proposed algorithm against recent dilution methods like MTC, REMIA, and WARA. To store the intermediate droplets produced during this process, an on-chip storage space of 0(n) is required. Key words: Digital Microfluidic Biochip, Drug Discovery, Sample Preparation, Electrowetting.",
      "original_id": "oai:arXiv.org:2110.00232",
      "created": "2021-10-01",
      "categories": [
        "cs.ET",
        "cs.AR"
      ]
    },
    {
      "title": "Towards Utilitarian Combinatorial Assignment with Deep Neural Networks\n  and Heuristic Algorithms",
      "original_abstract": "This paper presents preliminary work on using deep neural networks to guide\ngeneral-purpose heuristic algorithms for performing utilitarian combinatorial\nassignment. In more detail, we use deep learning in an attempt to produce\nheuristics that can be used together with e.g., search algorithms to generate\nfeasible solutions of higher quality more quickly. Our results indicate that\nour approach could be a promising future method for constructing such\nheuristics.",
      "generated_abstract": "Title: Progressing Towards Functional Combinatorial Assignments via Deep Neural Networks and Heuristic Algorithms\n\nRevised Abstract: This study introduces initial investigations into the application of deep neural networks in steering universal heuristic algorithms for executing utilitarian combinatorial assignment. Specifically, we employ deep learning with the aim of developing heuristics that can complement search algorithms to rapidly produce superior quality feasible solutions. Our findings suggest that this method holds potential to be a future-effective strategy for the creation of such heuristics.",
      "original_id": "oai:arXiv.org:2107.00317",
      "created": "2021-07-01",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Embarras de richesses in non-DLVO colloid interactions",
      "original_abstract": "In its original formulation, the seminal Deryaguin-Landau-Verwey-Overbeek\n(DLVO) theory of colloidal stability seemed like a simple but realistic\ndescription of the world of colloid interactions in electrolyte solutions. It\nis based on a straightforward superposition of the mean-field Poisson-Boltzmann\n(PB) electrostatics with the electrodynamic van der Waals (vdW) interactions\ndriven by thermal and quantum fluctuations. However, subsequent developments\ncontinued to reveal a much richer and deeper structure of fundamental\ninteractions on the nano- and micro-scale: the granularity and structure of the\nsolvent, charging equilibria of dissociable charge groups, inhomogeneous charge\ndistributions, the finite size of the ions, non-mean-field electrostatics,\nion-ion correlations, and more. Today, the original simplicity is gone and we\nare left with an embarrassingly rich variety of interactions that defy simple\nclassification and reduction to a few fundamental mechanisms. In this\nmini-review, we comment on the contemporary state-of-the-art picture of\ncolloidal interactions, in view of some recent progress in experiments.",
      "generated_abstract": "Title: The Complexity of Non-DLVO Colloid Interactions: A Profusion of Wealth\n\nRevised Abstract: Initially, the Deryaguin-Landau-Verwey-Overbeek (DLVO) theory provided a seemingly straightforward and realistic interpretation of colloidal stability in electrolyte solutions, integrating the Poisson-Boltzmann (PB) mean-field electrostatics and thermally and quantum fluctuation-driven van der Waals (vdW) interactions. However, continuing advancements have uncovered an increasingly intricate and profound framework of fundamental interactions at the nano and micro scales. These complexities encompass the granularity and structure of the solvent, charge equilibria of dissociable charge groups, uneven charge distributions, ion size considerations, non-mean-field electrostatics, and ion-ion correlations among others. The initial simplicity of the theory has been replaced by an overwhelmingly abundant array of interactions that resist simplistic categorization and reduction to basic mechanisms. This condensed review provides commentary on the current advanced understanding of colloidal interactions, taking into account recent experimental advancements.",
      "original_id": "oai:arXiv.org:2101.00187",
      "created": "2021-01-01",
      "categories": [
        "cond-mat.soft",
        "cond-mat.stat-mech",
        "physics.chem-ph"
      ]
    },
    {
      "title": "Stellar and accretion disk parameters of the close binary HD 50526",
      "original_abstract": "We present a photometric and spectroscopic study of HD 50526, an ellipsoidal\nbinary member of the group Double Periodic Variable stars. Performing\ndata-mining in photometric surveys and conducting new spectroscopic\nobservations with several spectrographs during 2008 to 2015, we obtained\norbital and stellar parameters of the system. The radial velocities were\nanalyzed with the genetic PIKAIA algorithm, whereas Doppler tomography maps for\nthe H$\\alpha$ and H$\\beta$ lines were constructed with the Total Variation\nMinimization code. An optimized simplex-algorithm was used to solve the\ninverse-problem adjusting the light curve with the best stellar parameters for\nthe system. We find an orbital period of $6.701 \\pm 0.001 ~\\mathrm{d}$ and a\nlong photometric cycle of $191 \\pm 2 ~\\mathrm{d}$. We detected the spectral\nfeatures of the coldest star, and modeled it with a $\\log{g} = 2.79 \\pm 0.02\n~\\mathrm{dex}$ giant of mass $1.13 \\pm 0.02 ~\\mathrm{M_{\\odot}}$ and effective\ntemperature $10500 \\pm 125 ~\\mathrm{K}$. In addition, we determine a mass ratio\n$q= 0.206 \\pm 0.033$ and that the hot star is a B-type dwarf of mass $5.48 \\pm\n0.02 ~\\mathrm{M_{\\odot}}$. The $V$-band orbital light curve can be modeled\nincluding the presence of an accretion disk around the hotter star. This fills\nthe Roche lobe of the hotter star, and has a radius $14.74 \\pm 0.02\n~\\mathrm{R_{\\odot}}$ and temperature at the outer edge $9400 ~\\mathrm{K}$. Two\nbright spots located in the disk account for the global morphology of the light\ncurve. The Doppler tomography maps of H$\\alpha$ and H$\\beta$, reveal complex\nstructures of mass fluxes in the system.",
      "generated_abstract": "Title: Analysis of Stellar and Accretion Disk Characteristics in the Close Binary HD 50526\nGenerated Abstract: This paper details a comprehensive photometric and spectroscopic exploration of HD 50526, a binary star system exhibiting ellipsoidal variations, grouped under Double Periodic Variable stars. Through meticulous data extraction from photometric surveys and fresh spectroscopic observations gathered from multiple spectrographs from 2008 to 2015, we were able to ascertain both the orbital and stellar properties of the system. The PIKAIA genetic algorithm was employed to scrutinize radial velocities, while the Doppler tomography maps for the H\u03b1 and H\u03b2 lines were generated with the help of the Total Variation Minimization code. To resolve the inverse problem, we utilized an optimized simplex algorithm, aligning the light curve with the optimal stellar parameters of the system. Our findings indicated an orbital period of $6.701 \\pm 0.001 ~\\mathrm{d}$ and a protracted photometric cycle of $191 \\pm 2 ~\\mathrm{d}$. The spectral features of the cooler star were identified and modeled as a giant with a $\\log{g} = 2.79 \\pm 0.02 ~\\mathrm{dex}$, mass of $1.13 \\pm 0.02 ~\\mathrm{M_{\\odot}}$ and an effective temperature of $10500 \\pm 125 ~\\mathrm{K}$. Moreover, we established a mass ratio of $q= 0.206 \\pm 0.033$ and characterized the hotter star as a B-type dwarf with a mass of $5.48 \\pm 0.02 ~\\mathrm{M_{\\odot}}$. The $V$-band orbital light curve was successfully modeled considering the existence of an accretion disk around the hotter star, occupying the Roche lobe of the star, with a radius of $14.74 \\pm 0.02 ~\\mathrm{R_{\\odot}}$ and an outer edge temperature of $9400 ~\\mathrm{K}$. The overall light curve morphology could be explained by two luminous spots located on the disk. The Doppler tomography maps of H\u03b1 and H\u03b2 disclose intricate mass flux structures within the system.",
      "original_id": "oai:arXiv.org:2109.00231",
      "created": "2021-09-01",
      "categories": [
        "astro-ph.SR"
      ]
    },
    {
      "title": "Industry Practice of Coverage-Guided Enterprise-Level DBMS Fuzzing",
      "original_abstract": "As an infrastructure for data persistence and analysis, Database Management\nSystems (DBMSs) are the cornerstones of modern enterprise software. To improve\ntheir correctness, the industry has been applying blackbox fuzzing for decades.\nRecently, the research community achieved impressive fuzzing gains using\ncoverage guidance. However, due to the complexity and distributed nature of\nenterprise-level DBMSs, seldom are these researches applied to the industry.\n  In this paper, we apply coverage-guided fuzzing to enterprise-level DBMSs\nfrom Huawei and Bloomberg LP. In our practice of testing GaussDB and Comdb2, we\nfound major challenges in all three testing stages. The challenges are\ncollecting precise coverage, optimizing fuzzing performance, and analyzing root\ncauses. In search of a general method to overcome these challenges, we propose\nRatel, a coverage-guided fuzzer for enterprise-level DBMSs. With its\nindustry-oriented design, Ratel improves the feedback precision, enhances the\nrobustness of input generation, and performs an on-line investigation on the\nroot cause of bugs. As a result, Ratel outperformed other fuzzers in terms of\ncoverage and bugs. Compared to industrial black box fuzzers SQLsmith and\nSQLancer, as well as coverage-guided academic fuzzer Squirrel, Ratel covered\n38.38%, 106.14%, 583.05% more basic blocks than the best results of other three\nfuzzers in GaussDB, PostgreSQL, and Comdb2, respectively. More importantly,\nRatel has discovered 32, 42, and 5 unknown bugs in GaussDB, Comdb2, and\nPostgreSQL.",
      "generated_abstract": "Title: Implementation and Evaluation of Ratel: A Coverage-Guided Fuzzer for Enterprise-Level DBMSs\n\nRevised Abstract: Database Management Systems (DBMSs) form the backbone of contemporary enterprise software, functioning as platforms for data retention and scrutiny. To enhance their accuracy, blackbox fuzzing has been a long-established industrial practice. Recently, impressive advancements in fuzzing have been made through coverage guidance by the research fraternity. Nevertheless, the intricate and fragmented nature of enterprise-level DBMSs often precludes the application of these research findings in the industry. \nThis study explores the application of coverage-guided fuzzing to enterprise-level DBMSs, specifically GaussDB and Comdb2 utilised by Huawei and Bloomberg LP. The investigation encountered significant hurdles in the three crucial stages of testing: gathering accurate coverage, boosting fuzzing efficiency, and root cause analysis. To address these issues, the paper introduces Ratel, a novel coverage-guided fuzzer specifically designed for enterprise-level DBMSs. Ratel improves feedback accuracy, strengthens the generation of robust input, and conducts real-time investigations to identify bug root causes, thereby outperforming other fuzzers in terms of coverage and bug identification. When compared to industry standard black box fuzzers such as SQLsmith and SQLancer, and the coverage-guided academic fuzzer Squirrel, Ratel demonstrated superior performance, covering 38.38%, 106.14%, 583.05% more basic blocks in GaussDB, PostgreSQL, and Comdb2 respectively. Moreover, Ratel was successful in identifying 32, 42, and 5 previously undiscovered bugs in GaussDB, Comdb2, and PostgreSQL correspondingly.",
      "original_id": "oai:arXiv.org:2103.00804",
      "created": "2021-03-01",
      "categories": [
        "cs.SE"
      ]
    },
    {
      "title": "Multiple solutions for a class of quasilinear problems with double\n  criticality",
      "original_abstract": "We establish multiplicity results for the following class of quasilinear\nproblems $$ \\left\\{ \\begin{array}{l} -\\Delta_{\\Phi}u=f(x,u) \\quad \\mbox{in}\n\\quad \\Omega, \\\\ u=0 \\quad \\mbox{on} \\quad \\partial \\Omega, \\end{array} \\right.\n\\leqno{(P)} $$ where $\\Delta_{\\Phi}u=\\text{div}(\\varphi(x,|\\nabla u|)\\nabla u)$\nfor a generalized N-function $\\Phi(x,t)=\\int_{0}^{|t|}\\varphi(x,s)s\\,ds$. We\nconsider $\\Omega\\subset\\mathbb{R}^N$ to be a smooth bounded domain that\ncontains two disjoint open regions $\\Omega_N$ and $\\Omega_p$ such that\n$\\overline{\\Omega_N}\\cap\\overline{\\Omega_p}=\\emptyset$. The main feature of the\nproblem $(P)$ is that the operator $-\\Delta_{\\Phi}$ behaves like $-\\Delta_N$ on\n$\\Omega_N$ and $-\\Delta_p$ on $\\Omega_p$. We assume the nonlinearity\n$f:\\Omega\\times\\mathbb{R}\\to\\mathbb{R}$ of two different types, but both\nbehaves like $e^{\\alpha|t|^\\frac{N}{N-1}}$ on $\\Omega_N$ and $|t|^{p^*-2}t$ on\n$\\Omega_p$ as $|t|$ is large enough, for some $\\alpha>0$ and\n$p^*=\\frac{Np}{N-p}$ being the critical Sobolev exponent for $1<p<N$. In this\ncontext, for one type of nonlinearity $f$, we provide multiplicity of solutions\nin a general smooth bounded domain and for another type of nonlinearity $f$, in\nan annular domain $\\Omega$, we establish existence of multiple solutions for\nthe problem $(P)$ that are nonradial and rotationally nonequivalent.",
      "generated_abstract": "Title: Numerous Solutions for a Category of Quasilinear Problems with Double Criticality\n\nRevised Abstract: This study demonstrates the existence of multiple solutions for a given set of quasilinear problems, represented as $$ \\left\\{ \\begin{array}{l} -\\Delta_{\\Phi}u=f(x,u) \\quad \\mbox{in}\n\\quad \\Omega, \\\\ u=0 \\quad \\mbox{on} \\quad \\partial \\Omega, \\end{array} \\right.\n\\leqno{(P)} $$ where $\\Delta_{\\Phi}u=\\text{div}(\\varphi(x,|\\nabla u|)\\nabla u)$\nis derived from an extended N-function $\\Phi(x,t)=\\int_{0}^{|t|}\\varphi(x,s)s\\,ds$. Our focus is on $\\Omega\\subset\\mathbb{R}^N$, a smooth bounded domain inclusive of two separate open regions $\\Omega_N$ and $\\Omega_p$ such that\n$\\overline{\\Omega_N}\\cap\\overline{\\Omega_p}=\\emptyset$. A significant characteristic of the problem $(P)$ is the operator $-\\Delta_{\\Phi}$'s behavior, which resembles $-\\Delta_N$ on\n$\\Omega_N$ and $-\\Delta_p$ on $\\Omega_p$. The nonlinearity\n$f:\\Omega\\times\\mathbb{R}\\to\\mathbb{R}$ is contemplated to be of two distinct types, both behaving as $e^{\\alpha|t|^\\frac{N}{N-1}}$ on $\\Omega_N$ and $|t|^{p^*-2}t$ on\n$\\Omega_p$ when $|t|$ is sufficiently large, given $\\alpha>0$ and\n$p^*=\\frac{Np}{N-p}$ as the critical Sobolev exponent for $1<p<N$. In this setting, we identify multiple solutions within a general smooth bounded domain for one type of nonlinearity $f$, and for another type of nonlinearity $f$, within an annular domain $\\Omega$, we confirm the existence of multiple, non-radial, and rotationally nonequivalent solutions for the problem $(P)$.",
      "original_id": "oai:arXiv.org:2107.00331",
      "created": "2021-07-01",
      "categories": [
        "math.AP"
      ]
    },
    {
      "title": "Achromatic photonic tricouplers for application in nulling\n  interferometry",
      "original_abstract": "Integrated-optic components are being increasingly used in astrophysics,\nmainly where accuracy and precision are paramount. One such emerging technology\nis nulling interferometry that targets high contrast and high angular\nresolution. Two of the most critical limitations encountered by nullers are\nrapid phase fluctuations in the incoming light causing instability in the\ninterference and chromaticity of the directional couplers that prevent a deep\nbroadband interferometric null. We explore the use of a tricoupler designed by\nultrafast laser inscription that solves both issues. Simulations of a\ntricoupler, incorporated into a nuller, result in order of a magnitude\nimprovement in null depth.",
      "generated_abstract": "Title: Utilization of Achromatic Photonic Tricouplers in Nulling Interferometry\n\nRewritten Abstract: The use of integrated-optic components in the field of astrophysics has been on the rise, largely due to their enhanced accuracy and precision. A notable technology in this domain is nulling interferometry, which aims to achieve high contrast and superior angular resolution. However, nullers often face two major challenges: rapid phase fluctuations in incoming light that lead to interference instability, and the directional couplers' chromaticity that hinders deep broadband interferometric null. This study investigates the potential of an ultrafast laser-inscribed tricoupler to address these issues. Incorporating a tricoupler into a nuller, according to our simulations, results in a significant improvement in null depth by an order of magnitude.",
      "original_id": "oai:arXiv.org:2106.00251",
      "created": "2021-06-01",
      "categories": [
        "astro-ph.IM"
      ]
    },
    {
      "title": "NeuTex: Neural Texture Mapping for Volumetric Neural Rendering",
      "original_abstract": "Recent work has demonstrated that volumetric scene representations combined\nwith differentiable volume rendering can enable photo-realistic rendering for\nchallenging scenes that mesh reconstruction fails on. However, these methods\nentangle geometry and appearance in a \"black-box\" volume that cannot be edited.\nInstead, we present an approach that explicitly disentangles\ngeometry--represented as a continuous 3D volume--from appearance--represented\nas a continuous 2D texture map. We achieve this by introducing a 3D-to-2D\ntexture mapping (or surface parameterization) network into volumetric\nrepresentations. We constrain this texture mapping network using an additional\n2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D\nsurface points map to 2D texture points that map back to the original 3D\npoints. We demonstrate that this representation can be reconstructed using only\nmulti-view image supervision and generates high-quality rendering results. More\nimportantly, by separating geometry and texture, we allow users to edit\nappearance by simply editing 2D texture maps.",
      "generated_abstract": "Title: NeuTex: The Application of Neural Texture Mapping in Volumetric Neural Rendering\n\nRevised Abstract: Prior research has displayed how the amalgamation of volumetric scene representations and differentiable volume rendering can facilitate the photo-realistic rendering in complex scenes where mesh reconstruction becomes ineffective. Nevertheless, these techniques intertwine geometry and appearance within an uneditable \"black-box\" volume. Contrarily, our research introduces a method that distinctly separates geometry, depicted as a continuous 3D volume, and appearance, illustrated as a continuous 2D texture map. This is accomplished by incorporating a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We further regulate this texture mapping network with an extra 2D-to-3D inverse mapping network and a novel cycle consistency loss to ensure that 3D surface points correspond to 2D texture points that map back to the originating 3D points. Our findings show that this representation can be reconstructed utilizing solely multi-view image supervision, generating high-caliber rendering outcomes. Crucially, by differentiating geometry and texture, we enable users to modify appearance by merely editing 2D texture maps.",
      "original_id": "oai:arXiv.org:2103.00762",
      "created": "2021-03-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "On the application of matrix congruence to QUBO formulations for systems\n  of linear equations",
      "original_abstract": "Recent studies on quantum computing algorithms focus on excavating features\nof quantum computers which have potential for contributing to computational\nmodel enhancements. Among various approaches, quantum annealing methods\neffectively parallelize quadratic unconstrained binary optimization (QUBO)\nformulations of systems of linear equations. In this paper, we simplify these\nformulations by exploiting congruence of real symmetric matrices to diagonal\nmatrices. We further exhibit computational merits of the proposed QUBO models,\nwhich can outperform classical algorithms such as QR and SVD decomposition.",
      "generated_abstract": "Title: Utilizing Matrix Congruence in QUBO Formulations for Linear Equation Systems\n\nNew Abstract: Contemporary research in quantum computing algorithms is centered around the exploration of characteristics inherent to quantum computers that hold promise in augmenting computational models. Among a plethora of methods, the use of quantum annealing techniques to concurrently process quadratic unconstrained binary optimization (QUBO) formulations for systems of linear equations has been found to be particularly effective. This study aims to streamline these formulations through the application of congruence in real symmetric matrices to diagonal matrices. Further, it elucidates the computational advantages of the suggested QUBO models, demonstrating their ability to surpass traditional algorithms like QR and SVD decomposition.",
      "original_id": "oai:arXiv.org:2111.00747",
      "created": "2021-11-01",
      "categories": [
        "quant-ph"
      ]
    },
    {
      "title": "Discriminating Quantum States with Quantum Machine Learning",
      "original_abstract": "Quantum machine learning (QML) algorithms have obtained great relevance in\nthe machine learning (ML) field due to the promise of quantum speedups when\nperforming basic linear algebra subroutines (BLAS), a fundamental element in\nmost ML algorithms. By making use of BLAS operations, we propose, implement and\nanalyze a quantum k-means (qk-means) algorithm with a low time complexity of\n$\\mathcal{O}(NKlog(D)I/C)$ to apply it to the fundamental problem of\ndiscriminating quantum states at readout. Discriminating quantum states allows\nthe identification of quantum states $|0\\rangle$ and $|1\\rangle$ from low-level\nin-phase and quadrature signal (IQ) data, and can be done using custom ML\nmodels. In order to reduce dependency on a classical computer, we use the\nqk-means to perform state discrimination on the IBMQ Bogota device and managed\nto find assignment fidelities of up to 98.7% that were only marginally lower\nthan that of the k-means algorithm. Inspection of assignment fidelity scores\nresulting from applying both algorithms to a combination of quantum states\nshowed concordance to our correlation analysis using Pearson Correlation\ncoefficients, where evidence shows cross-talk in the (1, 2) and (2, 3)\nneighboring qubit couples for the analyzed device.",
      "generated_abstract": "Title: Differentiating Quantum States Utilizing Quantum Machine Learning Techniques\n\nRevised Abstract: The importance of Quantum machine learning (QML) algorithms has significantly grown in the machine learning (ML) community, primarily due to the potential for superior speed in handling basic linear algebra subroutines (BLAS), which are integral to most ML approaches. Utilizing BLAS operations, we develop, execute, and scrutinize a quantum k-means (qk-means) algorithm, featuring a reduced time complexity of $\\mathcal{O}(NKlog(D)I/C)$, to address the crucial problem of differentiating quantum states at readout. This discrimination allows for the recognition of quantum states $|0\\rangle$ and $|1\\rangle$ from base-level in-phase and quadrature signal (IQ) data, achievable through tailor-made ML models. To decrease reliance on classical computers, we employ the qk-means for state differentiation on the IBMQ Bogota device, achieving assignment fidelities as high as 98.7%, only slightly lower than the k-means algorithm. A thorough examination of assignment fidelity rates obtained from applying both algorithms to a mix of quantum states aligns with our correlation study using Pearson Correlation coefficients, indicating evidence of cross-talk in the (1, 2) and (2, 3) adjacent qubit pairs for the device in question.",
      "original_id": "oai:arXiv.org:2112.00313",
      "created": "2021-12-01",
      "categories": [
        "quant-ph",
        "cs.LG"
      ]
    },
    {
      "title": "Segmentation of Breast Microcalcifications: A Multi-Scale Approach",
      "original_abstract": "Accurate characterization of microcalcifications (MCs) in 2D full-field\ndigital screening mammography is a necessary step towards reducing diagnostic\nuncertainty associated with the callback of women with suspicious MCs.\nQuantitative analysis of MCs has the potential to better identify MCs that have\na higher likelihood of corresponding to invasive cancer. However, automated\nidentification and segmentation of MCs remains a challenging task with high\nfalse positive rates. We present Hessian Difference of Gaussians Regression\n(HDoGReg), a two stage multi-scale approach to MC segmentation. Candidate high\noptical density objects are first delineated using blob detection and Hessian\nanalysis. A regression convolutional network, trained to output a function with\nhigher response near MCs, chooses the objects which constitute actual MCs. The\nmethod is trained and validated on 435 mammograms from two separate datasets.\nHDoGReg achieved a mean intersection over the union of 0.670$\\pm$0.121 per\nimage, intersection over the union per MC object of 0.607$\\pm$0.250 and true\npositive rate of 0.744 at 0.4 false positive detections per $cm^2$. The results\nof HDoGReg perform better when compared to state-of-the-art MC segmentation and\ndetection methods.",
      "generated_abstract": "Title: A Multi-Scale Solution to Segmenting Breast Microcalcifications\n\nRevised Abstract: The precise categorization of microcalcifications (MCs) in 2D full-field digital mammography is a crucial initiative in minimizing diagnostic uncertainty that arises from revisiting women with questionable MCs. The potential lies in a quantitative examination of MCs to more accurately pinpoint those that are more likely to be indicative of invasive cancer. Despite this, the autonomous recognition and segmentation of MCs still pose significant challenges, with a high prevalence of false positives. In this research, we introduce the Hessian Difference of Gaussians Regression (HDoGReg), a multi-scale, dual-stage strategy for MC segmentation. The initial stage involves the delineation of prospective high optical density entities using blob detection and Hessian analysis. A regression convolutional network, which is trained to generate a function with an increased response close to MCs, then selects the genuine MCs. The methodology was trained and validated using 435 mammograms from two distinct datasets. HDoGReg yielded a mean intersection over the union of 0.670$\\pm$0.121 per image, an intersection over the union per MC object of 0.607$\\pm$0.250, and a true positive rate of 0.744 at 0.4 false positive detections per $cm^2$. When juxtaposed with existing MC segmentation and detection techniques, HDoGReg showed superior performance.",
      "original_id": "oai:arXiv.org:2102.00754",
      "created": "2021-02-01",
      "categories": [
        "eess.IV"
      ]
    },
    {
      "title": "Early Prediction of Heart Disease Using PCA and Hybrid Genetic Algorithm\n  with k-Means",
      "original_abstract": "Worldwide research shows that millions of lives lost per year because of\nheart disease. The healthcare sector produces massive volumes of data on heart\ndisease that are sadly not used to locate secret knowledge for successful\ndecision making. One of the most important aspects at this moment is detecting\nheart disease at an early stage. Researchers have applied distinct techniques\nto the UCI Machine Learning heart disease dataset. Many researchers have tried\nto apply some complex techniques to this dataset, where detailed studies are\nstill missing. In this paper, Principal Component Analysis (PCA) has been used\nto reduce attributes. Apart from a Hybrid genetic algorithm (HGA) with k-means\nused for final clustering. Typically, the k-means method is using for\nclustering the data. This type of clustering can get stuck in the local optima\nbecause this method is heuristic. We used the Hybrid Genetic Algorithm (HGA)\nfor data clustering to avoid this problem. Our proposed method can predict\nearly heart disease with an accuracy of 94.06%.",
      "generated_abstract": "Title: Proactive Identification of Heart Disease Leveraging PCA and Hybrid Genetic Algorithm with k-Means\n\nRevised Abstract: Global studies reveal that heart disease is a leading cause of mortality, accounting for millions of deaths annually. Despite the vast amount of data generated by the healthcare industry on heart disease, its potential to unveil critical insights for effective decision-making remains largely untapped. A crucial area of focus is the early detection of heart disease. Various methodologies have been applied to the heart disease dataset from the UCI Machine Learning repository, with many researchers employing intricate techniques that have not been extensively explored. This paper utilizes Principal Component Analysis (PCA) to streamline attribute selection, alongside a Hybrid Genetic Algorithm (HGA) combined with k-means for ultimate clustering. Conventionally, k-means is employed for data clustering; however, its heuristic nature often leads to local optima, a limitation overcome by using the HGA. The proposed method demonstrates the potential to predict heart disease at an early stage with a high accuracy of 94.06%.",
      "original_id": "oai:arXiv.org:2101.00183",
      "created": "2021-01-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Deep Measurement Updates for Bayes Filters",
      "original_abstract": "Measurement update rules for Bayes filters often contain hand-crafted\nheuristics to compute observation probabilities for high-dimensional sensor\ndata, like images. In this work, we propose the novel approach Deep Measurement\nUpdate (DMU) as a general update rule for a wide range of systems. DMU has a\nconditional encoder-decoder neural network structure to process depth images as\nraw inputs. Even though the network is trained only on synthetic data, the\nmodel shows good performance at evaluation time on real-world data. With our\nproposed training scheme primed data training , we demonstrate how the DMU\nmodels can be trained efficiently to be sensitive to condition variables\nwithout having to rely on a stochastic information bottleneck. We validate the\nproposed methods in multiple scenarios of increasing complexity, beginning with\nthe pose estimation of a single object to the joint estimation of the pose and\nthe internal state of an articulated system. Moreover, we provide a benchmark\nagainst Articulated Signed Distance Functions(A-SDF) on the RBO dataset as a\nbaseline comparison for articulation state estimation.",
      "generated_abstract": "Title: Advanced Measurement Update Mechanisms for Bayesian Filters\n\nRevised Abstract: Traditional measurement update practices for Bayes filters often employ man-made heuristics to calculate observation probabilities pertinent to high-dimensional sensor data, such as images. This study introduces a ground-breaking approach known as the Deep Measurement Update (DMU), which serves as a universal update rule for a broad spectrum of systems. The DMU utilizes a conditional encoder-decoder neural network format to handle depth images in their raw form. Despite the network being trained exclusively on synthetic data, it exhibits commendable performance when evaluated on real-world data. With the implementation of our newly proposed training technique, primed data training, we illustrate how DMU models can be trained effectively to respond to condition variables without the necessity of a stochastic information bottleneck. We authenticate our proposed techniques across several scenarios of growing complexity, commencing with the pose estimation of a lone object and progressing to the combined estimation of the pose and the internal status of an interconnected system. Furthermore, we establish a benchmark for articulation state estimation against Articulated Signed Distance Functions (A-SDF) on the RBO dataset, providing a base for comparison.",
      "original_id": "oai:arXiv.org:2112.00380",
      "created": "2021-12-01",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Designing nudge agents that promote human altruism",
      "original_abstract": "Previous studies have found that nudging is key to promoting altruism in\nhuman-human interaction. However, in social robotics, there is still a lack of\nstudy on confirming the effect of nudging on altruism. In this paper, we apply\ntwo nudge mechanisms, peak-end and multiple viewpoints, to a video stimulus\nperformed by social robots (virtual agents) to see whether a subtle change in\nthe stimulus can promote human altruism. An experiment was conducted online\nthrough crowdsourcing with 136 participants. The result shows that the\nparticipants who watched the peak part set at the end of the video performed\nbetter at the Dictator game, which means that the nudge mechanism of the\npeak-end effect actually promoted human altruism.",
      "generated_abstract": "Title: Development of nudge agents to enhance human altruistic behavior\n\nRevised Abstract: Prior research has established the significant role of nudging in fostering altruistic behavior in human-to-human interactions. Nevertheless, the domain of social robotics has not yet thoroughly investigated the influence of nudging on altruism. In the present study, we incorporate two nudge techniques, namely peak-end and multiple viewpoints, into a video demonstration featuring social robots (virtual agents). The primary aim is to ascertain if minute modifications in the stimulus can boost human altruism. An online experiment was implemented via crowdsourcing, engaging 136 participants. Outcomes illustrate that participants exposed to the peak segment at the video's conclusion performed notably well in the Dictator game. This indicates that the peak-end nudge technique effectively enhances human altruism.",
      "original_id": "oai:arXiv.org:2110.00319",
      "created": "2021-10-01",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "title": "Neural Free-Viewpoint Performance Rendering under Complex Human-object\n  Interactions",
      "original_abstract": "4D reconstruction of human-object interaction is critical for immersive VR/AR\nexperience and human activity understanding. Recent advances still fail to\nrecover fine geometry and texture results from sparse RGB inputs, especially\nunder challenging human-object interactions scenarios. In this paper, we\npropose a neural human performance capture and rendering system to generate\nboth high-quality geometry and photo-realistic texture of both human and\nobjects under challenging interaction scenarios in arbitrary novel views, from\nonly sparse RGB streams. To deal with complex occlusions raised by human-object\ninteractions, we adopt a layer-wise scene decoupling strategy and perform\nvolumetric reconstruction and neural rendering of the human and object.\nSpecifically, for geometry reconstruction, we propose an interaction-aware\nhuman-object capture scheme that jointly considers the human reconstruction and\nobject reconstruction with their correlations. Occlusion-aware human\nreconstruction and robust human-aware object tracking are proposed for\nconsistent 4D human-object dynamic reconstruction. For neural texture\nrendering, we propose a layer-wise human-object rendering scheme, which\ncombines direction-aware neural blending weight learning and spatial-temporal\ntexture completion to provide high-resolution and photo-realistic texture\nresults in the occluded scenarios. Extensive experiments demonstrate the\neffectiveness of our approach to achieve high-quality geometry and texture\nreconstruction in free viewpoints for challenging human-object interactions.",
      "generated_abstract": "Title: Advanced Neural Performance Rendering under Complex Human-Object Interactions\n\nRevised Abstract: The 4D reconstruction of human-object interactions is essential for understanding human activity and enhancing virtual and augmented reality experiences. However, current technological breakthroughs struggle to accurately reconstruct fine geometry and texture from sparse RGB data, particularly during complex human-object interactions. This research introduces a neural system for capturing and rendering human performance, capable of generating superior geometry and photo-realistic textures for both humans and objects in arbitrary novel views, using only sparse RGB streams. We use a layer-wise scene decoupling strategy to address the complicated occlusions caused by human-object interactions, which allows us to carry out a volumetric reconstruction and neural rendering of humans and objects. Specifically, we develop an interaction-aware human-object capture method that simultaneously accounts for human and object reconstruction, taking their correlations into consideration. We propose occlusion-aware human reconstruction and robust human-aware object tracking for consistent 4D dynamic human-object reconstruction. For neural texture rendering, we introduce a layer-wise human-object rendering method, which merges direction-aware neural blending weight learning with spatial-temporal texture completion to deliver high-resolution, photo-realistic texture results in occluded scenarios. Our extensive experimental results validate the effectiveness of our method in achieving superior geometry and texture reconstruction from free viewpoints during complex human-object interactions.",
      "original_id": "oai:arXiv.org:2108.00362",
      "created": "2021-08-01",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "Mean-Field Game-Theoretic Edge Caching",
      "original_abstract": "In this book chapter, we study a problem of distributed content caching in an\nultra-dense edge caching network (UDCN), in which a large number of small base\nstations (SBSs) prefetch popular files to cope with the ever-growing user\ndemand in 5G and beyond. In a UDCN, even a small misprediction of user demand\nmay render a large amount of prefetched data obsolete. Furtherproacmore, the\ninterference variance is high due to the short inter-SBS distances, making it\ndifficult to quantify data downloading rates. Lastly, since the caching\ndecision of each SBS interacts with those of all other SBSs, the problem\ncomplexity of exponentially increases with the number of SBSs, which is unfit\nfor UDCNs. To resolve such challenging issues while reflecting time-varying and\nlocation-dependent user demand, we leverage mean-field game (MFG) theory\nthrough which each SBS interacts only with a single virtual SBS whose state is\ndrawn from the state distribution of the entire SBS population, i.e.,\nmean-field (MF) distribution. This MF approximation asymptotically guarantees\nachieving the epsilon Nash equilibrium as the number of SBSs approaches\ninfinity. To describe such an MFG-theoretic caching framework, this chapter\naims to provide a brief review of MFG, and demonstrate its effectiveness for\nUDCNs.",
      "generated_abstract": "Title: Application of Mean-Field Game Theory in Edge Caching Networks\n\nRewritten Abstract: This particular chapter of the book delves into the issue of decentralized content caching within an ultra-dense edge caching network (UDCN). This network is characterized by a multitude of small base stations (SBSs) that prefetch trending files, aiming to meet the increasing user demand in next-generation networks, such as 5G and beyond. However, in a UDCN, even a slight miscalculation in predicting user demand can lead to a substantial amount of prefetched data becoming redundant. Adding to the complexity, the interference variance is considerable due to the proximity of the SBSs, complicating the task of measuring data download rates. Moreover, the caching decision of each SBS is inextricably linked with the decisions of all other SBSs, exponentially escalating the problem complexity, making it unsuitable for UDCNs. To address these complexities while accommodating fluctuating and geographically-dependent user demand, we employ the mean-field game (MFG) theory. This theory allows each SBS to interact solely with a theoretical SBS, whose state is derived from the overall state distribution of all SBSs, otherwise known as the mean-field (MF) distribution. This MF approximation provides an asymptotic guarantee of reaching the epsilon Nash equilibrium as the SBS count tends towards infinity. This chapter aims to provide a succinct overview of MFG and its practical application in dealing with the challenges posed by UDCNs.",
      "original_id": "oai:arXiv.org:2101.00341",
      "created": "2021-01-01",
      "categories": [
        "cs.IT",
        "cs.NI",
        "math.IT"
      ]
    },
    {
      "title": "Energy-constrained Self-training for Unsupervised Domain Adaptation",
      "original_abstract": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a\nlabeled source domain distribution to perform well on an unlabeled target\ndomain. Recently, the deep self-training involves an iterative process of\npredicting on the target domain and then taking the confident predictions as\nhard pseudo-labels for retraining. However, the pseudo-labels are usually\nunreliable, and easily leading to deviated solutions with propagated errors. In\nthis paper, we resort to the energy-based model and constrain the training of\nthe unlabeled target sample with the energy function minimization objective. It\ncan be applied as a simple additional regularization. In this framework, it is\npossible to gain the benefits of the energy-based model, while retaining strong\ndiscriminative performance following a plug-and-play fashion. We deliver\nextensive experiments on the most popular and large scale UDA benchmarks of\nimage classification as well as semantic segmentation to demonstrate its\ngenerality and effectiveness.",
      "generated_abstract": "Title: Implementing Energy-based Constraints in Self-training for Unsupervised Domain Adaptation\n\nRevised Abstract: The primary objective of Unsupervised Domain Adaptation (UDA) is to effectively apply the insights gained from a labelled source domain to enhance performance on a target domain lacking labels. Recently, deep self-training, involving a cyclical process of prediction and retraining on the target domain using confident predictions as rigid pseudo-labels, has been utilized. Nonetheless, the pseudo-labels often prove unreliable, resulting in deviated outcomes and error propagation. This study proposes the use of an energy-based model to restrict the training of the unlabeled target sample via an energy function minimization objective, serving as an uncomplicated additional regularization. Employing this model allows for the exploitation of the benefits of an energy-based framework while ensuring robust discriminative performance in a user-friendly manner. Comprehensive experimentation on widely recognized, large-scale UDA benchmarks for image classification and semantic segmentation confirms the broad applicability and efficacy of this approach.",
      "original_id": "oai:arXiv.org:2101.00316",
      "created": "2021-01-01",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "Unsupervised Discovery of Unaccusative and Unergative Verbs",
      "original_abstract": "We present an unsupervised method to detect English unergative and\nunaccusative verbs. These categories allow us to identify verbs participating\nin the causative-inchoative alternation without knowing the semantic roles of\nthe verb. The method is based on the generation of intransitive sentence\nvariants of candidate verbs and probing a language model. We obtained results\non par with similar approaches, with the added benefit of not relying on\nannotated resources.",
      "generated_abstract": "Title: Autonomous Identification of Unaccusative and Unergative Verbs\n\nGenerated Abstract: This study introduces an autonomous technique for the detection of unergative and unaccusative verbs in the English language. The categorization aids in the identification of verbs involved in the causative-inchoative alternation, irrespective of the semantic roles of the verb. The proposed method hinges on the creation of intransitive sentence variations of potential verbs and scrutinizing a language model. The outcomes achieved are consistent with those garnered through comparable methodologies, with the distinctive advantage of eliminating the need for annotated resources.",
      "original_id": "oai:arXiv.org:2111.00808",
      "created": "2021-11-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Comparisons of Order Statistics from Some Heterogeneous Discrete\n  Distributions",
      "original_abstract": "In this paper, we compare extreme order statistics through vector\nmajorization arising from heterogeneous Poisson and geometric random variables.\nThese comparisons are carried out with respect to usual stochastic ordering.",
      "generated_abstract": "Title: An Analysis of Order Statistics from Various Heterogeneous Discrete Distributions\n\nRewritten Abstract: The present study undertakes a comparative analysis of extreme order statistics derived from disparate Poisson and geometric random variables, utilizing the concept of vector majorization. These comparative assessments are conducted in reference to standard stochastic ordering.",
      "original_id": "oai:arXiv.org:2103.00763",
      "created": "2021-03-01",
      "categories": [
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "BORM: Bayesian Object Relation Model for Indoor Scene Recognition",
      "original_abstract": "Scene recognition is a fundamental task in robotic perception. For human\nbeings, scene recognition is reasonable because they have abundant object\nknowledge of the real world. The idea of transferring prior object knowledge\nfrom humans to scene recognition is significant but still less exploited. In\nthis paper, we propose to utilize meaningful object representations for indoor\nscene representation. First, we utilize an improved object model (IOM) as a\nbaseline that enriches the object knowledge by introducing a scene parsing\nalgorithm pretrained on the ADE20K dataset with rich object categories related\nto the indoor scene. To analyze the object co-occurrences and pairwise object\nrelations, we formulate the IOM from a Bayesian perspective as the Bayesian\nobject relation model (BORM). Meanwhile, we incorporate the proposed BORM with\nthe PlacesCNN model as the combined Bayesian object relation model (CBORM) for\nscene recognition and significantly outperforms the state-of-the-art methods on\nthe reduced Places365 dataset, and SUN RGB-D dataset without retraining,\nshowing the excellent generalization ability of the proposed method. Code can\nbe found at https://github.com/hszhoushen/borm.",
      "generated_abstract": "Title: BORM: A Bayesian Approach to Object Relation Modeling for Indoor Scene Recognition\n\nRevised Abstract: The understanding of scenes is a fundamental component of robotic perception, a task humans perform effortlessly due to their extensive knowledge of real-world objects. However, the significant concept of transferring this pre-existing object understanding from humans to robotic scene recognition remains underutilized. In this study, we introduce the concept of using meaningful object representations to facilitate indoor scene comprehension. We initially employ an enhanced object model (EOM), which augments object knowledge through an advanced scene parsing algorithm. This algorithm is pretrained using the ADE20K dataset, which encompasses a wide range of object categories pertinent to indoor scenes. To scrutinize the co-occurrence of objects and their pairwise relationships, we reinterpret the EOM through a Bayesian lens, leading to the development of the Bayesian object relation model (BORM). Concurrently, we integrate the BORM with the PlacesCNN model, resulting in a combined Bayesian object relation model (CBORM) for scene recognition. This integration yields superior performance over existing methods on the reduced Places365 dataset and the SUN RGB-D dataset, without necessitating retraining. This highlights the exceptional generalization capability of our proposed methodology. The related code can be accessed via https://github.com/hszhoushen/borm.",
      "original_id": "oai:arXiv.org:2108.00397",
      "created": "2021-08-01",
      "categories": [
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "BINet: Learning to Solve Partial Differential Equations with Boundary\n  Integral Networks",
      "original_abstract": "We propose a method combining boundary integral equations and neural networks\n(BINet) to solve partial differential equations (PDEs) in both bounded and\nunbounded domains. Unlike existing solutions that directly operate over\noriginal PDEs, BINet learns to solve, as a proxy, associated boundary integral\nequations using neural networks. The benefits are three-fold. Firstly, only the\nboundary conditions need to be fitted since the PDE can be automatically\nsatisfied with single or double layer representations according to the\npotential theory. Secondly, the dimension of the boundary integral equations is\ntypically smaller, and as such, the sample complexity can be reduced\nsignificantly. Lastly, in the proposed method, all differential operators of\nthe original PDEs have been removed, hence the numerical efficiency and\nstability are improved. Adopting neural tangent kernel (NTK) techniques, we\nprovide proof of the convergence of BINets in the limit that the width of the\nneural network goes to infinity. Extensive numerical experiments show that,\nwithout calculating high-order derivatives, BINet is much easier to train and\nusually gives more accurate solutions, especially in the cases that the\nboundary conditions are not smooth enough. Further, BINet outperforms strong\nbaselines for both one single PDE and parameterized PDEs in the bounded and\nunbounded domains.",
      "generated_abstract": "Title: BINet: Utilizing Boundary Integral Networks for the Resolution of Partial Differential Equations\n\nGenerated Abstract: This paper introduces a novel method (BINet) that merges boundary integral equations and neural networks for the resolution of partial differential equations (PDEs) in various domains, both bounded and unbounded. Differing from current methods that directly interact with the original PDEs, BINet employs a learning approach to solve the associated boundary integral equations through neural networks. This approach offers three distinct advantages. First, the process only necessitates the fitting of boundary conditions as the PDE can be automatically complied with single or double layer representations in line with potential theory. Second, the boundary integral equations generally have a smaller dimension, allowing for a substantial decrease in sample complexity. Finally, the removal of all differential operators from the original PDEs in this method enhances numerical efficiency and stability. Leveraging neural tangent kernel (NTK) methods, we offer evidence of BINets' convergence as the width of the neural network approaches infinity. Comprehensive numerical tests reveal that BINet, which does not require the calculation of high-order derivatives, is simpler to train and typically provides more precise solutions, especially when the boundary conditions lack smoothness. Additionally, BINet surpasses strong baselines for a single PDE and parameterized PDEs across both bounded and unbounded domains.",
      "original_id": "oai:arXiv.org:2110.00352",
      "created": "2021-10-01",
      "categories": [
        "math.NA",
        "cs.NA"
      ]
    },
    {
      "title": "Online anomaly detection using statistical leverage for streaming\n  business process events",
      "original_abstract": "While several techniques for detecting trace-level anomalies in event logs in\noffline settings have appeared recently in the literature, such techniques are\ncurrently lacking for online settings. Event log anomaly detection in online\nsettings can be crucial for discovering anomalies in process execution as soon\nas they occur and, consequently, allowing to promptly take early corrective\nactions. This paper describes a novel approach to event log anomaly detection\non event streams that uses statistical leverage. Leverage has been used\nextensively in statistics to develop measures to identify outliers and it has\nbeen adapted in this paper to the specific scenario of event stream data. The\nproposed approach has been evaluated on both artificial and real event streams.",
      "generated_abstract": "Title: Utilizing Statistical Leverage for Anomaly Detection in Streaming Business Process Events Online\n\nRevised Abstract: Recent literature has seen the emergence of various techniques aimed at identifying anomalies in offline event logs at the trace-level. However, there is a noticeable absence of such methods for online settings. The early detection of anomalies in process execution, through online event log anomaly detection, is crucial for timely identification and swift corrective action. This study proposes an innovative method for anomaly detection in event streams, using statistical leverage. This statistical tool, widely used for outlier identification, has been tailored to fit the unique requirements of event stream data in this study. The proposed method has been assessed and found effective on both synthetic and actual event streams.",
      "original_id": "oai:arXiv.org:2103.00831",
      "created": "2021-03-01",
      "categories": [
        "cs.LG",
        "stat.CO"
      ]
    },
    {
      "title": "Graph-based Exercise- and Knowledge-Aware Learning Network for Student\n  Performance Prediction",
      "original_abstract": "Predicting student performance is a fundamental task in Intelligent Tutoring\nSystems (ITSs), by which we can learn about students' knowledge level and\nprovide personalized teaching strategies for them. Researchers have made plenty\nof efforts on this task. They either leverage educational psychology methods to\npredict students' scores according to the learned knowledge proficiency, or\nmake full use of Collaborative Filtering (CF) models to represent latent\nfactors of students and exercises. However, most of these methods either\nneglect the exercise-specific characteristics (e.g., exercise materials), or\ncannot fully explore the high-order interactions between students, exercises,\nas well as knowledge concepts. To this end, we propose a Graph-based Exercise-\nand Knowledge-Aware Learning Network for accurate student score prediction.\nSpecifically, we learn students' mastery of exercises and knowledge concepts\nrespectively to model the two-fold effects of exercises and knowledge concepts.\nThen, to model the high-order interactions, we apply graph convolution\ntechniques in the prediction process. Extensive experiments on two real-world\ndatasets prove the effectiveness of our proposed Graph-EKLN.",
      "generated_abstract": "Title: A Novel Graph-Based Approach for Student Performance Prediction Considering Exercise and Knowledge Factors\n\nRevised Abstract: A cornerstone function of Intelligent Tutoring Systems (ITSs) is the prediction of student performance, enabling the system to gauge individual knowledge proficiency and subsequently tailor teaching strategies. This crucial task has been the focus of extensive research, utilizing approaches from educational psychology to estimate student scores based on their acquired knowledge proficiency, or employing Collaborative Filtering (CF) models to depict the latent attributes of students and their assigned exercises. However, many of these methods either disregard exercise-specific variables such as the nature of the exercise materials, or inadequately investigate high-order interactions between students, the exercises, and the knowledge concepts. In response, we introduce a graph-based learning network that is both exercise- and knowledge-aware, aiming to optimize student score prediction. Specifically, our model considers both the impact of exercises and knowledge concepts by evaluating students' command of these two aspects. To capture high-order interactions, graph convolution techniques are employed during the prediction phase. Rigorous testing on two authentic datasets validates the efficacy of our newly proposed Graph-EKLN.",
      "original_id": "oai:arXiv.org:2106.00263",
      "created": "2021-06-01",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Atomistic simulations of twin boundary effect on the crack growth\n  behaviour in BCC Fe",
      "original_abstract": "In this paper, the effect of twin boundaries on the crack growth behaviour of\nsingle crystal BCC Fe has been investigated using molecular dynamics\nsimulations. The growth of an atomically sharp crack with an orientation of\n(111)$<$110$>$ (crack plane/crack front) has been studied under mode-I loading\nat constant strain rate. In order to study the influence of twin boundaries on\nthe crack growth behaviour, single and multiple twin boundaries were introduced\nperpendicular to crack growth direction. The results indicate that the\n(111)$<$110$>$ crack in single crystal BCC Fe grows in brittle manner. However,\nfollowing the introduction of twin boundaries, a noticeable plastic deformation\nhas been observed at the crack tip. Further, increasing the number of twin\nboundaries increased the amount of plastic deformation leading to better crack\nresistance and high failure strains. Finally, an interesting relationship has\nbeen observed between the crack growth rate and flow stress.",
      "generated_abstract": "Title: Computational Analysis of Twin Boundary Impact on Crack Propagation Behaviour in BCC Fe\n\nNew Abstract: The present study employs molecular dynamics simulations to examine the influence of twin boundaries on crack propagation characteristics in single crystal BCC Fe. The research scrutinizes the expansion of an atomically precise crack aligned with a (111)$<$110$>$ (crack plane/crack front) under mode-I loading at a steady strain rate. In an effort to explore how twin boundaries can modify the crack propagation behaviour, single and multiple twin boundaries were integrated perpendicular to the direction of crack expansion. The findings reveal that the (111)$<$110$>$ crack develops in a brittle fashion in single crystal BCC Fe. Nevertheless, the integration of twin boundaries resulted in discernible plastic deformation at the apex of the crack. Moreover, augmenting the count of twin boundaries escalated the degree of plastic deformation, thereby enhancing crack resistance and causing high strain at failure. The study concludes by noting a compelling correlation between the rate of crack growth and flow stress.",
      "original_id": "oai:arXiv.org:2112.00354",
      "created": "2021-12-01",
      "categories": [
        "cond-mat.mtrl-sci"
      ]
    },
    {
      "title": "gnlse-python: Open Source Software to Simulate Nonlinear Light\n  Propagation In Optical Fibers",
      "original_abstract": "The propagation of pulses in optical fibers is described by the generalized\nnonlinear Schrodinger equation (GNLSE), which takes into account the fiber\nlosses, nonlinear effects, and higher-order chromatic dispersion. The GNLSE is\na partial differential equation, whose order depends on the accounted nonlinear\nand dispersion effects. We present gnlse-python, a nonlinear optics modeling\ntoolbox that contains a rich set of components and modules to solve the GNLSE\nusing the split-step Fourier transform method (SSFM). The numerical solver is\nfreely available, implemented in Python language, and includes a number of\noptical fiber analysis tools. Code and data are available at\nhttps://github.com/WUST-FOG/gnlse-python.",
      "generated_abstract": "Title: gnlse-python: A Freely Accessible Program for Modeling Nonlinear Light Propagation in Optical Fibers\n\nRevised Abstract: The generalized nonlinear Schrodinger equation (GNLSE) is a comprehensive model that describes the transmission of pulses in optical fibers, accounting for fiber losses, nonlinear effects, and higher-order chromatic dispersion. The complexity of the GNLSE, a partial differential equation, varies with the inclusion of different nonlinear and dispersion factors. This paper introduces gnlse-python, an open-source toolbox for nonlinear optics simulation, equipped with a diverse suite of modules and components to tackle the GNLSE using the split-step Fourier transform method (SSFM). The computational solver, which is offered without charge, is developed in Python and comes packed with several optical fiber evaluation tools. The code and associated data can be accessed via https://github.com/WUST-FOG/gnlse-python.",
      "original_id": "oai:arXiv.org:2110.00298",
      "created": "2021-10-01",
      "categories": [
        "physics.optics",
        "cs.NA",
        "math.NA",
        "nlin.PS"
      ]
    },
    {
      "title": "Gradient potential estimates in elliptic obstacle problems with Orlicz\n  growth",
      "original_abstract": "In this paper,we consider the solutions of the non-homogeneous elliptic\nobstacle problems with Orlicz growth involving measure data. We first establish\nthe pointwise estimates of the approximable solutions to these problems via\nfractional maximal operators. As a result, we obtain pointwise and oscillation\nestimates for the gradients of solutions by the non-linear Wolff potentials,\nand these yield results on $C^{1,\\alpha}$-regularity of solutions.",
      "generated_abstract": "Title: Evaluations of Gradient Potential in Non-Homogeneous Elliptic Obstacle Problems with Orlicz Expansion\n\nGenerated Abstract: The focus of this research is on the solutions pertaining to non-homogeneous elliptic obstacle problems that are associated with Orlicz expansion and incorporate measure data. Initially, we propose pointwise estimates of the solutions that can be approximated to these issues through the application of fractional maximal operators. Consequently, we derive both pointwise and oscillation estimates for the solution gradients via the non-linear Wolff potentials. These findings further lead to results concerning $C^{1,\\alpha}$-regularity of the solutions.",
      "original_id": "oai:arXiv.org:2104.00260",
      "created": "2021-04-01",
      "categories": [
        "math.AP"
      ]
    },
    {
      "title": "Wiki to Automotive: Understanding the Distribution Shift and its impact\n  on Named Entity Recognition",
      "original_abstract": "While transfer learning has become a ubiquitous technique used across Natural\nLanguage Processing (NLP) tasks, it is often unable to replicate the\nperformance of pre-trained models on text of niche domains like Automotive. In\nthis paper we aim to understand the main characteristics of the distribution\nshift with automotive domain text (describing technical functionalities such as\nCruise Control) and attempt to explain the potential reasons for the gap in\nperformance. We focus on performing the Named Entity Recognition (NER) task as\nit requires strong lexical, syntactic and semantic understanding by the model.\nOur experiments with 2 different encoders, namely BERT-Base-Uncased and\nSciBERT-Base-Scivocab-Uncased have lead to interesting findings that showed: 1)\nThe performance of SciBERT is better than BERT when used for automotive domain,\n2) Fine-tuning the language models with automotive domain text did not make\nsignificant improvements to the NER performance, 3) The distribution shift is\nchallenging as it is characterized by lack of repeating contexts, sparseness of\nentities, large number of Out-Of-Vocabulary (OOV) words and class overlap due\nto domain specific nuances.",
      "generated_abstract": "Title: Comprehending the Shift in Distribution and its Effect on Named Entity Recognition in the Automotive Field\n\nRewritten Abstract: Even though transfer learning is extensively employed across tasks in Natural Language Processing (NLP), it often falls short in emulating the performance of pre-existing models when applied to specialized domains such as Automotive. This research seeks to decipher the primary features of the distribution shift associated with automotive domain text (detailing technical features like Cruise Control) and endeavors to rationalize the disparity in performance. The focus is on executing the task of Named Entity Recognition (NER), which necessitates a robust understanding of lexical, syntactic, and semantic aspects by the model. Our experimental trials with two distinct encoders, BERT-Base-Uncased and SciBERT-Base-Scivocab-Uncased, have resulted in intriguing discoveries: 1) SciBERT outperforms BERT in the automotive domain, 2) Refining the language models with automotive domain text failed to considerably enhance the NER performance, 3) The shift in distribution is a challenge due to the paucity of recurring contexts, entity sparseness, a high quantity of Out-Of-Vocabulary (OOV) terms, and class overlapping as a result of domain-specific subtleties.",
      "original_id": "oai:arXiv.org:2112.00283",
      "created": "2021-12-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Efficient and Differentiable Shadow Computation for Inverse Problems",
      "original_abstract": "Differentiable rendering has received increasing interest for image-based\ninverse problems. It can benefit traditional optimization-based solutions to\ninverse problems, but also allows for self-supervision of learning-based\napproaches for which training data with ground truth annotation is hard to\nobtain. However, existing differentiable renderers either do not model\nvisibility of the light sources from the different points in the scene,\nresponsible for shadows in the images, or are too slow for being used to train\ndeep architectures over thousands of iterations. To this end, we propose an\naccurate yet efficient approach for differentiable visibility and soft shadow\ncomputation. Our approach is based on the spherical harmonics approximations of\nthe scene illumination and visibility, where the occluding surface is\napproximated with spheres. This allows for a significantly more efficient\nshadow computation compared to methods based on ray tracing. As our formulation\nis differentiable, it can be used to solve inverse problems such as texture,\nillumination, rigid pose, and geometric deformation recovery from images using\nanalysis-by-synthesis optimization.",
      "generated_abstract": "Title: A Proficient and Differentiable Method for Shadow Calculation in Inverse Problems \n\nAlternative Abstract: The escalating interest in differentiable rendering has been noticeable in the realm of image-based inverse problems. It not only enhances traditional optimization-based resolutions to inverse problems but also enables learning-based methodologies to self-supervise, especially when it is challenging to acquire training data with accurate ground truth annotation. However, the prevailing differentiable renderers either fail to simulate the visibility of light sources from diverse points in the scene, which is crucial for creating shadows in images, or they are inadequately slow to train deep architectures over numerous iterations. In response to this, we introduce a precise and efficient technique for differentiable visibility and gentle shadow computation. Our method hinges on the approximation of scene illumination and visibility using spherical harmonics, where the obstructing surface is replicated with spheres. This significantly enhances the proficiency of shadow calculation, making it superior to ray tracing-based methods. Since our approach is differentiable, it can be employed to tackle inverse problems such as recovery of texture, illumination, rigid pose, and geometric deformation from images through analysis-by-synthesis optimization.",
      "original_id": "oai:arXiv.org:2104.00359",
      "created": "2021-04-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Mean-field particle swarm optimization",
      "original_abstract": "In this work we survey some recent results on the global minimization of a\nnon-convex and possibly non-smooth high dimensional objective function by means\nof particle based gradient-free methods. Such problems arise in many situations\nof contemporary interest in machine learning and signal processing. After a\nbrief overview of metaheuristic methods based on particle swarm optimization\n(PSO), we introduce a continuous formulation via second-order systems of\nstochastic differential equations that generalize PSO methods and provide the\nbasis for their theoretical analysis. Subsequently, we will show how through\nthe use of mean-field techniques it is possible to derive in the limit of large\nparticles number the corresponding mean-field PSO description based on\nVlasov-Fokker-Planck type equations. Finally, in the zero inertia limit, we\nwill analyze the corresponding macroscopic hydrodynamic equations, showing that\nthey generalize the recently introduced consensus-based optimization (CBO)\nmethods by including memory effects. Rigorous results concerning the mean-field\nlimit, the zero-inertia limit, and the convergence of the mean-field PSO method\ntowards the global minimum are provided along with a suite of numerical\nexamples.",
      "generated_abstract": "Title: Analysis of Mean-field Particle Swarm Optimization\n\nRevised Abstract: This study presents an examination of recent findings on the global minimization of a potentially non-smooth and non-convex high dimensional objective function, utilizing gradient-free strategies based on particles. Such complications are prevalent in current areas of interest such as machine learning and signal processing. An initial review of metaheuristic procedures founded on particle swarm optimization (PSO) is given, followed by the presentation of a continuous formulation through second-order systems of stochastic differential equations. These broaden the PSO methods, laying the groundwork for their theoretical assessment. We then demonstrate how the application of mean-field techniques enables the derivation of the corresponding mean-field PSO depiction based on Vlasov-Fokker-Planck type equations for a large number of particles. In the zero-inertia limit, we scrutinize the corresponding macroscopic hydrodynamic equations, indicating they broaden the recently proposed consensus-based optimization (CBO) methods by incorporating memory effects. Detailed findings regarding the mean-field limit, the zero-inertia limit, and the convergence of the mean-field PSO method towards the global minimum are supplied, accompanied by a range of numerical instances.",
      "original_id": "oai:arXiv.org:2108.00393",
      "created": "2021-08-01",
      "categories": [
        "math.OC",
        "cs.NA",
        "math.NA"
      ]
    },
    {
      "title": "A translation of G. S. Makanin's 1966 Ph.D. thesis \"On the Identity\n  Problem for Finitely Presented Groups and Semigroups\"",
      "original_abstract": "This is an English translation of the thesis written by G. S. Makanin for the\ndegree of Candidate of Physical and Mathematical Sciences (equivalent to a\nPh.D.), originally submitted to the Steklov Mathematical Institute in 1966. The\noriginal language is Russian. The named supervisors are A. A. Markov and S. I.\nAdian.",
      "generated_abstract": "Title: An English Interpretation of G. S. Makanin's 1966 Doctoral Thesis \"On the Identity Problem for Finitely Presented Groups and Semigroups\"\n\nRevised Abstract: This work presents an English interpretation of G. S. Makanin's pioneering doctoral research, which he initially presented in Russian to the Steklov Mathematical Institute in 1966 to earn the title of Candidate of Physical and Mathematical Sciences, a degree comparable to a Ph.D. The original research was supervised by A. A. Markov and S. I. Adian.",
      "original_id": "oai:arXiv.org:2102.00745",
      "created": "2021-02-01",
      "categories": [
        "math.GR",
        "math.HO"
      ]
    },
    {
      "title": "Multi-Sample based Contrastive Loss for Top-k Recommendation",
      "original_abstract": "The top-k recommendation is a fundamental task in recommendation systems\nwhich is generally learned by comparing positive and negative pairs. The\nContrastive Loss (CL) is the key in contrastive learning that has received more\nattention recently and we find it is well suited for top-k recommendations.\nHowever, it is a problem that CL treats the importance of the positive and\nnegative samples as the same. On the one hand, CL faces the imbalance problem\nof one positive sample and many negative samples. On the other hand, positive\nitems are so few in sparser datasets that their importance should be\nemphasized. Moreover, the other important issue is that the sparse positive\nitems are still not sufficiently utilized in recommendations. So we propose a\nnew data augmentation method by using multiple positive items (or samples)\nsimultaneously with the CL loss function. Therefore, we propose a Multi-Sample\nbased Contrastive Loss (MSCL) function which solves the two problems by\nbalancing the importance of positive and negative samples and data\naugmentation. And based on the graph convolution network (GCN) method,\nexperimental results demonstrate the state-of-the-art performance of MSCL. The\nproposed MSCL is simple and can be applied in many methods. We will release our\ncode on GitHub upon the acceptance.",
      "generated_abstract": "Title: A Novel Approach to Top-k Recommendation through Multi-Sample Contrastive Loss\n\nRevised Abstract: The primary function of recommendation systems, the top-k recommendation, typically learns by juxtaposing positive and negative pairs. In recent times, there has been a surge of interest in contrastive learning, particularly the Contrastive Loss (CL). Our study identifies the suitability of CL for top-k recommendations but also highlights an inherent issue - the treatment of positive and negative samples with equal importance. This leads to two problems: firstly, an imbalance between a singular positive sample and numerous negative samples, and secondly, the underemphasis of positive items which are scant in sparser datasets. Furthermore, the limited use of sparse positive items in recommendations is another significant concern. To address these issues, we introduce an innovative data augmentation technique that employs multiple positive items simultaneously with the CL loss function. Consequently, we present a Multi-Sample based Contrastive Loss (MSCL) function, adept at resolving the above problems through a balanced approach to positive and negative samples and effective data augmentation. Our experimental results, based on the graph convolution network (GCN) method, show that the MSCL function outperforms other methods, thereby establishing it as a cutting-edge solution. Simple in its design, MSCL can be incorporated into several other methods. Upon acceptance, we will make our code publicly available on GitHub.",
      "original_id": "oai:arXiv.org:2109.00217",
      "created": "2021-09-01",
      "categories": [
        "cs.IR",
        "cs.AI"
      ]
    },
    {
      "title": "A note on simple games with two equivalence classes of players",
      "original_abstract": "Many real-world voting systems consist of voters that occur in just two\ndifferent types. Indeed, each voting system with a {\\lq\\lq}House{\\rq\\rq} and a\n{\\lq\\lq}Senat{\\rq\\rq} is of that type. Here we present structural\ncharacterizations and explicit enumeration formulas for these so-called\nbipartite simple games.",
      "generated_abstract": "Title: An Observation on Bipartite Simple Games with Two Categories of Participants\n\nRewritten Abstract: A multitude of practical voting structures comprise only two distinct classifications of voters. This is particularly true for any voting system that includes a \"House\" and a \"Senate\". This study provides comprehensive structural definitions and precise calculation formulas for these types of games, often referred to as bipartite simple games.",
      "original_id": "oai:arXiv.org:2112.00307",
      "created": "2021-12-01",
      "categories": [
        "math.CO",
        "cs.GT"
      ]
    },
    {
      "title": "Plasmonic Microbubble Dynamics in Binary Liquids",
      "original_abstract": "The growth of surface plasmonic microbubbles in binary water/ethanol\nsolutions is experimentally studied. The microbubbles are generated by\nilluminating a gold nanoparticle array with a continuous wave laser. Plasmonic\nbubbles exhibit ethanol concentration-dependent behaviors. For low ethanol\nconcentrations (f_e) of < 67.5%, bubbles do not exist at the solid-liquid\ninterface. For high f_e values of >80%, the bubbles behave as in pure ethanol.\nOnly in an intermediate window of 67.5% < f_e < 80% do we find sessile\nplasmonic bubbles with a highly nontrivial temporal evolution, in which as a\nfunction of time three phases can be discerned. (1) In the first phase, the\nmicrobubbles grow, while wiggling. (2) As soon as the wiggling stops, the\nmicrobubbles enter the second phase in which they suddenly shrink, followed by\n(3) a steady reentrant growth phase. Our experiments reveal that the sudden\nshrinkage of the microbubbles in the second regime is caused by a depinning\nevent of the three phase contact line. We systematically vary the ethanol\nconcentration, laser power, and laser spot size to unravel water recondensation\nas the underlying mechanism of the sudden bubble shrinkage in phase 2.",
      "generated_abstract": "Title: Examination of Plasmonic Microbubble Behavior in Mixed Liquids\n\nUpdated Abstract: This study focuses on the experimental analysis of the expansion of surface plasmonic microbubbles in dual water and ethanol solutions. The microbubbles are produced through the illumination of an array of gold nanoparticles with a steady wave laser. It is observed that the behavior of plasmonic bubbles is dependent on the concentration of ethanol. The absence of bubbles at the solid-liquid boundary is noted when the ethanol concentration (f_e) is below 67.5%. On the other hand, with higher f_e values exceeding 80%, the bubbles mimic the behavior noted in pure ethanol. The formation of sessile plasmonic bubbles featuring a complex temporal evolution is only noted within an intermediate range of 67.5% < f_e < 80%. As time progresses, three distinct phases are discerned: (1) The initial phase involves the growth of the microbubbles, accompanied by a wiggling motion. (2) The cessation of the wiggling motion marks the start of the second phase, which is characterized by a sudden contraction of the microbubbles, followed by (3) a constant phase of regrowth. The study identifies the abrupt contraction of the microbubbles in the second stage as being triggered by a depinning occurrence at the three-phase contact line. By systematically adjusting the ethanol concentration, laser power, and laser spot size, the investigation seeks to understand the recondensation of water as the primary cause of the sudden bubble contraction in phase 2.",
      "original_id": "oai:arXiv.org:2101.00219",
      "created": "2021-01-01",
      "categories": [
        "cond-mat.soft",
        "physics.flu-dyn"
      ]
    },
    {
      "title": "Federated Learning: Issues in Medical Application",
      "original_abstract": "Since the federated learning, which makes AI learning possible without moving\nlocal data around, was introduced by google in 2017 it has been actively\nstudied particularly in the field of medicine. In fact, the idea of machine\nlearning in AI without collecting data from local clients is very attractive\nbecause data remain in local sites. However, federated learning techniques\nstill have various open issues due to its own characteristics such as non\nidentical distribution, client participation management, and vulnerable\nenvironments. In this presentation, the current issues to make federated\nlearning flawlessly useful in the real world will be briefly overviewed. They\nare related to data/system heterogeneity, client management, traceability, and\nsecurity. Also, we introduce the modularized federated learning framework, we\ncurrently develop, to experiment various techniques and protocols to find\nsolutions for aforementioned issues. The framework will be open to public after\ndevelopment completes.",
      "generated_abstract": "Title: The Challenges of Implementing Federated Learning in the Medical Field\n\nRewritten Abstract: The conception of federated learning by Google in 2017, a process enabling artificial intelligence (AI) to learn without the necessity of data relocation, has sparked significant interest, particularly in the realm of healthcare. The ability for AI to learn without gathering data from local clients is appealing as it allows data to remain at the originating sites. However, there exist numerous unresolved issues related to federated learning due to its inherent characteristics such as non-identical distribution, client participation oversight, and susceptibility to security breaches. This paper provides a succinct overview of the current challenges that prevent federated learning from being seamlessly implemented in practical applications, touching upon aspects such as data/system heterogeneity, client management, traceability, and security. Furthermore, we present our ongoing development of a modular federated learning framework designed to experiment with various techniques and protocols with the aim of resolving the highlighted issues. Upon completion, the framework will be made publicly accessible.",
      "original_id": "oai:arXiv.org:2109.00202",
      "created": "2021-09-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of\n  Dynamic Agents",
      "original_abstract": "In this paper, we address the problem of predicting the future motion of a\ndynamic agent (called a target agent) given its current and past states as well\nas the information on its environment. It is paramount to develop a prediction\nmodel that can exploit the contextual information in both static and dynamic\nenvironments surrounding the target agent and generate diverse trajectory\nsamples that are meaningful in a traffic context. We propose a novel prediction\nmodel, referred to as the lane-aware prediction (LaPred) network, which uses\nthe instance-level lane entities extracted from a semantic map to predict the\nmulti-modal future trajectories. For each lane candidate found in the\nneighborhood of the target agent, LaPred extracts the joint features relating\nthe lane and the trajectories of the neighboring agents. Then, the features for\nall lane candidates are fused with the attention weights learned through a\nself-supervised learning task that identifies the lane candidate likely to be\nfollowed by the target agent. Using the instance-level lane information, LaPred\ncan produce the trajectories compliant with the surroundings better than 2D\nraster image-based methods and generate the diverse future trajectories given\nmultiple lane candidates. The experiments conducted on the public nuScenes\ndataset and Argoverse dataset demonstrate that the proposed LaPred method\nsignificantly outperforms the existing prediction models, achieving\nstate-of-the-art performance in the benchmarks.",
      "generated_abstract": "Title: LaPred: A Novel Approach to Predicting Multi-modal Future Trajectories of Dynamic Agents \n\nRewritten Abstract: This study focuses on the prediction of future movements of a dynamic agent, termed a target agent, based on its current and historical states, and environmental context. The creation of a predictive model, capable of utilizing contextual information from both static and dynamic environments surrounding the target agent, is essential for generating a range of meaningful trajectory samples within a traffic context. We introduce a unique predictive model, the Lane-Aware Prediction (LaPred) network, which leverages instance-level lane entities derived from a semantic map to forecast multi-modal future trajectories. LaPred evaluates joint features connecting each lane candidate in the target agent's vicinity and the trajectories of neighbouring agents. These features are then amalgamated with attention weights, gleaned through a self-supervised learning task that determines the most likely lane candidate to be pursued by the target agent. Through the utilization of instance-level lane information, LaPred is able to generate trajectories that are more in line with the surrounding environment than 2D raster image-based methods, while also producing a variety of future trajectories given multiple lane candidates. Experimental results using the public nuScenes and Argoverse datasets reveal that LaPred significantly surpasses existing prediction models, achieving a new standard in benchmark performance.",
      "original_id": "oai:arXiv.org:2104.00249",
      "created": "2021-04-01",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Backhaul-Aware Intelligent Positioning of UAVs and Association of\n  Terrestrial Base Stations for Fronthaul Connectivity",
      "original_abstract": "The mushroom growth of cellular users requires novel advancements in the\nexisting cellular infrastructure. One way to handle such a tremendous increase\nis to densely deploy terrestrial small-cell base stations (TSBSs) with careful\nmanagement of smart backhaul/fronthaul networks. Nevertheless, terrestrial\nbackhaul hubs significantly suffer from the dense fading environment and are\ndifficult to install in a typical urban environment. Therefore, this paper\nconsiders the idea of replacing terrestrial backhaul network with an aerial\nnetwork consisting of unmanned aerial vehicles (UAVs) to provide the fronthaul\nconnectivity between the TSBSs and the ground core-network (GCN). To this end,\nwe focus on the joint positioning of UAVs and the association of TSBSs such\nthat the sum-rate of the overall system is maximized. In particular, the\nassociation problem of TSBSs with UAVs is formulated under\ncommunication-related constraints, i.e., bandwidth, number of connections to a\nUAV, power limit, interference threshold, UAV heights, and backhaul data rate.\nTo meet this joint objective, we take advantage of the genetic algorithm (GA)\ndue to the offline nature of our optimization problem. The performance of the\nproposed approach is evaluated using the unsupervised learning-based k-means\nclustering algorithm. We observe that the proposed approach is highly effective\nto satisfy the requirements of smart fronthaul networks.",
      "generated_abstract": "Title: Intelligent Placement of UAVs and Affiliation of Terrestrial Base Stations for Fronthaul Connectivity: A Backhaul-Aware Perspective\n\nRevised Abstract: The exponential surge in the number of cellular users necessitates innovative enhancements in the prevailing cellular infrastructure. One possible solution to manage this massive growth is to densely position terrestrial small-cell base stations (TSBSs) while efficiently controlling smart backhaul/fronthaul networks. However, terrestrial backhaul hubs encounter significant challenges due to the dense fading environment and their installation complexity in standard urban settings. This study proposes the substitution of terrestrial backhaul networks with an aerial network composed of unmanned aerial vehicles (UAVs) to facilitate fronthaul connections between the TSBSs and the ground core-network (GCN). The study emphasizes the combined positioning of UAVs and the association of TSBSs to maximize the overall system's sum-rate. Specifically, the paper formulates the TSBSs' association issue with UAVs considering communication-related constraints, including bandwidth, the number of UAV connections, power limits, interference thresholds, UAV heights, and backhaul data rates. To achieve this combined goal, we utilize the genetic algorithm (GA), an ideal approach for the offline nature of our optimization issue. The efficacy of the proposed method is assessed using the unsupervised learning-based k-means clustering algorithm, demonstrating its high effectiveness in meeting the demands of smart fronthaul networks.",
      "original_id": "oai:arXiv.org:2105.00286",
      "created": "2021-05-01",
      "categories": [
        "eess.SP"
      ]
    },
    {
      "title": "Inequalities for the Radon transform on convex sets",
      "original_abstract": "Several years ago the authors started looking at some problems of convex\ngeometry from a more general point of view, replacing volume by an arbitrary\nmeasure. This approach led to new general properties of the Radon transform on\nconvex bodies including an extension of the Busemann-Petty problem and a\nslicing inequality for arbitrary functions. The latter means that the sup-norm\nof the Radon transform of any probability density on a convex body of volume\none is bounded from below by a positive constant depending only on the\ndimension. In this note, we prove an inequality that serves as an umbrella for\nthese results",
      "generated_abstract": "Title: Disparities in the Radon Transformation Over Convex Structures\n\nReconstructed Abstract: A few years back, the researchers initiated an investigation into certain issues of convex geometry, adopting a broader perspective by substituting volume with an arbitrary measure. This innovative methodology unveiled novel general characteristics of the Radon transform applied to convex objects, incorporating an expansion of the Busemann-Petty problem and a slicing inequality applicable to arbitrary functions. The latter implies that the supreme norm of the Radon transform of any probability density, existing on a convex structure of volume one, is constrained from below by an unchanging positive constant, relying solely on the dimension. In this communication, we authenticate an inequality that functions as a comprehensive framework for these findings.",
      "original_id": "oai:arXiv.org:2101.00287",
      "created": "2021-01-01",
      "categories": [
        "math.MG",
        "math.FA"
      ]
    },
    {
      "title": "Combination of component fault trees and Markov chains to analyze\n  complex, software-controlled systems",
      "original_abstract": "Fault Tree analysis is a widely used failure analysis methodology to assess a\nsystem in terms of safety or reliability in many industrial application\ndomains. However, with Fault Tree methodology there is no possibility to\nexpress a temporal sequence of events or state-dependent behavior of\nsoftware-controlled systems. In contrast to this, Markov Chains are a\nstate-based analysis technique based on a stochastic model. But the use of\nMarkov Chains for failure analysis of complex safety-critical systems is\nlimited due to exponential explosion of the size of the model. In this paper,\nwe present a concept to integrate Markov Chains in Component Fault Tree models.\nBased on a component concept for Markov Chains, which enables the association\nof Markov Chains to system development elements such as components, complex or\nsoftware-controlled systems can be analyzed w.r.t. safety or reliability in a\nmodular and compositional way. We illustrate this approach using a case study\nfrom the automotive domain.",
      "generated_abstract": "Title: Integrating Component Fault Trees and Markov Chains for the Analysis of Complex, Software-Controlled Systems\n\nRevised Abstract: The prevalent failure analysis technique, Fault Tree Analysis, is extensively utilized for the evaluation of safety and reliability within various industrial sectors. Yet, this methodology lacks the capacity to articulate the progression of events over time or the state-dependent conduct of software-controlled mechanisms. Alternatively, Markov Chains, founded on a stochastic model, offer a state-based analysis technique. However, their application to failure analysis in intricate safety-critical systems is hindered by the exponential expansion of the model size. This paper proposes a novel approach that incorporates Markov Chains into Component Fault Tree models, using a component-based concept for Markov Chains. This concept allows for the attachment of Markov Chains to elements of system development, such as components, thereby enabling the analysis of complex or software-controlled systems in terms of safety or reliability in a modular and compositional manner. This method is exemplified through a case study in the automotive sector.",
      "original_id": "oai:arXiv.org:2106.00247",
      "created": "2021-06-01",
      "categories": [
        "cs.SE"
      ]
    },
    {
      "title": "Secure Transmission with Different Security Requirements Based on Covert\n  Communication and Information-Theoretic Security in Presence of Friendly\n  Jammer",
      "original_abstract": "In this paper, we investigate joint information-theoretic security and covert\ncommunication on a network in the presence of a single transmitter (Alice), a\nfriendly jammer, a single untrusted user, two legitimate users, and a single\nwarden of the channel (Willie). In the considered network, one of the\nauthorized users, Bob, needs a secure and covert communication, and therefore\nhis message must be sent securely, and at the same time, the existence of his\ncommunication with the transmitter should not be detected by the channel's\nwarden, Willie, Meanwhile, another authorized user, Carol, needs covert\ncommunication. The purpose of secure communication is to prevent the message\nbeing decoded by the untrusted user who is present on the network, which leads\nus to use one of the physical layer security methods, named the secure\ntransmission of information theory. In some cases, in addition to protecting\nthe content of the message, it is important for the user that the existence of\nthe transmission not being detected by an adversary, which leads us to covert\ncommunication. In the proposed network model, it is assumed that for covert\ncommunication requirements, Alice will not send any messages to legitimate\nusers in one time slot and in another time slot will send to them both (Bob and\nCarol). One of the main challenges in covert communication is low transmission\nrate, because we have to reduce the transmission power such that the main\nmessage get hide in background noise.",
      "generated_abstract": "Title: Ensuring Secure and Covert Communication via Information-Theoretic Security in a Network with a Friendly Jammer\n\nRevised Abstract: This study delves into the intersection of information-theoretic security and covert communication within a network involving a single transmitter (Alice), a friendly jammer, an untrusted user, two authorized users, and a single channel warden (Willie). The network framework under consideration involves one authorized user, Bob, who requires both security and covert nature for his communication. As such, not only should his message be transmitted securely, but the transmission itself should also remain undetected by Willie, the channel warden. Moreover, another authorized user, Carol, requires covert communication. The objective of secure communication is to safeguard the message from being deciphered by the untrusted user within the network, thereby necessitating the use of secure transmission of information theory, a method of physical layer security. In certain instances, it is essential to prevent the detection of the transmission itself, not merely protecting the message content, thus leading to the deployment of covert communication. The proposed network model presumes that Alice will cease communication with the authorized users in one time slot and resume in another with both Bob and Carol to meet the covert communication requirements. The primary challenge of covert communication lies in managing the low transmission rate, since the transmission power must be minimized to conceal the primary message within the background noise.",
      "original_id": "oai:arXiv.org:2107.00210",
      "created": "2021-07-01",
      "categories": [
        "eess.SP"
      ]
    },
    {
      "title": "Developing a Compressed Object Detection Model based on YOLOv4 for\n  Deployment on Embedded GPU Platform of Autonomous System",
      "original_abstract": "Latest CNN-based object detection models are quite accurate but require a\nhigh-performance GPU to run in real-time. They still are heavy in terms of\nmemory size and speed for an embedded system with limited memory space. Since\nthe object detection for autonomous system is run on an embedded processor, it\nis preferable to compress the detection network as light as possible while\npreserving the detection accuracy. There are several popular lightweight\ndetection models but their accuracy is too low for safe driving applications.\nTherefore, this paper proposes a new object detection model, referred as\nYOffleNet, which is compressed at a high ratio while minimizing the accuracy\nloss for real-time and safe driving application on an autonomous system. The\nbackbone network architecture is based on YOLOv4, but we could compress the\nnetwork greatly by replacing the high-calculation-load CSP DenseNet with the\nlighter modules of ShuffleNet. Experiments with KITTI dataset showed that the\nproposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could\nachieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).\nCompared to the high compression ratio, the accuracy is reduced slightly to\n85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network\nshowed a high potential to be deployed on the embedded system of the autonomous\nsystem for the real-time and accurate object detection applications.",
      "generated_abstract": "Title: Creation of a Highly Compressed Object Detection Model, YOffleNet, Based on YOLOv4 for Utilization on Autonomous System's Embedded GPU Platform\n\nModified Abstract: Modern object detection models based on Convolutional Neural Networks (CNN) provide high accuracy but necessitate a potent GPU to facilitate real-time operations. However, their extensive memory size and speed requirements pose a challenge for embedded systems, which typically have limited memory capacity. Given that autonomous systems use embedded processors for object detection, it is advantageous to develop a lightweight detection network that retains detection precision. Although there exist several renowned lightweight models, their accuracy levels fall short for applications necessitating safe driving. Consequently, this research introduces a novel object detection model, YOffleNet. This model is highly compressed while ensuring minimal loss in accuracy, making it ideal for real-time and safe driving operations within an autonomous system. The underlying network architecture is YOLOv4-based, but significant network compression is achieved by substituting the computationally intensive CSP DenseNet with the lighter ShuffleNet modules. Trials using the KITTI dataset reveal that the proposed YOffleNet model is 4.7 times more compressed than YOLOv4-s, achieving speeds of up to 46 FPS on an embedded GPU system, specifically the NVIDIA Jetson AGX Xavier. Despite a high compression ratio, accuracy only dips slightly to 85.8% mAP, a mere 2.6% decrement from YOLOv4-s. Therefore, the newly proposed network demonstrates considerable promise for deployment in autonomous systems' embedded platforms, offering real-time and accurate object detection.",
      "original_id": "oai:arXiv.org:2108.00392",
      "created": "2021-08-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Incorporating Transformer and LSTM to Kalman Filter with EM algorithm\n  for state estimation",
      "original_abstract": "Kalman Filter requires the true parameters of the model and solves optimal\nstate estimation recursively. Expectation Maximization (EM) algorithm is\napplicable for estimating the parameters of the model that are not available\nbefore Kalman filtering, which is EM-KF algorithm. To improve the preciseness\nof EM-KF algorithm, the author presents a state estimation method by combining\nthe Long-Short Term Memory network (LSTM), Transformer and EM-KF algorithm in\nthe framework of Encoder-Decoder in Sequence to Sequence (seq2seq). Simulation\non a linear mobile robot model demonstrates that the new method is more\naccurate. Source code of this paper is available at\nhttps://github.com/zshicode/Deep-Learning-Based-State-Estimation.",
      "generated_abstract": "Title: Integration of Transformer and LSTM with Kalman Filter via the EM Algorithm for Enhanced State Estimation\n\nRevised Abstract: The Kalman Filter is typically reliant on the actual parameters of a model for recursive optimal state estimation. However, the Expectation Maximization (EM) algorithm can be employed to estimate these parameters before the application of the Kalman filter, leading to the EM-KF algorithm. To augment the accuracy of the EM-KF algorithm, this paper introduces a state estimation technique that amalgamates the Long-Short Term Memory network (LSTM), Transformer, and EM-KF algorithm within the context of an Encoder-Decoder in the Sequence to Sequence (seq2seq) schema. A simulation conducted on a linear mobile robot model substantiates that this innovative method offers superior accuracy. The paper's source code can be accessed at https://github.com/zshicode/Deep-Learning-Based-State-Estimation.",
      "original_id": "oai:arXiv.org:2105.00250",
      "created": "2021-05-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Inequality in economic shock exposures across the global firm-level\n  supply network",
      "original_abstract": "For centuries, national economies created wealth by engaging in international\ntrade and production. The resulting international supply networks not only\nincrease wealth for countries, but also create systemic risk: economic shocks,\ntriggered by company failures in one country, may propagate to other countries.\nUsing global supply network data on the firm-level, we present a method to\nestimate a country's exposure to direct and indirect economic losses caused by\nthe failure of a company in another country. We show the network of systemic\nrisk-flows across the world. We find that rich countries expose poor countries\nmuch more to systemic risk than the other way round. We demonstrate that higher\nsystemic risk levels are not compensated with a risk premium in GDP, nor do\nthey correlate with economic growth. Systemic risk around the globe appears to\nbe distributed more unequally than wealth. These findings put the often praised\nbenefits for developing countries from globalized production in a new light,\nsince they relate them to the involved risks in the production processes.\nExposure risks present a new dimension of global inequality, that most affects\nthe poor in supply shock crises. It becomes fully quantifiable with the\nproposed method.",
      "generated_abstract": "Title: Economic Shock Disparities in International Firm-Level Supply Chains\n\nRevised Abstract: Over the years, global economies have amassed wealth through participation in international commerce and production. This has led to the establishment of an intricate web of global supply networks that, while profitable, present a systemic risk. Economic disruptions, initiated by the collapse of businesses in one nation, can ripple outwards, impacting other nations. Utilising data from international supply networks at a firm-level, this study introduces a methodology to gauge a nation's vulnerability to both direct and indirect financial damages arising from a company's failure in a different country. Our findings illustrate the global network of systemic risk currents and reveal a greater exposure of impoverished countries to systemic risks from affluent nations, as opposed to the reverse. Contrary to expectations, we found no correlation between higher systemic risk levels and an increase in GDP risk premiums or economic growth. Interestingly, systemic risk appears to be more unevenly distributed worldwide than wealth itself. These results offer a fresh perspective on the purported benefits of globalized production for developing nations by highlighting the associated production risks. The exposure to these risks presents a novel facet of global inequality, which disproportionately influences the economically disadvantaged during supply shock crises. This exposure can now be fully quantified using the proposed method.",
      "original_id": "oai:arXiv.org:2112.00415",
      "created": "2021-12-01",
      "categories": [
        "econ.GN",
        "physics.soc-ph",
        "q-fin.EC"
      ]
    },
    {
      "title": "Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene\n  Recognition",
      "original_abstract": "Accurate perception of the surrounding scene is helpful for robots to make\nreasonable judgments and behaviours. Therefore, developing effective scene\nrepresentation and recognition methods are of significant importance in\nrobotics. Currently, a large body of research focuses on developing novel\nauxiliary features and networks to improve indoor scene recognition ability.\nHowever, few of them focus on directly constructing object features and\nrelations for indoor scene recognition. In this paper, we analyze the\nweaknesses of current methods and propose an Object-to-Scene (OTS) method,\nwhich extracts object features and learns object relations to recognize indoor\nscenes. The proposed OTS first extracts object features based on the\nsegmentation network and the proposed object feature aggregation module (OFAM).\nAfterwards, the object relations are calculated and the scene representation is\nconstructed based on the proposed object attention module (OAM) and global\nrelation aggregation module (GRAM). The final results in this work show that\nOTS successfully extracts object features and learns object relations from the\nsegmentation network. Moreover, OTS outperforms the state-of-the-art methods by\nmore than 2\\% on indoor scene recognition without using any additional streams.\nCode is publicly available at: https://github.com/FreeformRobotics/OTS.",
      "generated_abstract": "Title: From Object to Scene: Enhancing Indoor Scene Recognition by Applying Object Knowledge \n\nRevised Abstract: For robots to form appropriate judgements and actions, a precise understanding of their surrounding environment is crucial. As a result, the development of efficient techniques for scene representation and recognition is of paramount importance in the field of robotics. Presently, the majority of research in this area is centered around the creation of innovative auxiliary features and networks to enhance the capability for indoor scene identification. However, there is a lack of focus on the direct formation of object features and their relationships for indoor scene recognition. This paper scrutinizes the shortcomings of current methodologies and introduces an Object-to-Scene (OTS) approach. This approach focuses on the extraction of object features and the learning of object relationships in order to recognize indoor scenes. Initially, the OTS technique extracts object features using a segmentation network and a newly proposed object feature aggregation module (OFAM). Following this, object relationships are determined, and the scene representation is formed using the newly introduced object attention module (OAM) and global relation aggregation module (GRAM). The ultimate findings of this research indicate that the OTS method successfully extracts object features and learns object relationships through the segmentation network, surpassing leading methods by over 2% in indoor scene recognition without the need for additional streams. The code for this research is openly accessible at: https://github.com/FreeformRobotics/OTS.",
      "original_id": "oai:arXiv.org:2108.00399",
      "created": "2021-08-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Volta at SemEval-2021 Task 9: Statement Verification and Evidence\n  Finding with Tables using TAPAS and Transfer Learning",
      "original_abstract": "Tables are widely used in various kinds of documents to present information\nconcisely. Understanding tables is a challenging problem that requires an\nunderstanding of language and table structure, along with numerical and logical\nreasoning. In this paper, we present our systems to solve Task 9 of\nSemEval-2021: Statement Verification and Evidence Finding with Tables\n(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a\nstatement, predicting whether the table supports the statement and (B)\nPredicting which cells in the table provide evidence for/against the statement.\nWe fine-tune TAPAS (a model which extends BERT's architecture to capture\ntabular structure) for both the subtasks as it has shown state-of-the-art\nperformance in various table understanding tasks. In subtask A, we evaluate how\ntransfer learning and standardizing tables to have a single header row improves\nTAPAS' performance. In subtask B, we evaluate how different fine-tuning\nstrategies can improve TAPAS' performance. Our systems achieve an F1 score of\n67.34 in subtask A three-way classification, 72.89 in subtask A two-way\nclassification, and 62.95 in subtask B.",
      "generated_abstract": "Title: Utilizing TAPAS and Transfer Learning for Task 9 of SemEval-2021: A Study on Statement Verification and Evidence Discovery with Tables\n\nRevised Abstract: Tables, ubiquitously found in a multitude of documents, serve to succinctly display information. The task of understanding tables poses significant challenges as it necessitates comprehension of the language, table structure, and numerical as well as logical reasoning. This study details our developed systems intended to tackle Task 9 of SemEval-2021, namely, Statement Verification and Evidence Finding with Tables (SEM-TAB-FACTS). The task is bifurcated into two subtasks: Subtask A, which involves predicting if a given table substantiates a statement and Subtask B, which involves identifying the table cells providing evidence supporting or contradicting the statement. Our approach utilizes fine-tuning of the TAPAS model - an extension of the BERT model specifically designed to comprehend tabular structures - for both subtasks, leveraging its proven excellence in numerous table comprehension tasks. In Subtask A, we assess the impact of transfer learning and table standardization into a single header row on enhancing TAPAS' effectiveness. For Subtask B, we examine the potential improvement in TAPAS' performance through varied fine-tuning methodologies. Our systems have demonstrated commendable performance, scoring an F1 of 67.34 in Subtask A's three-way classification, 72.89 in Subtask A's two-way classification, and 62.95 in Subtask B.",
      "original_id": "oai:arXiv.org:2106.00248",
      "created": "2021-06-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Multi-Access Coded Caching with Demand Privacy",
      "original_abstract": "The demand private coded caching problem in a multi-access network with $K$\nusers and $K$ caches, where each user has access to $L$ neighbouring caches in\na cyclic wrap-around manner, is studied. The additional constraint imposed is\nthat one user should not get any information regarding the demands of the\nremaining users. A lifting construction of demand private multi-access coded\ncaching scheme from conventional, non-private multi-access scheme is\nintroduced. The demand-privacy for a user is ensured by placing some additional\n\\textit{keys} in a set of caches called the \\textit{private set} of that user.\nFor a given $K$ and $L$, a technique is also devised to find the private sets\nof the users.",
      "generated_abstract": "Title: Examination of Multi-Access Coded Caching in the Context of Demand Privacy\n\nRevised Abstract: The study explores the problem of demand private coded caching within a multi-access network comprising of $K$ users and $K$ caches, in which each user can access $L$ adjacent caches through a cyclical wrap-around approach. It is further stipulated that one user must not obtain any details pertaining to the demands of other users. A novel method of lifting construction is presented, allowing for a transition from a standard, non-private multi-access scheme to a demand private multi-access coded caching plan. To ensure the demand-privacy for each user, additional \\textit{keys} are strategically placed within a group of caches, termed as the \\textit{private set} for that user. Furthermore, a procedure has been developed to identify the private sets of the users, given the values of $K$ and $L$.",
      "original_id": "oai:arXiv.org:2107.00226",
      "created": "2021-07-01",
      "categories": [
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "Multilingual Central Repository: a Cross-lingual Framework for\n  Developing Wordnets",
      "original_abstract": "Language resources are necessary for language processing,but building them is\ncostly, involves many researches from different areas and needs constant\nupdating. In this paper, we describe the crosslingual framework used for\ndeveloping the Multilingual Central Repository (MCR), a multilingual knowledge\nbase that includes wordnets of Basque, Catalan, English, Galician, Portuguese,\nSpanish and the following ontologies: Base Concepts, Top Ontology, WordNet\nDomains and Suggested Upper Merged Ontology. We present the story of MCR, its\nstate in 2017 and the developed tools.",
      "generated_abstract": "Title: Development of a Multilingual Knowledge Base: A Cross-Lingual Approach for Wordnet Construction\n\nRewritten Abstract: The establishment of linguistic resources is integral to language processing, albeit being resource-intensive, requiring interdisciplinary research, and necessitating regular updates. This study details the cross-lingual strategy employed in constructing the Multilingual Central Repository (MCR), a comprehensive multilingual knowledge base, incorporating wordnets for languages such as Basque, Catalan, English, Galician, Portuguese, and Spanish. Additionally, the MCR includes ontologies such as Base Concepts, Top Ontology, WordNet Domains, and Suggested Upper Merged Ontology. This paper also charts the evolution of MCR, its status in 2017, and the tools developed for its construction.",
      "original_id": "oai:arXiv.org:2107.00333",
      "created": "2021-07-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Algorithms and Complexity for Counting Configurations in Steiner Triple\n  Systems",
      "original_abstract": "Steiner triple systems form one of the most studied classes of combinatorial\ndesigns. Configurations, including subsystems, play a central role in the\ninvestigation of Steiner triple systems. With sporadic instances of small\nsystems, ad-hoc algorithms for counting or listing configurations are typically\nfast enough for practical needs, but with many systems or large systems, the\nrelevance of computational complexity and algorithms of low complexity is\nhighlighted. General theoretical results as well as specific practical\nalgorithms for important configurations are presented.",
      "generated_abstract": "Title: Examination of Algorithms and Complexity in Configurations of Steiner Triple Systems\n\nRevised Abstract: The domain of Steiner triple systems, a widely scrutinized class of combinatorial designs, is profoundly influenced by configurations, including subsystems. The study of these systems often utilizes ad-hoc algorithms for enumerating configurations, which prove efficient for infrequent instances of small systems. However, the importance of computational complexity and low-complexity algorithms becomes evident when dealing with numerous or large-scale systems. This paper elucidates both broad theoretical findings and specific practical algorithms associated with critical configurations.",
      "original_id": "oai:arXiv.org:2110.00320",
      "created": "2021-10-01",
      "categories": [
        "math.CO"
      ]
    },
    {
      "title": "On the number of $q$-ary quasi-perfect codes with covering radius 2",
      "original_abstract": "In this paper we present a family of $q$-ary nonlinear quasi-perfect codes\nwith covering radius 2. The codes have length $n = q^m$ and size $ M = q^{n - m\n- 1}$ where $q$ is a prime power, $q \\geq 3$, $m$ is an integer, $m \\geq 2$. We\nprove that there are more than $q^{q^{cn}}$ nonequivalent such codes of length\n$n$, for all sufficiently large $n$ and a constant $c = \\frac{1}{q} -\n\\varepsilon$.",
      "generated_abstract": "Title: Regarding the Quantity of $q$-ary Quasi-Perfect Codes with a Covering Radius of 2\n\nRewritten Abstract: This study introduces a collection of $q$-ary nonlinear quasi-perfect codes that possess a covering radius of 2. These codes are defined by their length $n = q^m$ and their size $ M = q^{n - m - 1}$, where $q$ represents a prime power that is equal to or greater than 3, and $m$ is an integer equal to or exceeding 2. We establish that the quantity of these nonequivalent codes of length $n$ exceeds $q^{q^{cn}}$ for all adequately large values of $n$, given a constant $c = \\frac{1}{q} - \\varepsilon$.",
      "original_id": "oai:arXiv.org:2111.00774",
      "created": "2021-11-01",
      "categories": [
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "Collision Chains among the Terrestrial Planets. II. An Asymmetry between\n  Earth and Venus",
      "original_abstract": "During the late stage of terrestrial planet formation, hit-and-run collisions\nare about as common as accretionary mergers, for expected velocities and angles\nof giant impacts. Average hit-and-runs leave two major remnants plus debris:\nthe target and impactor, somewhat modified through erosion, escaping at lower\nrelative velocity. Here we continue our study of the dynamical effects of such\ncollisions. We compare the dynamical fates of intact runners that start from\nhit-and-runs with proto-Venus at 0.7 AU and proto-Earth at 1.0 AU. We follow\nthe orbital evolutions of the runners, including the other terrestrial planets,\nJupiter, and Saturn, in an N-body code. We find that the accretion of these\nrunners can take $\\gtrsim$10 Myr (depending on the egress velocity of the first\ncollision) and can involve successive collisions with the original target\nplanet or with other planets. We treat successive collisions that the runner\nexperiences using surrogate models from machine learning, as in previous work,\nand evolve subsequent hit-and-runs in a similar fashion. We identify\nasymmetries in the capture, loss, and interchange of runners in the growth of\nVenus and Earth. Hit-and-run is a more probable outcome at proto-Venus, being\nsmaller and faster orbiting than proto-Earth. But Venus acts as a sink,\neventually accreting most of its runners, assuming typical events, whereas\nproto-Earth loses about half, many of those continuing to Venus. This leads to\na disparity in the style of late-stage accretion that could have led to\nsignificant differences in geology, composition, and satellite formation at\nEarth and Venus.",
      "generated_abstract": "Title: Comparative Analysis of Collision Sequences Among Terrestrial Planets: Unequal Consequences for Earth and Venus \n\nRevised Abstract: The concluding stages of terrestrial planet formation observe a comparable frequency of hit-and-run collisions and accretionary mergers, considering the predicted velocities and impact angles of enormous collisions. On average, such collisions result in two primary remnants and associated debris - the target and the impactor, slightly reshaped due to erosion, escaping at a decreased relative velocity. This paper extends our investigation into the dynamic impacts of these collisions. We examine the dynamic outcomes of intact remnants originating from hit-and-run collisions involving proto-Venus at 0.7 AU and proto-Earth at 1.0 AU. By tracking the orbital evolutions of these remnants, and taking into account other terrestrial planets, Jupiter, and Saturn, using an N-body code, we discover that the accretion of these remnants can necessitate over 10 Myr, depending on the escape velocity of the initial collision. This process could include numerous collisions with the initial target planet or other planets. We use surrogate machine learning models to manage consecutive collisions experienced by the remnants, similar to previous studies, and progress subsequent hit-and-run events likewise. We expose disparities in the absorption, loss, and exchange of remnants during the growth of Venus and Earth. Hit-and-run is a more likely result at proto-Venus, due to its smaller size and quicker orbit compared to proto-Earth. However, Venus acts as a gravitational well, eventually accreting the majority of its remnants, given typical scenarios, while proto-Earth loses approximately half, many of which continue to Venus. This results in an imbalance in the late-stage accretion process, potentially leading to significant divergences in geology, composition, and satellite formation between Earth and Venus.",
      "original_id": "oai:arXiv.org:2110.00221",
      "created": "2021-10-01",
      "categories": [
        "astro-ph.EP"
      ]
    },
    {
      "title": "Synthetic Design: An Optimization Approach to Experimental Design with\n  Synthetic Controls",
      "original_abstract": "We investigate the optimal design of experimental studies that have\npre-treatment outcome data available. The average treatment effect is estimated\nas the difference between the weighted average outcomes of the treated and\ncontrol units. A number of commonly used approaches fit this formulation,\nincluding the difference-in-means estimator and a variety of synthetic-control\ntechniques. We propose several methods for choosing the set of treated units in\nconjunction with the weights. Observing the NP-hardness of the problem, we\nintroduce a mixed-integer programming formulation which selects both the\ntreatment and control sets and unit weightings. We prove that these proposed\napproaches lead to qualitatively different experimental units being selected\nfor treatment. We use simulations based on publicly available data from the US\nBureau of Labor Statistics that show improvements in terms of mean squared\nerror and statistical power when compared to simple and commonly used\nalternatives such as randomized trials.",
      "generated_abstract": "Title: Experimental Design Optimization: A Synthetic Control Approach \n\nRevised Abstract: This research probes the ideal construction of experimental studies that provide pre-treatment outcome data. The general treatment effect is gauged as the divergence between the weighted mean outcomes of the treated and control units. This formulation is consistent with several prevalent approaches, incorporating the difference-in-means estimator and various synthetic control methods. We introduce multiple strategies for determining the treated units in relation to the weights. Acknowledging the NP- hardness of the issue, we incorporate a mixed-integer programming formulation that simultaneously selects the treatment and control groups and their respective unit weightings. We demonstrate that our proposed methods result in the selection of experimentally distinct units for treatment. Using simulations drawn from publicly accessible data from the US Bureau of Labor Statistics, we illustrate enhancements in terms of mean squared error and statistical power when juxtaposed with basic and frequently employed alternatives such as randomized trials.",
      "original_id": "oai:arXiv.org:2112.00278",
      "created": "2021-12-01",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning",
      "original_abstract": "Offline reinforcement learning (RL) aims at learning a good policy from a\nbatch of collected data, without extra interactions with the environment during\ntraining. However, current offline RL benchmarks commonly have a large reality\ngap, because they involve large datasets collected by highly exploratory\npolicies, and the trained policy is directly evaluated in the environment. In\nreal-world situations, running a highly exploratory policy is prohibited to\nensure system safety, the data is commonly very limited, and a trained policy\nshould be well validated before deployment. In this paper, we present a near\nreal-world offline RL benchmark, named NeoRL, which contains datasets from\nvarious domains with controlled sizes, and extra test datasets for policy\nvalidation. We evaluate existing offline RL algorithms on NeoRL and argue that\nthe performance of a policy should also be compared with the deterministic\nversion of the behavior policy, instead of the dataset reward. The empirical\nresults demonstrate that the tested offline RL algorithms become less\ncompetitive to the deterministic policy on many datasets, and the offline\npolicy evaluation hardly helps. The NeoRL suit can be found at\nhttp://polixir.ai/research/neorl. We hope this work will shed some light on\nfuture research and draw more attention when deploying RL in real-world\nsystems.",
      "generated_abstract": "Title: NeoRL: Establishing a More Realistic Standard for Offline Reinforcement Learning\n\nRevised Abstract: Offline Reinforcement Learning (RL) is designed to learn an effective policy from an accumulated data set, without requiring additional interaction with the environment during the training phase. Nonetheless, the vast reality gap in current offline RL benchmarks is a significant issue due to their reliance on sizeable data collections gathered through highly exploratory policies, and the immediate evaluation of the trained policy in the environment. In real-world scenarios, the execution of highly exploratory policies is restricted to maintain system safety, data availability is typically limited, and it is crucial to thoroughly validate a trained policy prior to implementation. This study introduces NeoRL, a nearly real-world offline RL benchmark that encompasses controlled-size datasets from diverse domains and supplementary test datasets for policy validation. We assess existing offline RL algorithms on NeoRL and propose that policy performance should be juxtaposed with the deterministic version of the behavior policy, rather than the dataset reward. Our empirical findings indicate that the assessed offline RL algorithms are less competitive compared to the deterministic policy across numerous datasets, and offline policy evaluation offers little assistance. The NeoRL suite can be accessed at http://polixir.ai/research/neorl. We anticipate that our research will encourage further investigation and increased vigilance in the implementation of RL in real-world systems.",
      "original_id": "oai:arXiv.org:2102.00714",
      "created": "2021-02-01",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Holonomic functions and prehomogeneous spaces",
      "original_abstract": "A function that is analytic on a domain of $\\mathbb{C}^n$ is holonomic if it\nis the solution to a holonomic system of linear homogeneous differential\nequations with polynomial coefficients. We define and study the Bernstein-Sato\npolynomial of a holonomic function on a smooth algebraic variety. We analyze\nthe structure of certain sheaves of holonomic functions, such as the algebraic\nfunctions along a hypersurface, determining their direct sum decompositions\ninto indecomposables, that further respect decompositions of Bernstein-Sato\npolynomials. When the space is endowed with the action of a linear algebraic\ngroup $G$, we study the class of $G$-finite analytic functions, i.e. functions\nthat under the action of the Lie algebra of $G$ generate a finite dimensional\nrational $G$-module. These are automatically algebraic functions on a variety\nwith a dense orbit. When $G$ is reductive, we give several\nrepresentation-theoretic techniques toward the determination of Bernstein-Sato\npolynomials of $G$-finite functions. We classify the $G$-finite functions on\nall but one of the irreducible reduced prehomogeneous vector spaces, and\ncompute the Bernstein-Sato polynomials for distinguished $G$-finite functions.\nThe results can be used to construct explicitly equivariant\n$\\mathcal{D}$-modules.",
      "generated_abstract": "Title: Examination of Holonomic Functions and Prehomogeneous Spaces\n\nIn this research, we explore the concept of a holonomic function as a solution to a holonomic system of linear homogeneous differential equations with polynomial coefficients, within a domain of $\\mathbb{C}^n$ where the function is analytic. We delve into the Bernstein-Sato polynomial of a holonomic function situated on a smooth algebraic variety, elucidating its definition and properties. The study further scrutinizes the architecture of specific sheaves of holonomic functions, such as the algebraic functions present on a hypersurface, with an emphasis on the delineation of their direct sum decompositions into indecomposables that adhere to the decomposition of Bernstein-Sato polynomials. In spaces where a linear algebraic group $G$ is operational, we investigate the category of $G$-finite analytic functions - functions that generate a finite dimensional rational $G$-module under the influence of $G$'s Lie algebra. These functions are inherently algebraic on a variety with a dense orbit. In reductive $G$ instances, we present several representation-theoretic methods to identify Bernstein-Sato polynomials of $G$-finite functions. We categorize the $G$-finite functions on nearly all irreducible reduced prehomogeneous vector spaces, besides one, and compute Bernstein-Sato polynomials for notable $G$-finite functions. The findings offer a foundation to construct unequivocally equivariant $\\mathcal{D}$-modules.",
      "original_id": "oai:arXiv.org:2102.00766",
      "created": "2021-02-01",
      "categories": [
        "math.AG",
        "math.RT"
      ]
    },
    {
      "title": "Combating small molecule aggregation with machine learning",
      "original_abstract": "Biological screens are plagued by false positive hits resulting from\naggregation. Thus, methods to triage small colloidally aggregating molecules\n(SCAMs) are in high demand. Herein, we disclose a bespoke machine-learning tool\nto confidently and intelligibly flag such entities. Our data demonstrate an\nunprecedented utility of machine learning for predicting SCAMs, achieving 80%\nof correct predictions in a challenging out-of-sample validation. The tool\noutperformed a panel of expert chemists, who correctly predicted 61 +/- 7% of\nthe same test molecules in a Turing-like test. Further, the computational\nroutine provided insight into molecular features governing aggregation that had\nremained hidden to expert intuition. Leveraging our tool, we quantify that up\nto 15-20% of ligands in publicly available chemogenomic databases have the high\npotential to aggregate at typical screening concentrations, imposing caution in\nsystems biology and drug design programs. Our approach provides a means to\naugment human intuition, mitigate attrition and a pathway to accelerate future\nmolecular medicine.",
      "generated_abstract": "Title: Addressing the Issue of Small Molecule Aggregation Through Machine Learning\n\nRevised Abstract: The occurrence of false positives in biological screenings due to aggregation has become a significant concern. Therefore, there is a pressing need for tools that can effectively identify small colloidally aggregating molecules (SCAMs). In this study, we introduce a specialized machine-learning tool, designed to accurately and comprehensibly detect these aggregating entities. The data presented showcases the exceptional capability of machine learning in predicting SCAMs, with an 80% accuracy rate in a demanding out-of-sample validation. This tool surpassed the performance of a group of expert chemists who managed to correctly predict only 61 +/- 7% of the same test molecules in a comparable Turing-like test. Moreover, the computational method offered insights into molecular features that influence aggregation, which were previously undetectable by expert intuition. Utilizing this tool, we deduced that approximately 15-20% of ligands in publicly accessible chemogenomic databases exhibit a high propensity to aggregate at usual screening concentrations, indicating the need for caution in systems biology and drug design projects. Our methodology offers a solution to enhance human intuition, reduce attrition, and pave the way for expediting future molecular medicine.",
      "original_id": "oai:arXiv.org:2105.00267",
      "created": "2021-05-01",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ]
    },
    {
      "title": "Generalized torsion for hyperbolic $3$--manifold groups with arbitrary\n  large rank",
      "original_abstract": "Let $G$ be a group and $g$ a non-trivial element in $G$. If some non-empty\nfinite product of conjugates of $g$ equals to the trivial element, then $g$ is\ncalled a generalized torsion element. To the best of our knowledge, we have no\nhyperbolic $3$--manifold groups with generalized torsion elements whose rank is\nexplicitly known to be greater than two. The aim of this short note is to\ndemonstrate that for a given integer $n > 1$ there are infinitely many closed\nhyperbolic $3$--manifolds $M_n$ which enjoy the property: (i) the Heegaard\ngenus of $M_n$ is $n$, (ii) the rank of the fundamental group of $M_n$ is $n$,\nand (ii) the fundamental group of $M_n$ has a generalized torsion element.\nFurthermore, we may choose $M_n$ as homology lens spaces and so that the order\nof the generalized torsion element is arbitrarily large.",
      "generated_abstract": "Title: The Extended Scope of Torsion in Hyperbolic 3-Manifold Groups of Indefinite Large Rank\n\nRevised Abstract: Suppose that $G$ is a group and a non-trivial element, $g$, resides in $G$. The element $g$ is classified as a generalized torsion element if any finite non-null product of its conjugates equals the trivial element. To date, our understanding lacks any hyperbolic 3-manifold groups containing generalized torsion elements with a rank known to exceed two. This brief communication endeavors to prove that for any given integer $n > 1$, there exists an infinite number of closed hyperbolic 3-manifolds $M_n$ that possess the following characteristics: (i) the Heegaard genus of $M_n$ equals $n$, (ii) the fundamental group of $M_n$ has a rank of $n$, and (iii) the fundamental group of $M_n$ includes a generalized torsion element. Additionally, we can select $M_n$ as homology lens spaces, and the generalized torsion element's order can be chosen to be arbitrarily large.",
      "original_id": "oai:arXiv.org:2112.00418",
      "created": "2021-12-01",
      "categories": [
        "math.GT",
        "math.GR"
      ]
    },
    {
      "title": "Improving Automatic Hate Speech Detection with Multiword Expression\n  Features",
      "original_abstract": "The task of automatically detecting hate speech in social media is gaining\nmore and more attention. Given the enormous volume of content posted daily,\nhuman monitoring of hate speech is unfeasible. In this work, we propose new\nword-level features for automatic hate speech detection (HSD): multiword\nexpressions (MWEs). MWEs are lexical units greater than a word that have\nidiomatic and compositional meanings. We propose to integrate MWE features in a\ndeep neural network-based HSD framework. Our baseline HSD system relies on\nUniversal Sentence Encoder (USE). To incorporate MWE features, we create a\nthree-branch deep neural network: one branch for USE, one for MWE categories,\nand one for MWE embeddings. We conduct experiments on two hate speech tweet\ncorpora with different MWE categories and with two types of MWE embeddings,\nword2vec and BERT. Our experiments demonstrate that the proposed HSD system\nwith MWE features significantly outperforms the baseline system in terms of\nmacro-F1.",
      "generated_abstract": "Title: Enhancement of Automated Hate Speech Identification through the Utilization of Multiword Expression Characteristics\n\nRevised Abstract: The growing focus on the automatic identification of hate speech in social media platforms necessitates more effective detection techniques. Given the impracticality of manually monitoring the sheer volume of content shared daily, we suggest the introduction of novel word-level attributes for automated hate speech identification (HSD): multiword expressions (MWEs). These are lexical entities that exceed a single word and carry both idiomatic and compositional implications. We advocate for the incorporation of these MWE elements into a deep learning-based HSD structure. Our initial HSD system is dependent on Universal Sentence Encoder (USE). To implement MWE characteristics, we establish a tripartite deep learning network: one segment for USE, one for MWE types, and one for MWE embeddings. We carry out tests on two separate hate speech Twitter datasets with varying MWE types and two variants of MWE embeddings - word2vec and BERT. Our studies reveal that the suggested HSD mechanism, outfitted with MWE characteristics, notably surpasses the original system in macro-F1 performance.",
      "original_id": "oai:arXiv.org:2106.00237",
      "created": "2021-06-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Multi-Messenger studies with the Pierre Auger Observatory",
      "original_abstract": "Over the past decade the multi-messenger astrophysics has emerged as a\ndistinct discipline, providing unique insights into the properties of\nhigh-energy phenomena in the Universe. The Pierre Auger Observatory, located in\nMalarg\\\"ue, Argentina, is the world's largest cosmic ray detector sensitive to\nphotons, neutrinos, and hadrons at ultra-high energies. Using its data,\nstringent limits on photon and neutrino fluxes at EeV energies have been\nobtained. The collaboration uses the excellent angular resolution and the\nneutrino identification capabilities of the Observatory for follow-up studies\nof events detected in gravitational waves or other messengers, through\ncooperation with global multi-messenger networks. We present a science\nmotivation together with an overview of the multi-messenger capabilities and\nresults of the Pierre Auger Observatory.",
      "generated_abstract": "Title: Comprehensive Analysis Utilizing the Pierre Auger Observatory in Multi-Messenger Studies \n\nIn the recent decade, multi-messenger astrophysics has evolved into a distinct field, offering unparalleled perspectives into the characteristics of high-energy cosmic events. Situated in Malarg\u00fce, Argentina, the Pierre Auger Observatory stands as the most extensive cosmic ray detector globally, with the capacity to detect photons, neutrinos, and hadrons at extreme energies. The application of its data has led to the establishment of strict boundaries on photon and neutrino fluxes at EeV energies. The collaborative team harnesses the exceptional angular resolution and neutrino identification abilities of the Observatory to conduct supplementary studies of incidents detected in gravitational waves or alternate messengers, through alliances with international multi-messenger networks. In this paper, we provide the scientific rationale, a comprehensive review of the multi-messenger capacities, and the resultant findings of the Pierre Auger Observatory.",
      "original_id": "oai:arXiv.org:2102.00828",
      "created": "2021-02-01",
      "categories": [
        "astro-ph.HE"
      ]
    },
    {
      "title": "A New Tool for Efficiently Generating Quality Estimation Datasets",
      "original_abstract": "Building of data for quality estimation (QE) training is expensive and\nrequires significant human labor. In this study, we focus on a data-centric\napproach while performing QE, and subsequently propose a fully automatic\npseudo-QE dataset generation tool that generates QE datasets by receiving only\nmonolingual or parallel corpus as the input. Consequently, the QE performance\nis enhanced either by data augmentation or by encouraging multiple language\npairs to exploit the applicability of QE. Further, we intend to publicly\nrelease this user friendly QE dataset generation tool as we believe this tool\nprovides a new, inexpensive method to the community for developing QE datasets.",
      "generated_abstract": "Title: An Innovative Approach to Cost-Effective Creation of Quality Estimation Datasets\n\nGenerated Abstract: The process of creating datasets for quality estimation (QE) training is both time-consuming and costly, necessitating a substantial amount of human effort. Our research concentrates on a method driven by data during the QE process, subsequently introducing a fully automated tool for generating pseudo-QE datasets. This tool operates by accepting either a monolingual or parallel corpus as its sole input, thereby generating QE datasets. As a result, the QE performance is improved, either through data augmentation or by enabling multiple language pairs to utilize the potential of QE. We plan to make this user-friendly QE dataset creation tool available to the public, as we are confident it offers the academic community a novel and economical technique for the creation of QE datasets.",
      "original_id": "oai:arXiv.org:2111.00767",
      "created": "2021-11-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "VA-GCN: A Vector Attention Graph Convolution Network for learning on\n  Point Clouds",
      "original_abstract": "Owing to the development of research on local aggregation operators, dramatic\nbreakthrough has been made in point cloud analysis models. However, existing\nlocal aggregation operators in the current literature fail to attach decent\nimportance to the local information of the point cloud, which limits the power\nof the models. To fit this gap, we propose an efficient Vector Attention\nConvolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract\nthe neighbor points of each input point, and then uses the elevation and\nazimuth relationship of the vectors between the center point and its neighbors\nto construct an attention weight matrix for edge features. Afterwards, the\nVAConv adopts a dual-channel structure to fuse weighted edge features and\nglobal features. To verify the efficiency of the VAConv, we connect the VAConvs\nwith different receptive fields in parallel to obtain a Multi-scale graph\nconvolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art\nperformance on standard benchmarks including ModelNet40, S3DIS and ShapeNet.\nRemarkably, on the ModelNet40 dataset for 3D classification, VA-GCN increased\nby 2.4% compared to the baseline.",
      "generated_abstract": "Title: VA-GCN: An Innovative Vector Attention Graph Convolution Network for Point Cloud Analysis\n\nNewly Revised Abstract: The advancement in local aggregation operators has significantly revolutionized point cloud analysis models. Despite this progress, current local aggregation operators do not sufficiently emphasize the local information within the point cloud, thereby limiting the effectiveness of the models. To address this issue, we introduce a robust Vector Attention Convolution module (VAConv) that employs K-Nearest Neighbor (KNN) to identify the neighboring points of each input point. The VAConv then determines the attention weight matrix for edge features based on the elevation and azimuth relationship of vectors between a central point and its neighboring points. The VAConv further enhances its efficacy by using a dual-channel structure to amalgamate weighted edge features and global features. To assess the potency of the VAConv, we amalgamate the VAConvs with varying receptive fields in parallel to create a Multi-scale graph convolutional network, coined VA-GCN. The cutting-edge VA-GCN outperforms other models on standard benchmarks such as ModelNet40, S3DIS, and ShapeNet. Notably, the VA-GCN brought about an impressive 2.4% improvement compared to the baseline on the ModelNet40 dataset for 3D classification.",
      "original_id": "oai:arXiv.org:2106.00227",
      "created": "2021-06-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Global existence and boundedness in a fully parabolic\n  attraction-repulsion chemotaxis system with signal-dependent sensitivities\n  without logistic source",
      "original_abstract": "This paper deals with the fully parabolic attraction-repulsion chemotaxis\nsystem with signal-dependent sensitivities, \\begin{align*} \\begin{cases}\n  u_t=\\Delta u-\\nabla \\cdot (u\\chi(v)\\nabla v)\n  +\\nabla \\cdot (u\\xi(w)\\nabla w),\n  &x \\in \\Omega,\\ t>0,\\\\[1.05mm]\n  v_t=\\Delta v-v+u,\n  &x \\in \\Omega,\\ t>0,\\\\[1.05mm]\n  w_t=\\Delta w-w+u,\n  &x \\in \\Omega,\\ t>0 \\end{cases} \\end{align*} under homogeneous Neumann\nboundary conditions and initial conditions, where $\\Omega \\subset \\mathbb{R}^n$\n$(n \\ge 2)$ is a bounded domain with smooth boundary, $\\chi, \\xi$ are functions\nsatisfying some conditions. Global existence and boundedness of classical\nsolutions to the system with logistic source have already been obtained by\ntaking advantage of the effect of logistic dampening (J. Math. Anal. Appl.;\n2020;489;124153). This paper establishes existence of global bounded classical\nsolutions despite the loss of logistic dampening.",
      "generated_abstract": "Title: Worldwide Presence and Limitations in a Fully Parabolic Attraction-Repulsion Chemotaxis System with Signal-Dependent Sensitivities Absent of Logistic Source\n\nRemodelled Abstract: This research investigates the fully parabolic attraction-repulsion chemotaxis system, which incorporates signal-dependent sensitivities, represented by \\begin{align*} \\begin{cases}\n  u_t=\\Delta u-\\nabla \\cdot (u\\chi(v)\\nabla v)\n  +\\nabla \\cdot (u\\xi(w)\\nabla w),\n  &x \\in \\Omega,\\ t>0,\\\\[1.05mm]\n  v_t=\\Delta v-v+u,\n  &x \\in \\Omega,\\ t>0,\\\\[1.05mm]\n  w_t=\\Delta w-w+u,\n  &x \\in \\Omega,\\ t>0 \\end{cases} \\end{align*}, under the same homogeneous Neumann boundary conditions and initial conditions. Here, $\\Omega \\subset \\mathbb{R}^n$ $(n \\ge 2)$ is a restricted domain with a smooth boundary, and $\\chi, \\xi$ are functions that adhere to specific conditions. The global existence and the limitations of the traditional solutions to the system with a logistic source have previously been identified by leveraging the effect of logistic dampening (J. Math. Anal. Appl.; 2020;489;124153). This study confirms the existence of global, limited, traditional solutions, even in the absence of logistic dampening.",
      "original_id": "oai:arXiv.org:2104.00381",
      "created": "2021-04-01",
      "categories": [
        "math.AP"
      ]
    },
    {
      "title": "AAPM DL-Sparse-View CT Challenge Submission Report: Designing an\n  Iterative Network for Fanbeam-CT with Unknown Geometry",
      "original_abstract": "This report is dedicated to a short motivation and description of our\ncontribution to the AAPM DL-Sparse-View CT Challenge (team name:\n\"robust-and-stable\"). The task is to recover breast model phantom images from\nlimited view fanbeam measurements using data-driven reconstruction techniques.\nThe challenge is distinctive in the sense that participants are provided with a\ncollection of ground truth images and their noiseless, subsampled sinograms (as\nwell as the associated limited view filtered backprojection images), but not\nwith the actual forward model. Therefore, our approach first estimates the\nfanbeam geometry in a data-driven geometric calibration step. In a subsequent\ntwo-step procedure, we design an iterative end-to-end network that enables the\ncomputation of near-exact solutions.",
      "generated_abstract": "Title: AAPM DL-Sparse-View CT Challenge Submission Report: Development of an Iterative Network for Fanbeam-CT with Undefined Geometry\n\nRevised Abstract: This paper presents a concise motivation and outline of our submission to the AAPM DL-Sparse-View CT Challenge, represented by the team \"robust-and-stable\". Our objective is to reconstruct breast model phantom images from limited view fanbeam measurements utilizing data-oriented reconstruction methodologies. The competition is unique as it provides participants with a series of reference images and their noise-free, undersampled sinograms, alongside the associated limited view filtered backprojection images, but omits the actual forward model. Consequently, our method initiates with the estimation of the fanbeam geometry through a data-driven geometric calibration stage. Following this, we devise an iterative end-to-end network in a two-step process that facilitates the calculation of near-precise solutions.",
      "original_id": "oai:arXiv.org:2106.00280",
      "created": "2021-06-01",
      "categories": [
        "cs.LG",
        "cs.NA",
        "eess.IV",
        "math.NA",
        "physics.med-ph"
      ]
    },
    {
      "title": "Algebraic perspectives on signomial optimization",
      "original_abstract": "Signomials are obtained by generalizing polynomials to allow for arbitrary\nreal exponents. This generalization offers great expressive power, but has\nhistorically sacrificed the organizing principle of ``degree'' that is central\nto polynomial optimization theory. We reclaim that principle here through the\nconcept of signomial rings, which we use to derive complete convex relaxation\nhierarchies of upper and lower bounds for signomial optimization via sums of\narithmetic-geometric exponentials (SAGE) nonnegativity certificates. The\nPositivstellensatz underlying the lower bounds relies on the concept of\nconditional SAGE and removes regularity conditions required by earlier works,\nsuch as convexity and Archimedeanity of the feasible set. Through worked\nexamples we illustrate the practicality of this hierarchy in areas such as\nchemical reaction network theory and chemical engineering. These examples\ninclude comparisons to direct global solvers (e.g., BARON and ANTIGONE) and the\nLasserre hierarchy (where appropriate). The completeness of our hierarchy of\nupper bounds follows from a generic construction whereby a Positivstellensatz\nfor signomial nonnegativity over a compact set provides for arbitrarily strong\nouter approximations of the corresponding cone of nonnegative signomials. While\nworking toward that result, we prove basic facts on the existence and\nuniqueness of solutions to signomial moment problems.",
      "generated_abstract": "Title: An Algebraic Examination of Signomial Optimization \n\nGenerated Abstract: By expanding the scope of polynomials to encompass any real exponents, we arrive at signomials. Such expansion yields a substantial increase in expressive power, albeit at the expense of the pivotal principle of \"degree\" that is integral to polynomial optimization theory. In this study, we reintroduce this principle by exploiting the concept of signomial rings. This approach facilitates the derivation of comprehensive convex relaxation hierarchies for both upper and lower bounds in signomial optimization, using sums of arithmetic-geometric exponentials (SAGE) nonnegativity certificates. The Positivstellensatz forming the basis for the lower bounds hinges on the concept of conditional SAGE, thereby eliminating the regularity conditions mandated by previous studies, such as the convexity and Archimedeanity of the feasible set. The practical application of this hierarchy is demonstrated through examples drawn from fields like chemical reaction network theory and chemical engineering. These examples are compared to direct global solvers (e.g., BARON and ANTIGONE) and the Lasserre hierarchy (where applicable). A generic construction ensures the comprehensive nature of our hierarchy of upper bounds, as a Positivstellensatz for signomial nonnegativity over a compact set allows for increasingly accurate outer approximations of the respective cone of nonnegative signomials. In the process of achieving this result, we establish basic truths concerning the existence and uniqueness of solutions to signomial moment problems.",
      "original_id": "oai:arXiv.org:2107.00345",
      "created": "2021-07-01",
      "categories": [
        "math.AG",
        "math.OC"
      ]
    },
    {
      "title": "The Comprehensive Blub Archive Network: Towards Design Principals for\n  Open Source Programming Language Repositories",
      "original_abstract": "Many popular open source programming languages (Perl, Ruby or Python for\nexample) have systems for distributing packaged source code that software\ndevelopers can use when working in that particular programming language. This\npaper will consider the design principals that should be followed if designing\nsuch an open source code repository.",
      "generated_abstract": "Title: A Holistic Examination of the Blub Archive Network: Proposing Design Guidelines for Open Source Programming Language Libraries \n\nRevised Abstract: Numerous widely-used open source programming languages, such as Perl, Ruby, or Python, incorporate systems that distribute packaged source code, which software developers leverage while operating within that specific programming language. This study intends to deliberate the essential design guidelines that ought to be observed in the creation of such open source code libraries.",
      "original_id": "oai:arXiv.org:2104.00378",
      "created": "2021-04-01",
      "categories": [
        "cs.PL"
      ]
    },
    {
      "title": "MeanShift++: Extremely Fast Mode-Seeking With Applications to\n  Segmentation and Object Tracking",
      "original_abstract": "MeanShift is a popular mode-seeking clustering algorithm used in a wide range\nof applications in machine learning. However, it is known to be prohibitively\nslow, with quadratic runtime per iteration. We propose MeanShift++, an\nextremely fast mode-seeking algorithm based on MeanShift that uses a grid-based\napproach to speed up the mean shift step, replacing the computationally\nexpensive neighbors search with a density-weighted mean of adjacent grid cells.\nIn addition, we show that this grid-based technique for density estimation\ncomes with theoretical guarantees. The runtime is linear in the number of\npoints and exponential in dimension, which makes MeanShift++ ideal on\nlow-dimensional applications such as image segmentation and object tracking. We\nprovide extensive experimental analysis showing that MeanShift++ can be more\nthan 10,000x faster than MeanShift with competitive clustering results on\nbenchmark datasets and nearly identical image segmentations as MeanShift.\nFinally, we show promising results for object tracking.",
      "generated_abstract": "Title: MeanShift++: Advanced Fast Mode-Seeking Algorithm with Relevance to Segmentation and Object Tracking \n\nNew Abstract: MeanShift is a widely adopted clustering algorithm for mode-seeking that is employed extensively in various machine learning applications. The algorithm, however, is recognized to be exceedingly slow, characterized by a quadratic runtime for each iteration. In this paper, we present MeanShift++, an enhanced fast mode-seeking algorithm that is built on the foundations of MeanShift. It employs a grid-based methodology to expedite the mean shift step, substituting the computationally demanding neighbors search with a density-weighted average of adjacent grid cells. We provide evidence that this grid-based density estimation method is supported by theoretical assurances. MeanShift++ has a runtime that scales linearly with the number of points and exponentially with the dimension, making it ideally suited for applications with low dimensions, such as image segmentation and object tracking. We offer a comprehensive experimental analysis demonstrating that MeanShift++ can achieve speeds that are over 10,000 times faster than MeanShift, while still producing competitive clustering results on benchmark datasets and generating image segmentations almost identical to those of MeanShift. We conclude by presenting encouraging outcomes for object tracking.",
      "original_id": "oai:arXiv.org:2104.00303",
      "created": "2021-04-01",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Mean-field BDSDEs and associated nonlocal semi-linear backward\n  stochastic partial differential equations",
      "original_abstract": "In this paper we investigate mean-field backward doubly stochastic\ndifferential equations (BDSDEs), i.e., BDSDEs whose driving coefficients also\ndepend on the joint law of the solution process as well as the solution of an\nassociated mean-field forward SDE. Unlike the pioneering paper on BDSDEs by\nPardoux-Peng (1994), we handle a driving coefficient in the backward integral\nof the BDSDE for which the Lipschitz assumption w.r.t. the law of the solution\nis sufficient, without assuming that this Lipschitz constant is small enough.\nOn the other hand, as the parameters $(x,P_\\xi)$ and $(x,P_\\xi,y)$ run an\ninfinite-dimensional space, unlike Pardoux and Peng, we cannot apply\nKolmogorov's continuity criterion to the value function\n$V(t,x,P_{\\xi}):=Y_t^{t,x,P_{\\xi}}$, while in the classical case studied in\nPardoux-Peng the value function $V(t,x)=Y_t^{t,x}$ can be shown to be of class\n$C^{1,2}([0,T]\\times\\mathbb{R}^d)$, we have for our value function\n$V(t,x,P_{\\xi})$ and its derivative $\\partial_\\mu V(t,x,P_{\\xi},y)$ only the\n$L^2$-differentiability with respect to $x$ and $y$, respectively. Using a new\nmethod we prove the characterization of $V=(V(t,x,P_{\\xi}))$ as the unique\nsolution of the associated mean-field backward stochastic PDE.",
      "generated_abstract": "Title: Investigation of Mean-field BDSDEs and Related Semi-linear Backward Stochastic Partial Differential Equations\n\nNew Abstract: This study delves into the exploration of mean-field backward doubly stochastic differential equations (BDSDEs), specifically those BDSDEs where the driving coefficients are influenced by both the combined law of the solution process and the solution of a related mean-field forward SDE. In contrast to the foundational paper on BDSDEs by Pardoux-Peng (1994), we manage a driving coefficient in the backward integral of the BDSDE that is sufficiently explained by the Lipschitz assumption concerning the law of the solution, without requiring the Lipschitz constant to be minimal. On the contrary, while the parameters $(x,P_\\xi)$ and $(x,P_\\xi,y)$ traverse an infinite-dimensional space, differing from Pardoux and Peng, we are unable to apply Kolmogorov's continuity criterion to the value function $V(t,x,P_{\\xi}):=Y_t^{t,x,P_{\\xi}}$. In the conventional case examined in Pardoux-Peng where the value function $V(t,x)=Y_t^{t,x}$ is demonstrated to be of class $C^{1,2}([0,T]\\times\\mathbb{R}^d)$, our value function $V(t,x,P_{\\xi})$ and its derivative $\\partial_\\mu V(t,x,P_{\\xi},y)$ display only $L^2$-differentiability with respect to $x$ and $y$. Utilizing a novel method, we demonstrate the characterization of $V=(V(t,x,P_{\\xi}))$ as the singular solution of the related mean-field backward stochastic PDE.",
      "original_id": "oai:arXiv.org:2111.00759",
      "created": "2021-11-01",
      "categories": [
        "math.PR"
      ]
    },
    {
      "title": "Short wavelength infrared avalanche photodetector using Sb-based\n  strained layer superlattice",
      "original_abstract": "We demonstrate a low noise short wavelength infrared (SWIR) Sb based type II\nsuperlattice (T2SL) avalanche photodiodes (APD). The SWIR GaSb/(AlAsSb/GaSb)\nAPD structure was designed based on impact ionization engineering and grown by\nmolecular beam epitaxy on GaSb substrate. At room temperature, the device\nexhibits a 50 % cut-off wavelength of 1.74 micron. The device revealed to have\nelectron dominated avalanching mechanism with a gain value of 48 at room\ntemperature. The electron and hole impact ionization coefficients were\ncalculated and compared to give better prospect of the performance of the\ndevice. Low excess noise, as characterized by the carrier ionization ratio of ~\n0.07, has been achieved.",
      "generated_abstract": "Title: An Sb-Based Strained Layer Superlattice Utilized in Short Wavelength Infrared Avalanche Photodetectors\n\nGenerated Abstract: This research presents a short wavelength infrared (SWIR) avalanche photodiode (APD) with low noise, utilizing an Sb-based type II superlattice (T2SL). The SWIR APD, consisting of GaSb/(AlAsSb/GaSb), was designed utilizing impact ionization engineering and was developed on a GaSb substrate through molecular beam epitaxy. Under room conditions, the device demonstrated a 50% cut-off wavelength at 1.74 microns. The analysis revealed that the device is dominated by an electron avalanching mechanism, with a gain value of 48 at room temperature. The comparison of calculated electron and hole impact ionization coefficients provided a more in-depth understanding of the device's performance. The device achieved a low excess noise, marked by a carrier ionization ratio of approximately 0.07.",
      "original_id": "oai:arXiv.org:2104.00251",
      "created": "2021-04-01",
      "categories": [
        "physics.app-ph"
      ]
    },
    {
      "title": "Geometric Control for Load Transportation with Quadrotor UAVs by Elastic\n  Cables",
      "original_abstract": "Groups of unmanned aerial vehicles (UAVs) are increasingly utilized in\ntransportation task as the combined strength allows to increase the maximum\npayload. However, the resulting mechanical coupling of the UAVs impose new\nchallenges in terms of the tracking control. Thus, we design a geometric\ntrajectory tracking controller for the cooperative task of four quadrotor UAVs\ncarrying and transporting a rigid body, which is attached to the quadrotors via\ninflexible elastic cables. The elasticity of the cables together with\ntechniques of singular perturbation allows a reduction in the model to that of\na similar model with inelastic cables. In this reduced model, we design a\ncontroller such that the position and attitude of the load exponentially\nconverges to a given desired trajectory. We then show that this result leads to\nan uniformly converging tracking error for the original elastic model under\nsome assumptions. Furthermore, under the presence of unstructured disturbances\non the system, we show that the error is ultimately bounded with an arbitrarily\nsmall bound. Finally, a simulation illustrates the theoretical results.",
      "generated_abstract": "Title: Load Transportation via Quadrotor UAVs: A Study on Geometric Control with Rigid Elastic Cables\n\nRevised Abstract: The collective power of unmanned aerial vehicle (UAV) groups is increasingly being harnessed for transportation tasks, enhancing their maximum payload capacity. However, this mechanical interconnection introduces new complexities in tracking control. As a solution, we have developed a geometric trajectory tracking controller for a team of four quadrotor UAVs tasked with transporting a rigid object, connected to the UAVs through non-flexible elastic cables. By combining the cable's elasticity with singular perturbation techniques, the model is simplified to resemble one using inelastic cables. Within this simplified model, a controller is designed, ensuring both the position and orientation of the load converges exponentially to a pre-determined trajectory. Our research demonstrates that this outcome prompts a uniformly converging tracking error in the initial elastic model, given certain presumptions. Moreover, even in the presence of unstructured system disturbances, the error remains within an arbitrarily small range. Theoretical findings are further substantiated through simulation.",
      "original_id": "oai:arXiv.org:2111.00777",
      "created": "2021-11-01",
      "categories": [
        "math.OC",
        "cs.SY",
        "eess.SY"
      ]
    },
    {
      "title": "Edge-competing Pathological Liver Vessel Segmentation with Limited\n  Labels",
      "original_abstract": "The microvascular invasion (MVI) is a major prognostic factor in\nhepatocellular carcinoma, which is one of the malignant tumors with the highest\nmortality rate. The diagnosis of MVI needs discovering the vessels that contain\nhepatocellular carcinoma cells and counting their number in each vessel, which\ndepends heavily on experiences of the doctor, is largely subjective and\ntime-consuming. However, there is no algorithm as yet tailored for the MVI\ndetection from pathological images. This paper collects the first pathological\nliver image dataset containing 522 whole slide images with labels of vessels,\nMVI, and hepatocellular carcinoma grades. The first and essential step for the\nautomatic diagnosis of MVI is the accurate segmentation of vessels. The unique\ncharacteristics of pathological liver images, such as super-large size,\nmulti-scale vessel, and blurred vessel edges, make the accurate vessel\nsegmentation challenging. Based on the collected dataset, we propose an\nEdge-competing Vessel Segmentation Network (EVS-Net), which contains a\nsegmentation network and two edge segmentation discriminators. The segmentation\nnetwork, combined with an edge-aware self-supervision mechanism, is devised to\nconduct vessel segmentation with limited labeled patches. Meanwhile, two\ndiscriminators are introduced to distinguish whether the segmented vessel and\nbackground contain residual features in an adversarial manner. In the training\nstage, two discriminators are devised tocompete for the predicted position of\nedges. Exhaustive experiments demonstrate that, with only limited labeled\npatches, EVS-Net achieves a close performance of fully supervised methods,\nwhich provides a convenient tool for the pathological liver vessel\nsegmentation. Code is publicly available at\nhttps://github.com/zju-vipa/EVS-Net.",
      "generated_abstract": "Title: Advanced Pathological Liver Vessel Segmentation via Edge-Competing Techniques with Limited Labels \n\nGenerated Abstract: Hepatocellular carcinoma, a highly lethal malignancy, has microvascular invasion (MVI) as a significant determinant of prognosis. The identification and enumeration of vessels containing hepatocellular carcinoma cells, a crucial part of MVI diagnosis, is primarily reliant on physician expertise, leading to subjectivity and lengthy processes. To date, no algorithm specifically designed for MVI detection from pathological images exists. This study presents a novel dataset of 522 pathological liver images featuring whole slide images labelled with vessel, MVI, and hepatocellular carcinoma grade information. The inaugural step to automate MVI diagnosis entails accurate vessel segmentation, a task complicated by the pathological liver images' unique elements such as immense size, multi-scale vessels, and ambiguous vessel edges. Using the newly collected dataset, we introduce an Edge-competing Vessel Segmentation Network (EVS-Net), comprising a segmentation network and two edge segmentation discriminators. The segmentation network, enhanced with an edge-aware self-supervision mechanism, performs vessel segmentation using limited labeled patches. Simultaneously, two discriminators are employed to adversarially determine if the segmented vessel and background retain residual features. During the training phase, two discriminators are designed to contest the predicted edge locations. Comprehensive experiments affirm that EVS-Net, despite relying on limited labeled patches, delivers comparable results to fully supervised methods, thereby offering a practical tool for pathological liver vessel segmentation. The code can be accessed at https://github.com/zju-vipa/EVS-Net.",
      "original_id": "oai:arXiv.org:2108.00384",
      "created": "2021-08-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Direct measurement of the 13C({\\alpha},n)16O cross section into the\n  s-process Gamow peak",
      "original_abstract": "One of the main neutron sources for the astrophysical s-process is the\nreaction 13C({\\alpha},n)16O, taking place in thermally pulsing Asymptotic Giant\nBranch stars at temperatures around 90 MK. To model the nucleosynthesis during\nthis process the reaction cross section needs to be known in the 150-230keV\nenergy window (Gamow peak). At these sub-Coulomb energies cross section direct\nmeasurements are severely affected by the low event rate, making us rely on\ninput from indirect methods and extrapolations from higher-energy direct data.\nThis leads to an uncertainty in the cross section at the relevant energies too\nhigh to reliably constrain the nuclear physics input to s-process calculations.\nWe present the results from a new deep-underground measurement of\n13C({\\alpha},n)16O, covering the energy range 230-300keV, with drastically\nreduced uncertainties over previous measurements and for the first time\nproviding data directly inside the s-process Gamow peak. Selected stellar\nmodels have been computed to estimate the impact of our revised reaction rate.\nFor stars of nearly solar composition, we find sizeable variations of some\nisotopes, whose production is influenced by the activation of close-by\nbranching points that are sensitive to the neutron density, in particular the\ntwo radioactive nuclei 60Fe and 205Pb, as well as 152Gd",
      "generated_abstract": "Title: Direct Evaluation of the 13C({\\alpha},n)16O Reaction Cross Section Within the s-process Gamow Peak\n\nRevised Abstract: The astrophysical s-process is primarily influenced by the reaction 13C({\\alpha},n)16O, occurring within thermally pulsating Asymptotic Giant Branch stars at approximately 90 MK temperatures. Accurate modeling of nucleosynthesis during this process necessitates precise knowledge of the reaction cross section within the energy window of 150-230keV, known as the Gamow peak. Due to the low event rate at these sub-Coulomb energies, direct measurements of cross sections are significantly compromised, leading to dependence on indirect methodologies and extrapolations from high-energy direct data. This results in an uncertainty level in the cross section at relevant energies, which is too elevated to accurately constrain the nuclear physics input for s-process computations. This work presents outcomes from a novel deep-underground measurement of 13C({\\alpha},n)16O, encompassing the energy range of 230-300keV, offering substantially reduced uncertainties compared to prior measurements, and for the first time, contributing data directly within the s-process Gamow peak. The impact of our revised reaction rate has been estimated using selected stellar models. For stars with almost solar composition, we discover considerable variations in the production of certain isotopes, specifically the two radioactive nuclei 60Fe and 205Pb, and 152Gd, whose production is influenced by the activation of nearby branching points sensitive to neutron density.",
      "original_id": "oai:arXiv.org:2110.00303",
      "created": "2021-10-01",
      "categories": [
        "nucl-ex"
      ]
    }
  ]
}